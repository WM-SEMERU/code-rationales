{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local Aggregations Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def param_default():\n",
    "    return {\n",
    "        'model_name' : '/workspaces/code-rationales/data/codeparrot-small/checkpoints/checkpoint-29000', \n",
    "        'cache_dir': '/workspaces/code-rationales/datax/df_cache_dir',\n",
    "        'max_generation_length': 50, \n",
    "        'number_experiments': 5, \n",
    "        'bootstrapping': 50\n",
    "    }\n",
    "prompts = [\n",
    "    'def multiply_numbers(a*b):\\n   print(\\'This function multiply two numbers\\')'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = param_default()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CORE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import csv\n",
    "import seaborn as sns; sns.set_theme()\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import functools\n",
    "import json\n",
    "\n",
    "pd.options.display.float_format = '{:.2f}'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from code_rationales.loader import download_grammars\n",
    "from tree_sitter import Language, Parser\n",
    "import code_rationales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (2.0.2) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n",
      "2023-07-19 20:44:02.844425: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-19 20:44:03.002353: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset, load_from_disk\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import importlib\n",
    "from matplotlib import colors\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, '/workspaces/code-rationales/sequential-rationales/huggingface')\n",
    "from rationalization import rationalize_lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taxonomies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Programming Language Taxonomy\n",
    "def pl_taxonomy_python() -> dict:\n",
    "    return {\n",
    "  \"punctuation\": ['{', '}', '[', ']', '(', ')','\\\"', ',', '.', '...', ';', ':'], \n",
    "  \"exceptions\": ['raise_statement','catch', 'try', 'finally', 'throw', 'throws', 'except'],\n",
    "  \"oop\": ['def','class','instanceof','interface','private','protected','public','abstract','extends','package','this','implements','import','new','super'],\n",
    "  \"asserts\": ['assert'],\n",
    "  \"types\": ['tuple','set','list','pair','subscript','type','none','dictionary','integer','native','static','synchronized','transient','volatile','void','final','enum','byte','char','float','boolean','double','int','long','short','strictfp'],\n",
    "  \"conditionals\": ['else', 'if', 'switch', 'case', 'default'],\n",
    "  \"loops\": ['break', 'do', 'for', 'while', 'continue'],\n",
    "  \"operators\": ['as','yield','is','@','in','and','or','not','**','slice','%','+','<','>','=','+','-','*','/','%','++','--','!','==','!=','>=','<=','&&','||','?',':','~','<<','>>','>>>','&','^','|','//'],\n",
    "  \"indentation\": ['\\n','\\t'],\n",
    "  \"bool\": ['true', 'false'], \n",
    "  \"functional\":['lambda','lambda_parameters'],\n",
    "  \"with\" : ['with','with_item','with_statement','with_clause'], \n",
    "  \"return\" :['return'],\n",
    "  \"structural\" : ['attribute','module', 'argument_list','parenthesized_expression','pattern_list','class_definition','function_definition','block'],\n",
    "  \"statements\" : ['return_statement','break_statement','assignment','while_statement','expression_statement','assert_statement'],\n",
    "  \"expression\": ['call','exec','async','ellipsis','unary_operator','binary_operator','as_pattern_target','boolean_operator','as_pattern','comparison_operator','conditional_expression','named_expression','not_operator','primary_expression','as_pattern'],\n",
    "  \"errors\": [\"ERROR\"],\n",
    "  \"identifier\":[\"identifier\"],  \n",
    "  \"comment\":[\"comment\"],\n",
    "  \"string\": ['string','interpolation','string_content','string_end','string_start','escape_sequence']  \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model, Tokenizer Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(32768, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=32768, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(params['model_name'], cache_dir=params['cache_dir'])\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(params['model_name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local Dataset Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sampled_code = pd.DataFrame(prompts, columns=['prompt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sampled_code['input_ids'] = tokenizer(df_sampled_code['prompt'].tolist())['input_ids']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Sampling Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_sampled_generation(\n",
    "        df_sampled_code, \n",
    "        model,\n",
    "        number_samples = 1,\n",
    "        max_gen_tok = 100, \n",
    "        top_k = 0\n",
    "    ):\n",
    "    dict_generated_code = {i: [] for i in range(number_samples)}\n",
    "    for idx_prompt, prompt in enumerate(df_sampled_code['prompt']):\n",
    "        input = tokenizer([prompt], return_tensors=\"pt\")\n",
    "        input.to(model.device)\n",
    "        outputs = model.generate(**input, do_sample=True,\n",
    "                                 max_length=max_gen_tok,\n",
    "                                 top_k=top_k, \n",
    "                                 num_return_sequences=number_samples, \n",
    "                                 pad_token_id=tokenizer.eos_token_id)\n",
    "        for index, output in enumerate(outputs):\n",
    "            dict_generated_code[index].append(output.tolist())\n",
    "    df_temp = pd.DataFrame().from_dict(data=dict_generated_code) # DataFrame from Generation\n",
    "    df_temp = pd.concat([df_sampled_code.reset_index(), df_temp ], axis=1) #Index before concating\n",
    "    return df_temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Rationales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If the model is not fine-tuned or compatible, it will rise an error\n",
    "#This function works for one tensor of source token and one tensor of target tokens\n",
    "def rationalize_model(model, tokenizer, input_ids, max_token_size: int, verbose=True):\n",
    "    torch.cuda.empty_cache() #Cleaning Cache\n",
    "    all_rationales, log = rationalize_lm(\n",
    "        model = model,\n",
    "        input_ids = input_ids[:max_token_size],\n",
    "        tokenizer = tokenizer,\n",
    "        verbose = verbose,\n",
    "        max_steps=1024 #Max number of steps for greedy rationalization\n",
    "    )\n",
    "    return all_rationales, log "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_multiple_rational(\n",
    "    model,\n",
    "    tokenizer, \n",
    "    arr_target_tokens, \n",
    "    seq_id, #mapping sequence id\n",
    "    max_token_size,\n",
    "    verbose=True\n",
    "):\n",
    "    arr_log = []\n",
    "    for index, val in enumerate(arr_target_tokens):\n",
    "        all_rationales, log = rationalize_model(\n",
    "            model=model, \n",
    "            tokenizer=tokenizer, \n",
    "            input_ids=val,\n",
    "            max_token_size=max_token_size,\n",
    "            verbose=False\n",
    "        )\n",
    "        arr_log.append(log)\n",
    "    arr_code_rationales = [ log['rationalization'] for log in arr_log ] #extracting just rationalizations\n",
    "    arr_from_sentence = [ list(np.full( len(val), seq_id[arr_i] )) #arr_i maps to the real sequence id\n",
    "                            for arr_i, val in enumerate(arr_code_rationales)]\n",
    "    arr_code_rationales = sum( arr_code_rationales, [] ) #flatting\n",
    "    arr_from_sentence = sum( arr_from_sentence, [] ) #flatting\n",
    "    return arr_code_rationales, arr_from_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pandas_rationales( arr_code_rationales, arr_from_sentence ):\n",
    "    #Creating pandas_1 {p_rationale}\n",
    "    rational = lambda list_log,typeset: [ (dict_tok['added_token_text'],round(dict_tok['true_token_prob'],6)) for dict_tok in list_log if dict_tok['from']==typeset]\n",
    "    log = lambda log_row: [(log_dict['added_token_text'],log_dict['true_token_prob']) for log_dict in log_row] #Typeset\n",
    "\n",
    "    log_position = lambda log_row: [log_dict['added_token_position'] for log_dict in log_row] #Position of the Rationale\n",
    "    log_prediction = lambda log_row: [log_dict['true_token_prob'] for log_dict in log_row] #Rationale Prob\n",
    "\n",
    "    p_rationale = pd.DataFrame()\n",
    "\n",
    "    p_rationale['goal_token'] = [dict_token['goal_word'] for dict_token in arr_code_rationales]\n",
    "    p_rationale['from_seq_id'] = arr_from_sentence\n",
    "\n",
    "    p_rationale['typesets_tgt'] = [ log(log_row) for log_row in [dict_token['log'] for dict_token in arr_code_rationales]]\n",
    "    \n",
    "    p_rationale['rationale_pos_tgt'] = [ log_position(log_row) for log_row in [dict_token['log'] for dict_token in arr_code_rationales]]\n",
    "    p_rationale['rationale_prob_tgt'] = [ log_prediction(log_row) for log_row in [dict_token['log'] for dict_token in arr_code_rationales]]\n",
    "\n",
    "\n",
    "    return p_rationale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Running Rationalization\n",
    "def run_code_rational( \n",
    "        df_generated_input,\n",
    "        tensor_size, #Control the size of the experiment\n",
    "        experiment = '5',\n",
    "        batch_size = 100, \n",
    "        model = model,\n",
    "        max_token_size = 44,\n",
    "        verbose = True \n",
    "    ):\n",
    "\n",
    "    arr_rationals = []\n",
    "    arr_from_seq = []\n",
    "\n",
    "    for i in range( 0 , tensor_size , batch_size ):\n",
    "        print('************************' + str(i) + '************************')\n",
    "        t_generated_input = df_generated_input[experiment].values[i:i+batch_size]\n",
    "        t_generated_input = [ torch.tensor(s).to(model.device) for s in t_generated_input]\n",
    "\n",
    "        t_arr_rationals,t_arr_from_seq = run_multiple_rational(\n",
    "            model = model,\n",
    "            tokenizer = tokenizer,\n",
    "            arr_target_tokens =  t_generated_input, \n",
    "            seq_id = list(range(i,i+batch_size)),\n",
    "            max_token_size = max_token_size,\n",
    "            verbose = verbose\n",
    "        )\n",
    "\n",
    "        arr_rationals = arr_rationals + t_arr_rationals\n",
    "        arr_from_seq = arr_from_seq + t_arr_from_seq\n",
    "\n",
    "        torch.cuda.empty_cache() #Cleaning Cache\n",
    "        \n",
    "    print(\"Experiment Finished: \" + str(experiment))\n",
    "    return pandas_rationales( arr_rationals, arr_from_seq )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_code_rational_all_set(exp, tensor_n = 100, BATCH = 10, max_token_size = 44): #When Tensor_n and batch differs then 'from_seq_id' is lost\n",
    "    torch.cuda.empty_cache() #Cleaning Cache\n",
    "    EXP = exp\n",
    "    test_arr_rationals = run_code_rational( \n",
    "            df_generated_input = df_generated_input,\n",
    "            tensor_size = tensor_n,\n",
    "            experiment = EXP,\n",
    "            batch_size = BATCH, \n",
    "            model = model,\n",
    "            max_token_size = max_token_size,\n",
    "            verbose = False \n",
    "        )\n",
    "    #Saving process\n",
    "    return test_arr_rationals\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AST Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unroll_node_types(\n",
    "    nested_node_types: dict  # node_types from tree-sitter\n",
    ") -> list: # list of node types\n",
    "    def iterate_and_unroll_dict(nested_node_types: dict, all_node_types: set):\n",
    "        for key, value in nested_node_types.items():\n",
    "            if key == 'type' and type(value) == str:\n",
    "                all_node_types.add(value)\n",
    "            if type(value) == dict:\n",
    "                iterate_and_unroll_dict(value, all_node_types)\n",
    "            if type(value) == list:\n",
    "                for element in value:\n",
    "                    iterate_and_unroll_dict(element, all_node_types) \n",
    "    all_node_types = set()\n",
    "    for dictionary in nested_node_types:\n",
    "        iterate_and_unroll_dict(dictionary, all_node_types)\n",
    "    all_node_types.add('ERROR')\n",
    "    return list(all_node_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_parser(lang: str):\n",
    "    # Grab the node types from the tree-sitter language\n",
    "    language = Language(f\"{code_rationales.__path__[0]}/grammars/tree-sitter-languages.so\", lang)\n",
    "    node_path = f\"{code_rationales.__path__[0]}/grammars/tree-sitter-{lang}/src/node-types.json\"\n",
    "    with open(node_path) as f:\n",
    "            node_types = json.load(f)\n",
    "    node_types = unroll_node_types(node_types)\n",
    "    # Create a parser for the language\n",
    "    parser = Parser()\n",
    "    parser.set_language(language)\n",
    "    return parser, node_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def traverse(\n",
    "    node,       # tree-sitter node\n",
    ") -> None:\n",
    "    \"\"\"Traverse in a recursive way, a tree-sitter node and append results to a list.\"\"\"\n",
    "    results = []\n",
    "    def traverse_tree(node, results):\n",
    "        if node.type == 'string':\n",
    "            results.append(node)\n",
    "            return\n",
    "        for n in node.children:\n",
    "            traverse_tree(n, results)\n",
    "        if not node.children:\n",
    "            results.append(node)\n",
    "    traverse_tree(node, results)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_offset(\n",
    "    point,              #point to convert\n",
    "    lines: list         #list of lines in the source code\n",
    "    ):\n",
    "        \"\"\"Convert the point to an offset\"\"\"\n",
    "        row, column = point\n",
    "        chars_in_rows = sum(map(len, lines[:row])) + row\n",
    "        chars_in_columns = len(lines[row][:column])\n",
    "        offset = chars_in_rows + chars_in_columns\n",
    "        return offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_node_span(node, lines):\n",
    "    \"\"\"Get the span position of the node in the code string\"\"\"\n",
    "    start_span = convert_to_offset(node.start_point, lines)\n",
    "    end_span = convert_to_offset(node.end_point, lines)\n",
    "    return start_span, end_span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_type(\n",
    "    tok_span: tuple, # (start, end) position of a token in tokenizer\n",
    "    nodes: list,     # list of tree-sitter nodes\n",
    "    lines: list,     # list of lines in the code\n",
    ") -> tuple: # (parent_type, token_type) of the token\n",
    "    \"\"\"Get the parent AST type and token AST type of a token.\"\"\"\n",
    "    node_spans = [get_node_span(node, lines) for node in nodes]\n",
    "    for i, span in enumerate(node_spans):\n",
    "        if (span[0] <= tok_span[0] and tok_span[0] < span[1]) or (span[0] < tok_span[1] and tok_span[1] <= span[1]):\n",
    "            return nodes[i].parent.type, nodes[i].type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_nodes(\n",
    "    tok_span: tuple, # (start, end) position of a token in tokenizer\n",
    "    node,            # tree-sitter node\n",
    "    lines: list,     # list of lines in the code\n",
    ") -> list: \n",
    "    \"\"\"Get all AST types for the given token span\"\"\"\n",
    "    results = []\n",
    "    def traverse_and_get_types(tok_span, node, lines, results) -> None:\n",
    "        node_span = get_node_span(node, lines)\n",
    "        if (node_span[0] <= tok_span[0] and tok_span[0] < node_span[1]) or (node_span[0] < tok_span[1] and tok_span[1] <= node_span[1]):\n",
    "            results.append(node.type)\n",
    "        for n in node.children:\n",
    "            traverse_and_get_types(tok_span, n, lines, results)\n",
    "    traverse_and_get_types(tok_span, node, lines, results)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rational Local Aggregates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_left_span = lambda index, initial_token, df_rationals : len(str(initial_token) + ''.join(str(df_rationals['goal_token'][:index])))\n",
    "calculate_right_span = lambda left_span, token : len(left_span) + len(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_local_results(global_results):\n",
    "    def clean_dictonary(result_dict):\n",
    "        clean_dict = result_dict.copy()\n",
    "        for key, value in result_dict.items():\n",
    "            if not value:\n",
    "                clean_dict.pop(key)\n",
    "        return clean_dict\n",
    "    for key, value in global_results.items():\n",
    "        global_results[key] = clean_dictonary(value)\n",
    "    return clean_dictonary(global_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrapping( np_data, np_func, size ):\n",
    "    \"\"\"Create a bootstrap sample given data and a function\n",
    "    For instance, a bootstrap sample of means, or mediands. \n",
    "    The bootstrap replicates are a long as the original size\n",
    "    we can choose any observation more than once (resampling with replacement:np.random.choice)\n",
    "    \"\"\"\n",
    "    \n",
    "    #Cleaning NaNs\n",
    "    #np_data_clean = np_data[ np.logical_not( np.isnan(np_data) ) ] \n",
    "    \n",
    "    #The size of the bootstrap replicate is as big as size\n",
    "    #Creating the boostrap replicates as long as the orignal data size\n",
    "    #This strategy might work as imputation \n",
    "    bootstrap_repl = [ np_func( np.random.choice( np_data, size=len(np_data) ) ) for i in range( size ) ]\n",
    "    \n",
    "    #logging.info(\"Covariate: \" + cov) #Empirical Mean\n",
    "    #logging.info(\"Empirical Mean: \" + str(np.mean(np_data_clean))) #Empirical Mean\n",
    "    #logging.info(\"Bootstrapped Mean: \" + str( np.mean(bootstrap_repl) ) ) #Bootstrapped Mean\n",
    "    \n",
    "    return np.array( bootstrap_repl )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_samples_local_results(local_results: dict, size: int):\n",
    "    local_results = local_results.copy()\n",
    "    for target_type, target_value in local_results.items():\n",
    "        for source_type, source_value in target_value.items():\n",
    "            local_results[target_type][source_type] = bootstrapping(source_value, np.mean, size).tolist()\n",
    "    return local_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_rationals(df_sample_experiments: list, parser, node_types: list, number_samples: int):\n",
    "    experiments_aggregation = []\n",
    "    for exp_idx, df_experiment in enumerate(df_sample_experiments):\n",
    "        experiment_rational_result = df_experiment.reset_index()\n",
    "        local_experiment_results = {goal_token : {node_type : [] for node_type in node_types} for goal_token in experiment_rational_result['goal_token']}\n",
    "        initial_token = experiment_rational_result['typesets_tgt'][0][0][0]\n",
    "        experiment_rational_result.insert(loc=experiment_rational_result.columns.get_loc('goal_token')+1, column='span', value=list(map(lambda tuple: (tuple[0],tuple[0]+len(tuple[1])),[(calculate_left_span(index, initial_token, experiment_rational_result), str(token)) for index, token in experiment_rational_result['goal_token'].items()])))\n",
    "        target_code = experiment_rational_result['typesets_tgt'][0][0][0] + ''.join(str(experiment_rational_result['goal_token']))\n",
    "        target_ast = parser.parse(bytes(target_code, 'utf8')).root_node\n",
    "        for target_token_idx in range(len(experiment_rational_result['span'])):\n",
    "            for rational_idx, rational_pos in enumerate(experiment_rational_result['rationale_pos_tgt'][target_token_idx]):\n",
    "                if experiment_rational_result['rationale_pos_tgt'][target_token_idx][rational_idx] > 0: #rational 1 position.\n",
    "                    try:\n",
    "                        rational_node_types = get_token_nodes(experiment_rational_result['span'][experiment_rational_result['rationale_pos_tgt'][target_token_idx][rational_idx]-1], target_ast, target_code.split(\"\\n\"))\n",
    "                        [local_experiment_results[experiment_rational_result['goal_token'][target_token_idx]][rational_node_type].append(experiment_rational_result['rationale_prob_tgt'][target_token_idx][rational_idx]) for rational_node_type in rational_node_types]\n",
    "                    except Exception as e:\n",
    "                        print('rational pos out of range')\n",
    "        experiments_aggregation.append(clean_local_results(local_experiment_results))\n",
    "    return experiments_aggregation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taxonomy Mapping "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_category_by_token(taxonomy_dict: dict, token_type: str):\n",
    "    for key, value in taxonomy_dict.items():\n",
    "        if token_type in value:\n",
    "            return key\n",
    "    return 'unknown'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_to_taxonomy(taxonomy_dict: dict, result_dict: dict):\n",
    "    result_dict = result_dict.copy()\n",
    "    mappings = {token: {category : [] for category in taxonomy_dict.keys()} for token in result_dict.keys()}\n",
    "    mappings['unknown'] = {}\n",
    "    for target_token, value in result_dict.items():\n",
    "        for source_token, probs in value.items():\n",
    "            mappings[target_token][search_category_by_token(taxonomy_dict, source_token)]+=probs\n",
    "    return clean_local_results(mappings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_results_to_taxonomy(taxonomy_dict: dict, results:list):\n",
    "    results = results.copy()\n",
    "    for sample_idx, sample_result in enumerate(results):\n",
    "        for exp_idx, experiment_result in enumerate(sample_result):\n",
    "            results[sample_idx][exp_idx] = map_to_taxonomy(taxonomy_dict, experiment_result)\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_local_results(local_results: list, size: int):\n",
    "    local_results = local_results.copy()\n",
    "    for sample_id, sample_result in enumerate(local_results):\n",
    "        for exp_id, exp_result in enumerate(sample_result):\n",
    "            for target_token, result_dict in exp_result.items():\n",
    "                for rational, probs in result_dict.items():\n",
    "                    local_results[sample_id][exp_id][target_token][rational] = bootstrapping(probs, np.mean, size)\n",
    "    return local_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOCAL EXPERIMENT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SAMPLING GENERATION \n",
    "df_generated_input = df_sampled_generation(\n",
    "    df_sampled_code=df_sampled_code, \n",
    "    model=model, \n",
    "    number_samples=params['number_experiments'], \n",
    "    max_gen_tok=params['max_generation_length'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************0************************\n",
      "Experiment Finished: 0\n",
      "************************0************************\n",
      "Experiment Finished: 1\n",
      "************************0************************\n",
      "Experiment Finished: 2\n",
      "************************0************************\n",
      "Experiment Finished: 3\n",
      "************************0************************\n",
      "Experiment Finished: 4\n"
     ]
    }
   ],
   "source": [
    "### RATIONALES\n",
    "experiment_results = []\n",
    "for i in df_generated_input.columns[3:]: #Only Generated Sequences \n",
    "    experiment_result = run_code_rational_all_set(exp=i, tensor_n=df_generated_input.shape[0], BATCH=10, max_token_size=params['max_generation_length'])\n",
    "    experiment_result['exp'] = i\n",
    "    experiment_results.append(experiment_result)\n",
    "df_experiment_results = pd.concat(experiment_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Gourping by sample\n",
    "samples_experiments = [df_experiment_results[df_experiment_results['from_seq_id'] == sample_id] for sample_id in df_experiment_results['from_seq_id'].unique()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/code_rationales/grammars\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "### DEFINE PARSER\n",
    "languages=['python', 'java']\n",
    "download_grammars(languages)\n",
    "parser, node_types = create_parser('python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "###AGGREGATE LOCAL RESULTS BY AST\n",
    "results_by_ast = [aggregate_rationals([sample_experiments[sample_experiments['exp'] == exp_id] for exp_id in sample_experiments['exp'].unique()], parser, node_types, df_generated_input.shape[0]) for sample_experiments in samples_experiments]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "###AGGREGATES BY TAXONOMY\n",
    "taxonomy_results = convert_results_to_taxonomy(pl_taxonomy_python(), results_by_ast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BOOTSTRAPPING\n",
    "taxonomy_results = bootstrap_local_results(taxonomy_results, params['bootstrapping'])\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize - AST Aggregation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#results[<sample>][<experiment>][<token>] = grouped rationals dict\n",
    "print(results_by_ast[0][0].keys())\n",
    "print(results_by_ast[0][0]['numbers'].keys())\n",
    "print(np.mean(results_by_ast[0][0]['numbers']['errors']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize - Taxonomy Aggregation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys([' multiply', '_', 'numbers', '(', 'a', '*', 'b', '):', '\\n  ', ' print', \"('\", 'This', ' function', ' two', ' numbers', \"')\", 'MATCH', 'ES', ' TO', ' COMP', 'REPLY', ' full', 'number', ' =', \" ''\", ' analy', 'tical', ' run', 'times', ' 0', ' theta'])\n",
      "dict_keys(['structural', 'statements', 'expression', 'errors', 'identifier'])\n",
      "0.000702932826243341\n"
     ]
    }
   ],
   "source": [
    "#results[<sample>][<experiment>][<token>] = grouped rationals dict\n",
    "print(taxonomy_results[0][0].keys())\n",
    "print(taxonomy_results[0][0]['numbers'].keys())\n",
    "print(np.mean(taxonomy_results[0][0]['numbers']['errors']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['structural', 'statements', 'expression', 'errors', 'identifier'])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxonomy_results[0][0]['numbers'].keys()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
