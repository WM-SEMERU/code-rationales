{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import logging\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, set_seed\n",
    "from accelerate import Accelerator\n",
    "import tensorflow as tf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataloader import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import IterableDataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.0 Model evaluation  for mmodel generation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part of the notebook loads a Json testbed, generate the prediction, align the output for each generation agaisnt the ground truth the measure the distance. NO Bootstraping applied!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConstantLengthDataset(IterableDataset):\n",
    "    def __init__(self, tokenizer, dataset, field, seq_length=1024, num_of_sequences=1024, chars_per_token=3.6):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.concat_token_id = tokenizer.bos_token_id\n",
    "        self.dataset = dataset\n",
    "        self.seq_length = seq_length\n",
    "        self.input_characters = seq_length * chars_per_token * num_of_sequences\n",
    "        self.field=field\n",
    "\n",
    "    def __iter__(self):\n",
    "        iterator = iter(self.dataset)\n",
    "        more_examples = True\n",
    "        while more_examples:\n",
    "            buffer, buffer_len = [], 0\n",
    "            while True:\n",
    "                if buffer_len >= self.input_characters:\n",
    "                    break\n",
    "                try:\n",
    "                    buffer.append(next(iterator)[self.field])\n",
    "                    buffer_len += len(buffer[-1])\n",
    "                except StopIteration:\n",
    "                    more_examples = False\n",
    "                    break\n",
    "            tokenized_inputs = self.tokenizer(buffer, truncation=False)[\"input_ids\"]\n",
    "            all_token_ids = []\n",
    "            for tokenized_input in tokenized_inputs:\n",
    "                all_token_ids.extend(tokenized_input + [self.concat_token_id])\n",
    "            for i in range(0, len(all_token_ids), self.seq_length):\n",
    "                input_ids = all_token_ids[i : i + self.seq_length]\n",
    "                if len(input_ids) == self.seq_length:\n",
    "                    yield torch.tensor(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader(args,tokenizer):\n",
    "    data_files  = {\"test\":args['test_bed_name']}\n",
    "    valid_data = load_dataset(args['data_path'], data_files=data_files, split=\"test\")\n",
    "    valid_dataset = ConstantLengthDataset(tokenizer, valid_data, args['field'], seq_length=args['seq_length'])\n",
    "    eval_dataloader = DataLoader(valid_dataset, batch_size=args['batch_size'])\n",
    "    return  eval_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(args,model,eval_dataloader):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    for step, batch in enumerate(eval_dataloader):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(batch, labels=batch)\n",
    "        loss = outputs.loss.repeat(args['batch_size'])\n",
    "        losses.append(accelerator.gather(loss))\n",
    "\n",
    "        if args['max_eval_steps'] > 0 and step >= args['max_eval_steps']:\n",
    "            break\n",
    "    loss = torch.mean(torch.cat(losses))\n",
    "    try:\n",
    "        perplexity = torch.exp(loss)\n",
    "    except OverflowError:\n",
    "        perplexity = float(\"inf\")\n",
    "    return loss.item(), perplexity.item()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def param_default():\n",
    "    model_name = 'codeparrot-small' #<-- Scope\n",
    "    test_bed_name='code_completion_random_cut_5k_30_512_tokens.json'\n",
    "    semeru_datases_path= '/workspaces/code-rationales/'\n",
    "    data_path = Path(semeru_datases_path+'datax/' + model_name + '/')\n",
    "    data_path= semeru_datases_path+'semeru-datasets/semeru/galeras/code_rationales'\n",
    "    return {\n",
    "        'out_processed' : '/datasets/out_processed/',\n",
    "        'checkpoint_file': Path(semeru_datases_path+'data/codeparrot-small/checkpoints/checkpoint-29000'), #Model\n",
    "        'output_results' : 'results/' ,\n",
    "        'seed': 1,\n",
    "        'data_path': data_path,\n",
    "        'test_bed_name':test_bed_name,\n",
    "        'seq_length': 64,\n",
    "        'batch_size': 2,\n",
    "        'field': \"random_cut\",\n",
    "        'max_eval_steps':-1\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.memory_allocated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Accelerator\n",
    "accelerator = Accelerator()\n",
    "params = param_default()\n",
    "# Parse configuration\n",
    "set_seed(params['seed'])\n",
    "\n",
    "# Logging\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\", datefmt=\"%m/%d/%Y %H:%M:%S\", level=logging.INFO\n",
    ")\n",
    "\n",
    "# Load model and tokenizer\n",
    "checkpoint = params['checkpoint_file']\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint)\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = model.to( device ) #WARNING, Verify the device before assigning to memory\n",
    "\n",
    "# Load dataset and dataloader\n",
    "valid_dataset, eval_dataloader = create_dataloader(params,tokenizer)\n",
    "\n",
    "# Prepare everything with our `accelerator`.\n",
    "model, valid_dataset, eval_dataloader = accelerator.prepare(model, valid_dataset, eval_dataloader)\n",
    "\n",
    "# Evaluate and save the last checkpoint\n",
    "logger.info(\"Evaluating and saving model after training\")\n",
    "eval_loss, perplexity = evaluate(params, model, eval_dataloader)\n",
    "logger.info(f\"loss/eval: {eval_loss}, perplexity: {perplexity}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive test for code completion"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing a basic example for code generation from codeparrot model and using the given checkpoint from compatibilization process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#device =\"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt =\"def duntion_test():\"\n",
    "prompt=\"def test_frequency_condition_alone(self):\\n        prev_hour = timezone.now() - timedelta(hours=1)\"\n",
    "params = param_default()\n",
    "\n",
    "#torch.manual_seed(0)\n",
    "model = AutoModelForCausalLM.from_pretrained(params['checkpoint_file'])\n",
    "tokenizer = AutoTokenizer.from_pretrained(params['checkpoint_file'])\n",
    "model = model.to( device ) #WARNING, Verify the device before assigning to memory\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "outputs = model.generate(input_ids, do_sample=True, max_length=128)\n",
    "\n",
    "tokenizer.batch_decode(outputs, skip_special_tokens=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.0 Evaluation for codeparrot using the sampling testbeds"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Outcome generation & Levenshtein evaluation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This iterator is NOT working for batches > 1!!\n",
    "class ConstantTokenLengthDataset(IterableDataset):\n",
    "    def __init__(self, tokenizer, dataset, field, num_of_tokens=64, num_of_sequences=1024):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.dataset = dataset\n",
    "        self.num_of_tokens = min(num_of_tokens, tokenizer.model_max_length)\n",
    "        self.field=field\n",
    "        self.input_char = int(self.num_of_tokens*3.6)\n",
    "        self.num_of_sequences=num_of_sequences\n",
    "        self.prompts=[]\n",
    "\n",
    "    def __iter__(self):  \n",
    "        for i, buffer in enumerate(self.dataset):\n",
    "            size = min(len(buffer[self.field]),self.input_char)\n",
    "            input = buffer[self.field][:size]\n",
    "            self.prompts.append(input)\n",
    "            if i > self.num_of_sequences:\n",
    "                break\n",
    "        tokenized_inputs = self.tokenizer(self.prompts, max_length= self.num_of_tokens, padding=True, truncation=True, return_tensors=\"pt\")[\"input_ids\"]\n",
    "        for tokenized_input in tokenized_inputs:\n",
    "            yield torch.tensor(tokenized_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader(args,tokenizer):\n",
    "    data_files  = {\"test\":args['test_bed_name']}\n",
    "    valid_data = load_dataset(args['data_path'], data_files=data_files, split=\"test\")\n",
    "    valid_dataset = ConstantTokenLengthDataset(tokenizer, valid_data, args['field'], num_of_tokens=args['seq_length'])\n",
    "    eval_dataloader = DataLoader(valid_dataset, batch_size=1)\n",
    "    return  valid_dataset, eval_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_outcomes(args,model,eval_dataloader,valid_data):\n",
    "    model.eval()\n",
    "    results = []\n",
    "    for step, inputs in enumerate(eval_dataloader):\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(inputs, do_sample=True, max_length=128,  pad_token_id=tokenizer.eos_token_id)\n",
    "            outcome = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "        prompt=valid_data.prompts[step]\n",
    "        result = {\"prompt\": prompt, \"outcome\":outcome}\n",
    "        results.append(result)\n",
    "        if args['max_eval_steps'] > 0 and step >= args['max_eval_steps']:\n",
    "            break\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Accelerator\n",
    "accelerator = Accelerator()\n",
    "params = param_default()\n",
    "# Parse configuration\n",
    "set_seed(params['seed'])\n",
    "\n",
    "# Logging\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\", datefmt=\"%m/%d/%Y %H:%M:%S\", level=logging.INFO\n",
    ")\n",
    "\n",
    "# Load model and tokenizer\n",
    "checkpoint = params['checkpoint_file']\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint)\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = model.to( device ) #WARNING, Verify the device before assigning to memory\n",
    "\n",
    "# Load dataset and dataloader\n",
    "valid_dataset, eval_dataloader = create_dataloader(params,tokenizer)\n",
    "\n",
    "# Prepare everything with our `accelerator`.\n",
    "model, valid_dataset, eval_dataloader = accelerator.prepare(model, valid_dataset, eval_dataloader)\n",
    "\n",
    "# Evaluate and save the last checkpoint\n",
    "logger.info(\"Evaluating and saving model after training\")\n",
    "outcomes = generate_outcomes(params, model, eval_dataloader,valid_dataset)\n",
    "logger.info(f\"outomces: {len(outcomes)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import textdistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "levenshtein_similarity = textdistance.levenshtein\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lev_calc = [levenshtein_similarity.normalized_similarity(x[\"prompt\"].strip(), x[\"outcome\"][0].strip() ) for x in outcomes]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(outcomes)\n",
    "df = df.assign(lev_sim=lev_calc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Evaluation model from samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import textdistance\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/workspaces/code-rationales/data/sampling/gpt/code_completion_docstring_5k_30_150_tokens.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testbeds = ['docstring','docst_randcut','docst_sign','randcut']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(path, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def param_default():\n",
    "    model_name = 'codeparrot-small' #<-- Model\n",
    "    test_bed_name= 'code_completion_docstring_5k_30_150_tokens.csv'\n",
    "    datasets_path= Path('/workspaces/code-rationales')\n",
    "    datax = Path(datasets_path/'datax')\n",
    "    data = Path(datasets_path/'data')\n",
    "    galeras_sampling= Path(data/ 'sampling/gpt')\n",
    "    model_path='codeparrot-small/checkpoints/checkpoint-29000'\n",
    "    return {\n",
    "        'out_processed' : '/datasets/out_processed/',\n",
    "        'checkpoint_file': str(Path(data/model_path)), #Model\n",
    "        'output_results' : 'results/' ,\n",
    "        'seed': 1,\n",
    "        'data_path': str(Path(galeras_sampling/test_bed_name)),\n",
    "        'test_bed_name':test_bed_name,\n",
    "        'seq_length': 64,\n",
    "        'batch_size': 2,\n",
    "        'field': \"random_cut\",\n",
    "        'max_eval_steps':-1,\n",
    "        'log_path': str(Path(datax/'logs/logs.log'))\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.device(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accelerator = Accelerator()\n",
    "params = param_default()\n",
    "# Parse configuration\n",
    "set_seed(params['seed'])\n",
    "# Logging\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\", datefmt=\"%m/%d/%Y %H:%M:%S\", level=logging.INFO, filename=params['log_path']\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "# Load model and tokenizer\n",
    "checkpoint = params['checkpoint_file']\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint)\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "#tokenizer.pad_token = tokenizer.eos_token\n",
    "model = model.to( device ) #WARNING, Verify the device before assigning to memory\n",
    "\n",
    "# Prepare everything with our `accelerator`.\n",
    "model = accelerator.prepare(model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "levenshtein_similarity = textdistance.levenshtein\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_df_string_to_array(df,column):\n",
    "    try : \n",
    "        df[column] = df[column].apply(ast.literal_eval)\n",
    "    except:\n",
    "        logger.warning(\"Not column convertion from string to array\")\n",
    "    finally:\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_levenshtein_for_samples(tokenizer, df, samples= 30):\n",
    "    outcomes_df = pd.DataFrame()\n",
    "    outcomes_df['ground_truth'] = df['ground_truth'].apply(lambda x: x.strip())\n",
    "    for i in range(0,samples):\n",
    "        with torch.no_grad():\n",
    "            df = convert_df_string_to_array(df, str(i))\n",
    "            outcomes = tokenizer.batch_decode(df[str(i)].to_list(), skip_special_tokens=True)\n",
    "            outcomes_df['outcome_'+str(i)] = outcomes\n",
    "            col_name = 'lev_'+str(i)\n",
    "            outcomes_df[col_name] = [levenshtein_similarity.normalized_similarity(x.strip(), outcomes_df['ground_truth'][step]) for step,x in enumerate(outcomes)]\n",
    "            logger.info(\"computed levenshtein for outome \" + str(i))   \n",
    "       \n",
    "    return outcomes_df\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_levenshtenin_mean_pivot(df,  df_name, samples=30):\n",
    "    result_df = pd.DataFrame()\n",
    "    lev_columns = []\n",
    "\n",
    "    for i in range(0,samples):\n",
    "        col_name = 'lev_'+str(i)\n",
    "        lev_columns.append(col_name)\n",
    "        result_df.loc[i,df_name] = df[col_name].mean()\n",
    "    #result_df.loc[samples,df_name]  = result_df[df_name].mean(axis=1)\n",
    "    #result_df['avg_std_lev'] = result_df[lev_columns].std(axis=1)\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Only docstring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute levenshtein for each outcome and the average\n",
    "path = \"/workspaces/code-rationales/data/sampling/gpt/code_completion_docstring_5k_30_150_tokens.csv\"\n",
    "df = pd.read_csv(path, index_col=0)\n",
    "logger.info(\"Calculating levenshtein simmilarity\")\n",
    "docstring_df = compute_levenshtein_for_samples(tokenizer, df, 30)\n",
    "logger.info(f\"outomces: {len(docstring_df.shape)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_df = compute_levenshtenin_mean_pivot(docstring_df,'docstring')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Docstring & random cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/workspaces/code-rationales/data/sampling/gpt/code_completion_docstring_random_cut_3.8k_30_150_tokens.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_cut_df = pd.read_csv(path, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_cut_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute levenshtein for each outcome and the average\n",
    "logger.info(\"Computing docstring and random cut\")\n",
    "lev_rand_df = compute_levenshtein_for_samples(tokenizer, random_cut_df, 30)\n",
    "logger.info(f\"outcomes: {len(lev_rand_df.shape)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lev_rand_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_df = pd.concat([mean_df,compute_levenshtenin_mean_pivot(lev_rand_df,'docst_randcut')],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute levenshtein for each outcome and the average\n",
    "path = \"/workspaces/code-rationales/data/sampling/gpt/code_completion_docstring_signature_3.8k_30_150_tokens.csv\"\n",
    "df = pd.read_csv(path, index_col=0)\n",
    "logger.info(\"Evaluating adocstring and signature\")\n",
    "lev_signature_df = compute_levenshtein_for_samples(tokenizer, df, 30)\n",
    "logger.info(f\"outcomes: {len(lev_signature_df.shape)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_df = pd.concat([mean_df,compute_levenshtenin_mean_pivot(lev_signature_df,'docst_sign')],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute levenshtein for each outcome and the average\n",
    "path = \"/workspaces/code-rationales/data/sampling/gpt/code_completion_random_cut_5k_30_512_tokens.csv\"\n",
    "df = pd.read_csv(path, index_col=0)\n",
    "logger.info(\"Evaluating and saving model after training\")\n",
    "lev_rand_code_df = compute_levenshtein_for_samples(tokenizer, df, 30)\n",
    "logger.info(f\"outomces: {len(lev_rand_code_df.shape)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_df = pd.concat([mean_df,compute_levenshtenin_mean_pivot(lev_rand_code_df,'rand_cut')],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 6)) \n",
    "sns.boxplot(data=mean_df, showfliers=False,palette=\"Set2\").set_title(\"Codeparrot - AVG Levenshtein - 30 samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path= \"/workspaces/code-rationales/datax/evaluation/\"\n",
    "logger.info(\"Saving levenshtain calculation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Save the dataframe to a Parquet file\n",
    "docstring_df.to_parquet(save_path+'code_completion_docstring_5k_30_150_tokens.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lev_rand_code_df.to_parquet(save_path+'code_completion_random_cut_5k_30_512_tokens.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lev_signature_df.to_parquet(save_path+'code_completion_docstring_signature_3.8k_30_150_tokens.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lev_rand_df.to_parquet(save_path+'code_completion_docstring_random_cut_3.8k_30_150_tokens.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_df.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0 Calculating BLUE and codeBLEU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each sample calculate the BLUE and CodeBLUE then calculate the AVG for each sample and then the AVG of AVG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Params for codebleu: alpha, beta, gamma, theta\n",
    "params='0.25,0.25,0.25,0.25'\n",
    "lang= 'python'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sys.path.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/workspaces/code-rationales/nbs',\n",
       " '/usr/lib/python38.zip',\n",
       " '/usr/lib/python3.8',\n",
       " '/usr/lib/python3.8/lib-dynload',\n",
       " '',\n",
       " '/usr/local/lib/python3.8/dist-packages',\n",
       " '/usr/lib/python3/dist-packages',\n",
       " '/workspaces/code-rationales/scripts']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/workspaces/code-rationales/scripts')\n",
    "\n",
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "## based on microsoft script for calculating codeBLEU in codeSearchNet\n",
    "import CodeBLEU.bleu as bleu\n",
    "import CodeBLEU.weighted_ngram_match as weighted_ngram_match\n",
    "import CodeBLEU.syntax_match as syntax_match\n",
    "import CodeBLEU.dataflow_match as dataflow_match\n",
    "import sys\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load sampled testbeds "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path= \"/workspaces/code-rationales/datax/evaluation/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "docstring_df =pd.read_parquet(path+'code_completion_docstring_5k_30_150_tokens.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lev_rand_code_df = pd.read_parquet(path+'code_completion_random_cut_5k_30_512_tokens.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lev_signature_df=pd.read_parquet(path+'code_completion_docstring_signature_3.8k_30_150_tokens.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lev_rand_df= pd.read_parquet(path+'code_completion_docstring_random_cut_3.8k_30_150_tokens.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bleu_codeBleu(lang,params,df, gt_col, pred_col, keywords):\n",
    "    alpha,beta,gamma,theta = [float(x) for x in params.split(',')]\n",
    "    # preprocess inputs\n",
    "    pre_references = [df[gt_col].to_list()]\n",
    "    hypothesis = df[pred_col].to_list()\n",
    "    for i in range(len(pre_references)):\n",
    "        assert len(hypothesis) == len(pre_references[i])\n",
    "\n",
    "    references = []\n",
    "    for i in range(len(hypothesis)):\n",
    "        ref_for_instance = []\n",
    "        for j in range(len(pre_references)):\n",
    "            ref_for_instance.append(pre_references[j][i])\n",
    "        references.append(ref_for_instance)\n",
    "    assert len(references) == len(pre_references)*len(hypothesis)\n",
    "\n",
    "\n",
    "    # calculate ngram match (BLEU)\n",
    "    tokenized_hyps = [x.split() for x in hypothesis]\n",
    "    tokenized_refs = [[x.split() for x in reference] for reference in references]\n",
    "\n",
    "    ngram_match_score = bleu.corpus_bleu(tokenized_refs,tokenized_hyps)\n",
    "    \n",
    "    # calculate weighted ngram match\n",
    "    keywords = [x.strip() for x in open(keywords, 'r', encoding='utf-8').readlines()]\n",
    "    def make_weights(reference_tokens, key_word_list):\n",
    "        return {token:1 if token in key_word_list else 0.2 \\\n",
    "                for token in reference_tokens}\n",
    "    tokenized_refs_with_weights = [[[reference_tokens, make_weights(reference_tokens, keywords)]\\\n",
    "                for reference_tokens in reference] for reference in tokenized_refs]\n",
    "\n",
    "    weighted_ngram_match_score = weighted_ngram_match.corpus_bleu(tokenized_refs_with_weights,tokenized_hyps)\n",
    "\n",
    "    # calculate syntax match\n",
    "    syntax_match_score = syntax_match.corpus_syntax_match(references, hypothesis,lang)\n",
    "\n",
    "    # calculate dataflow match\n",
    "    dataflow_match_score = dataflow_match.corpus_dataflow_match(references, hypothesis,lang)\n",
    "\n",
    "    logger.info('ngram match: {0}, weighted ngram match: {1}, syntax_match: {2}, dataflow_match: {3}'.\\\n",
    "                        format(ngram_match_score, weighted_ngram_match_score, syntax_match_score, dataflow_match_score))\n",
    "\n",
    "    code_bleu_score = alpha*ngram_match_score\\\n",
    "                    + beta*weighted_ngram_match_score\\\n",
    "                    + gamma*syntax_match_score\\\n",
    "                    + theta*dataflow_match_score\n",
    "\n",
    "    logger.info('CodeBLEU score: '+ str(code_bleu_score))\n",
    "    return ngram_match_score, code_bleu_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang='python'\n",
    "keywords = '/workspaces/code-rationales/scripts/CodeBLEU/keywords/'+lang+'.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate docstring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>outcome_0</th>\n",
       "      <th>lev_0</th>\n",
       "      <th>outcome_1</th>\n",
       "      <th>lev_1</th>\n",
       "      <th>outcome_2</th>\n",
       "      <th>lev_2</th>\n",
       "      <th>outcome_3</th>\n",
       "      <th>lev_3</th>\n",
       "      <th>outcome_4</th>\n",
       "      <th>...</th>\n",
       "      <th>outcome_25</th>\n",
       "      <th>lev_25</th>\n",
       "      <th>outcome_26</th>\n",
       "      <th>lev_26</th>\n",
       "      <th>outcome_27</th>\n",
       "      <th>lev_27</th>\n",
       "      <th>outcome_28</th>\n",
       "      <th>lev_28</th>\n",
       "      <th>outcome_29</th>\n",
       "      <th>lev_29</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Check if the grid client is up.\\n\\n    Check f...</td>\n",
       "      <td>Generate Pyhton code that Check if the grid cl...</td>\n",
       "      <td>0.224626</td>\n",
       "      <td>Generate Pyhton code that Check if the grid cl...</td>\n",
       "      <td>0.192362</td>\n",
       "      <td>Generate Pyhton code that Check if the grid cl...</td>\n",
       "      <td>0.243636</td>\n",
       "      <td>Generate Pyhton code that Check if the grid cl...</td>\n",
       "      <td>0.227273</td>\n",
       "      <td>Generate Pyhton code that Check if the grid cl...</td>\n",
       "      <td>...</td>\n",
       "      <td>Generate Pyhton code that Check if the grid cl...</td>\n",
       "      <td>0.222586</td>\n",
       "      <td>Generate Pyhton code that Check if the grid cl...</td>\n",
       "      <td>0.217252</td>\n",
       "      <td>Generate Pyhton code that Check if the grid cl...</td>\n",
       "      <td>0.245931</td>\n",
       "      <td>Generate Pyhton code that Check if the grid cl...</td>\n",
       "      <td>0.241071</td>\n",
       "      <td>Generate Pyhton code that Check if the grid cl...</td>\n",
       "      <td>0.198839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ensure that powershell processes inline script...</td>\n",
       "      <td>Generate Pyhton code that Ensure that powershe...</td>\n",
       "      <td>0.322621</td>\n",
       "      <td>Generate Pyhton code that Ensure that powershe...</td>\n",
       "      <td>0.326347</td>\n",
       "      <td>Generate Pyhton code that Ensure that powershe...</td>\n",
       "      <td>0.325524</td>\n",
       "      <td>Generate Pyhton code that Ensure that powershe...</td>\n",
       "      <td>0.327132</td>\n",
       "      <td>Generate Pyhton code that Ensure that powershe...</td>\n",
       "      <td>...</td>\n",
       "      <td>Generate Pyhton code that Ensure that powershe...</td>\n",
       "      <td>0.345768</td>\n",
       "      <td>Generate Pyhton code that Ensure that powershe...</td>\n",
       "      <td>0.326271</td>\n",
       "      <td>Generate Pyhton code that Ensure that powershe...</td>\n",
       "      <td>0.364109</td>\n",
       "      <td>Generate Pyhton code that Ensure that powershe...</td>\n",
       "      <td>0.331646</td>\n",
       "      <td>Generate Pyhton code that Ensure that powershe...</td>\n",
       "      <td>0.306836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Encode a bytestring to a base64 string for use...</td>\n",
       "      <td>Generate Pyhton code that Encode a bytestring ...</td>\n",
       "      <td>0.257911</td>\n",
       "      <td>Generate Pyhton code that Encode a bytestring ...</td>\n",
       "      <td>0.255556</td>\n",
       "      <td>Generate Pyhton code that Encode a bytestring ...</td>\n",
       "      <td>0.258224</td>\n",
       "      <td>Generate Pyhton code that Encode a bytestring ...</td>\n",
       "      <td>0.275510</td>\n",
       "      <td>Generate Pyhton code that Encode a bytestring ...</td>\n",
       "      <td>...</td>\n",
       "      <td>Generate Pyhton code that Encode a bytestring ...</td>\n",
       "      <td>0.227596</td>\n",
       "      <td>Generate Pyhton code that Encode a bytestring ...</td>\n",
       "      <td>0.253756</td>\n",
       "      <td>Generate Pyhton code that Encode a bytestring ...</td>\n",
       "      <td>0.224818</td>\n",
       "      <td>Generate Pyhton code that Encode a bytestring ...</td>\n",
       "      <td>0.229412</td>\n",
       "      <td>Generate Pyhton code that Encode a bytestring ...</td>\n",
       "      <td>0.260450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Add the arguments for the protocol to the clie...</td>\n",
       "      <td>Generate Pyhton code that Add the arguments fo...</td>\n",
       "      <td>0.328746</td>\n",
       "      <td>Generate Pyhton code that Add the arguments fo...</td>\n",
       "      <td>0.326996</td>\n",
       "      <td>Generate Pyhton code that Add the arguments fo...</td>\n",
       "      <td>0.343606</td>\n",
       "      <td>Generate Pyhton code that Add the arguments fo...</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>Generate Pyhton code that Add the arguments fo...</td>\n",
       "      <td>...</td>\n",
       "      <td>Generate Pyhton code that Add the arguments fo...</td>\n",
       "      <td>0.385366</td>\n",
       "      <td>Generate Pyhton code that Add the arguments fo...</td>\n",
       "      <td>0.316712</td>\n",
       "      <td>Generate Pyhton code that Add the arguments fo...</td>\n",
       "      <td>0.334405</td>\n",
       "      <td>Generate Pyhton code that Add the arguments fo...</td>\n",
       "      <td>0.325581</td>\n",
       "      <td>Generate Pyhton code that Add the arguments fo...</td>\n",
       "      <td>0.343548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Locking should include hashes for *all* platfo...</td>\n",
       "      <td>Generate Pyhton code that Locking should inclu...</td>\n",
       "      <td>0.248555</td>\n",
       "      <td>Generate Pyhton code that Locking should inclu...</td>\n",
       "      <td>0.254360</td>\n",
       "      <td>Generate Pyhton code that Locking should inclu...</td>\n",
       "      <td>0.257576</td>\n",
       "      <td>Generate Pyhton code that Locking should inclu...</td>\n",
       "      <td>0.273921</td>\n",
       "      <td>Generate Pyhton code that Locking should inclu...</td>\n",
       "      <td>...</td>\n",
       "      <td>Generate Pyhton code that Locking should inclu...</td>\n",
       "      <td>0.269108</td>\n",
       "      <td>Generate Pyhton code that Locking should inclu...</td>\n",
       "      <td>0.262270</td>\n",
       "      <td>Generate Pyhton code that Locking should inclu...</td>\n",
       "      <td>0.262519</td>\n",
       "      <td>Generate Pyhton code that Locking should inclu...</td>\n",
       "      <td>0.275081</td>\n",
       "      <td>Generate Pyhton code that Locking should inclu...</td>\n",
       "      <td>0.227213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Return the wind_speed for backward compatibili...</td>\n",
       "      <td>Generate Pyhton code that Return the wind_spee...</td>\n",
       "      <td>0.265306</td>\n",
       "      <td>Generate Pyhton code that Return the wind_spee...</td>\n",
       "      <td>0.240057</td>\n",
       "      <td>Generate Pyhton code that Return the wind_spee...</td>\n",
       "      <td>0.260597</td>\n",
       "      <td>Generate Pyhton code that Return the wind_spee...</td>\n",
       "      <td>0.267227</td>\n",
       "      <td>Generate Pyhton code that Return the wind_spee...</td>\n",
       "      <td>...</td>\n",
       "      <td>Generate Pyhton code that Return the wind_spee...</td>\n",
       "      <td>0.289982</td>\n",
       "      <td>Generate Pyhton code that Return the wind_spee...</td>\n",
       "      <td>0.252239</td>\n",
       "      <td>Generate Pyhton code that Return the wind_spee...</td>\n",
       "      <td>0.304432</td>\n",
       "      <td>Generate Pyhton code that Return the wind_spee...</td>\n",
       "      <td>0.234586</td>\n",
       "      <td>Generate Pyhton code that Return the wind_spee...</td>\n",
       "      <td>0.250377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Return the visibility for backward compatibili...</td>\n",
       "      <td>Generate Pyhton code that Return the visibilit...</td>\n",
       "      <td>0.235905</td>\n",
       "      <td>Generate Pyhton code that Return the visibilit...</td>\n",
       "      <td>0.246032</td>\n",
       "      <td>Generate Pyhton code that Return the visibilit...</td>\n",
       "      <td>0.223979</td>\n",
       "      <td>Generate Pyhton code that Return the visibilit...</td>\n",
       "      <td>0.268750</td>\n",
       "      <td>Generate Pyhton code that Return the visibilit...</td>\n",
       "      <td>...</td>\n",
       "      <td>Generate Pyhton code that Return the visibilit...</td>\n",
       "      <td>0.225585</td>\n",
       "      <td>Generate Pyhton code that Return the visibilit...</td>\n",
       "      <td>0.193838</td>\n",
       "      <td>Generate Pyhton code that Return the visibilit...</td>\n",
       "      <td>0.217277</td>\n",
       "      <td>Generate Pyhton code that Return the visibilit...</td>\n",
       "      <td>0.219645</td>\n",
       "      <td>Generate Pyhton code that Return the visibilit...</td>\n",
       "      <td>0.195274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Whitelister.clean should remove disallowed tag...</td>\n",
       "      <td>Generate Pyhton code that Whitelister.clean sh...</td>\n",
       "      <td>0.272346</td>\n",
       "      <td>Generate Pyhton code that Whitelister.clean sh...</td>\n",
       "      <td>0.297721</td>\n",
       "      <td>Generate Pyhton code that Whitelister.clean sh...</td>\n",
       "      <td>0.299248</td>\n",
       "      <td>Generate Pyhton code that Whitelister.clean sh...</td>\n",
       "      <td>0.273598</td>\n",
       "      <td>Generate Pyhton code that Whitelister.clean sh...</td>\n",
       "      <td>...</td>\n",
       "      <td>Generate Pyhton code that Whitelister.clean sh...</td>\n",
       "      <td>0.308397</td>\n",
       "      <td>Generate Pyhton code that Whitelister.clean sh...</td>\n",
       "      <td>0.307937</td>\n",
       "      <td>Generate Pyhton code that Whitelister.clean sh...</td>\n",
       "      <td>0.288824</td>\n",
       "      <td>Generate Pyhton code that Whitelister.clean sh...</td>\n",
       "      <td>0.286885</td>\n",
       "      <td>Generate Pyhton code that Whitelister.clean sh...</td>\n",
       "      <td>0.298197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Building a Job twice from should return differ...</td>\n",
       "      <td>Generate Pyhton code that Building a Job twice...</td>\n",
       "      <td>0.312782</td>\n",
       "      <td>Generate Pyhton code that Building a Job twice...</td>\n",
       "      <td>0.305866</td>\n",
       "      <td>Generate Pyhton code that Building a Job twice...</td>\n",
       "      <td>0.289474</td>\n",
       "      <td>Generate Pyhton code that Building a Job twice...</td>\n",
       "      <td>0.340764</td>\n",
       "      <td>Generate Pyhton code that Building a Job twice...</td>\n",
       "      <td>...</td>\n",
       "      <td>Generate Pyhton code that Building a Job twice...</td>\n",
       "      <td>0.302290</td>\n",
       "      <td>Generate Pyhton code that Building a Job twice...</td>\n",
       "      <td>0.342282</td>\n",
       "      <td>Generate Pyhton code that Building a Job twice...</td>\n",
       "      <td>0.326748</td>\n",
       "      <td>Generate Pyhton code that Building a Job twice...</td>\n",
       "      <td>0.306864</td>\n",
       "      <td>Generate Pyhton code that Building a Job twice...</td>\n",
       "      <td>0.286331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Return whether *point* (pair of pixel coordina...</td>\n",
       "      <td>Generate Pyhton code that Return whether *poin...</td>\n",
       "      <td>0.219287</td>\n",
       "      <td>Generate Pyhton code that Return whether *poin...</td>\n",
       "      <td>0.218579</td>\n",
       "      <td>Generate Pyhton code that Return whether *poin...</td>\n",
       "      <td>0.276740</td>\n",
       "      <td>Generate Pyhton code that Return whether *poin...</td>\n",
       "      <td>0.255162</td>\n",
       "      <td>Generate Pyhton code that Return whether *poin...</td>\n",
       "      <td>...</td>\n",
       "      <td>Generate Pyhton code that Return whether *poin...</td>\n",
       "      <td>0.260000</td>\n",
       "      <td>Generate Pyhton code that Return whether *poin...</td>\n",
       "      <td>0.311284</td>\n",
       "      <td>Generate Pyhton code that Return whether *poin...</td>\n",
       "      <td>0.245487</td>\n",
       "      <td>Generate Pyhton code that Return whether *poin...</td>\n",
       "      <td>0.281034</td>\n",
       "      <td>Generate Pyhton code that Return whether *poin...</td>\n",
       "      <td>0.269592</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         ground_truth  \\\n",
       "0   Check if the grid client is up.\\n\\n    Check f...   \n",
       "1   Ensure that powershell processes inline script...   \n",
       "2   Encode a bytestring to a base64 string for use...   \n",
       "3   Add the arguments for the protocol to the clie...   \n",
       "4   Locking should include hashes for *all* platfo...   \n",
       "..                                                ...   \n",
       "95  Return the wind_speed for backward compatibili...   \n",
       "96  Return the visibility for backward compatibili...   \n",
       "97  Whitelister.clean should remove disallowed tag...   \n",
       "98  Building a Job twice from should return differ...   \n",
       "99  Return whether *point* (pair of pixel coordina...   \n",
       "\n",
       "                                            outcome_0     lev_0  \\\n",
       "0   Generate Pyhton code that Check if the grid cl...  0.224626   \n",
       "1   Generate Pyhton code that Ensure that powershe...  0.322621   \n",
       "2   Generate Pyhton code that Encode a bytestring ...  0.257911   \n",
       "3   Generate Pyhton code that Add the arguments fo...  0.328746   \n",
       "4   Generate Pyhton code that Locking should inclu...  0.248555   \n",
       "..                                                ...       ...   \n",
       "95  Generate Pyhton code that Return the wind_spee...  0.265306   \n",
       "96  Generate Pyhton code that Return the visibilit...  0.235905   \n",
       "97  Generate Pyhton code that Whitelister.clean sh...  0.272346   \n",
       "98  Generate Pyhton code that Building a Job twice...  0.312782   \n",
       "99  Generate Pyhton code that Return whether *poin...  0.219287   \n",
       "\n",
       "                                            outcome_1     lev_1  \\\n",
       "0   Generate Pyhton code that Check if the grid cl...  0.192362   \n",
       "1   Generate Pyhton code that Ensure that powershe...  0.326347   \n",
       "2   Generate Pyhton code that Encode a bytestring ...  0.255556   \n",
       "3   Generate Pyhton code that Add the arguments fo...  0.326996   \n",
       "4   Generate Pyhton code that Locking should inclu...  0.254360   \n",
       "..                                                ...       ...   \n",
       "95  Generate Pyhton code that Return the wind_spee...  0.240057   \n",
       "96  Generate Pyhton code that Return the visibilit...  0.246032   \n",
       "97  Generate Pyhton code that Whitelister.clean sh...  0.297721   \n",
       "98  Generate Pyhton code that Building a Job twice...  0.305866   \n",
       "99  Generate Pyhton code that Return whether *poin...  0.218579   \n",
       "\n",
       "                                            outcome_2     lev_2  \\\n",
       "0   Generate Pyhton code that Check if the grid cl...  0.243636   \n",
       "1   Generate Pyhton code that Ensure that powershe...  0.325524   \n",
       "2   Generate Pyhton code that Encode a bytestring ...  0.258224   \n",
       "3   Generate Pyhton code that Add the arguments fo...  0.343606   \n",
       "4   Generate Pyhton code that Locking should inclu...  0.257576   \n",
       "..                                                ...       ...   \n",
       "95  Generate Pyhton code that Return the wind_spee...  0.260597   \n",
       "96  Generate Pyhton code that Return the visibilit...  0.223979   \n",
       "97  Generate Pyhton code that Whitelister.clean sh...  0.299248   \n",
       "98  Generate Pyhton code that Building a Job twice...  0.289474   \n",
       "99  Generate Pyhton code that Return whether *poin...  0.276740   \n",
       "\n",
       "                                            outcome_3     lev_3  \\\n",
       "0   Generate Pyhton code that Check if the grid cl...  0.227273   \n",
       "1   Generate Pyhton code that Ensure that powershe...  0.327132   \n",
       "2   Generate Pyhton code that Encode a bytestring ...  0.275510   \n",
       "3   Generate Pyhton code that Add the arguments fo...  0.333333   \n",
       "4   Generate Pyhton code that Locking should inclu...  0.273921   \n",
       "..                                                ...       ...   \n",
       "95  Generate Pyhton code that Return the wind_spee...  0.267227   \n",
       "96  Generate Pyhton code that Return the visibilit...  0.268750   \n",
       "97  Generate Pyhton code that Whitelister.clean sh...  0.273598   \n",
       "98  Generate Pyhton code that Building a Job twice...  0.340764   \n",
       "99  Generate Pyhton code that Return whether *poin...  0.255162   \n",
       "\n",
       "                                            outcome_4  ...  \\\n",
       "0   Generate Pyhton code that Check if the grid cl...  ...   \n",
       "1   Generate Pyhton code that Ensure that powershe...  ...   \n",
       "2   Generate Pyhton code that Encode a bytestring ...  ...   \n",
       "3   Generate Pyhton code that Add the arguments fo...  ...   \n",
       "4   Generate Pyhton code that Locking should inclu...  ...   \n",
       "..                                                ...  ...   \n",
       "95  Generate Pyhton code that Return the wind_spee...  ...   \n",
       "96  Generate Pyhton code that Return the visibilit...  ...   \n",
       "97  Generate Pyhton code that Whitelister.clean sh...  ...   \n",
       "98  Generate Pyhton code that Building a Job twice...  ...   \n",
       "99  Generate Pyhton code that Return whether *poin...  ...   \n",
       "\n",
       "                                           outcome_25    lev_25  \\\n",
       "0   Generate Pyhton code that Check if the grid cl...  0.222586   \n",
       "1   Generate Pyhton code that Ensure that powershe...  0.345768   \n",
       "2   Generate Pyhton code that Encode a bytestring ...  0.227596   \n",
       "3   Generate Pyhton code that Add the arguments fo...  0.385366   \n",
       "4   Generate Pyhton code that Locking should inclu...  0.269108   \n",
       "..                                                ...       ...   \n",
       "95  Generate Pyhton code that Return the wind_spee...  0.289982   \n",
       "96  Generate Pyhton code that Return the visibilit...  0.225585   \n",
       "97  Generate Pyhton code that Whitelister.clean sh...  0.308397   \n",
       "98  Generate Pyhton code that Building a Job twice...  0.302290   \n",
       "99  Generate Pyhton code that Return whether *poin...  0.260000   \n",
       "\n",
       "                                           outcome_26    lev_26  \\\n",
       "0   Generate Pyhton code that Check if the grid cl...  0.217252   \n",
       "1   Generate Pyhton code that Ensure that powershe...  0.326271   \n",
       "2   Generate Pyhton code that Encode a bytestring ...  0.253756   \n",
       "3   Generate Pyhton code that Add the arguments fo...  0.316712   \n",
       "4   Generate Pyhton code that Locking should inclu...  0.262270   \n",
       "..                                                ...       ...   \n",
       "95  Generate Pyhton code that Return the wind_spee...  0.252239   \n",
       "96  Generate Pyhton code that Return the visibilit...  0.193838   \n",
       "97  Generate Pyhton code that Whitelister.clean sh...  0.307937   \n",
       "98  Generate Pyhton code that Building a Job twice...  0.342282   \n",
       "99  Generate Pyhton code that Return whether *poin...  0.311284   \n",
       "\n",
       "                                           outcome_27    lev_27  \\\n",
       "0   Generate Pyhton code that Check if the grid cl...  0.245931   \n",
       "1   Generate Pyhton code that Ensure that powershe...  0.364109   \n",
       "2   Generate Pyhton code that Encode a bytestring ...  0.224818   \n",
       "3   Generate Pyhton code that Add the arguments fo...  0.334405   \n",
       "4   Generate Pyhton code that Locking should inclu...  0.262519   \n",
       "..                                                ...       ...   \n",
       "95  Generate Pyhton code that Return the wind_spee...  0.304432   \n",
       "96  Generate Pyhton code that Return the visibilit...  0.217277   \n",
       "97  Generate Pyhton code that Whitelister.clean sh...  0.288824   \n",
       "98  Generate Pyhton code that Building a Job twice...  0.326748   \n",
       "99  Generate Pyhton code that Return whether *poin...  0.245487   \n",
       "\n",
       "                                           outcome_28    lev_28  \\\n",
       "0   Generate Pyhton code that Check if the grid cl...  0.241071   \n",
       "1   Generate Pyhton code that Ensure that powershe...  0.331646   \n",
       "2   Generate Pyhton code that Encode a bytestring ...  0.229412   \n",
       "3   Generate Pyhton code that Add the arguments fo...  0.325581   \n",
       "4   Generate Pyhton code that Locking should inclu...  0.275081   \n",
       "..                                                ...       ...   \n",
       "95  Generate Pyhton code that Return the wind_spee...  0.234586   \n",
       "96  Generate Pyhton code that Return the visibilit...  0.219645   \n",
       "97  Generate Pyhton code that Whitelister.clean sh...  0.286885   \n",
       "98  Generate Pyhton code that Building a Job twice...  0.306864   \n",
       "99  Generate Pyhton code that Return whether *poin...  0.281034   \n",
       "\n",
       "                                           outcome_29    lev_29  \n",
       "0   Generate Pyhton code that Check if the grid cl...  0.198839  \n",
       "1   Generate Pyhton code that Ensure that powershe...  0.306836  \n",
       "2   Generate Pyhton code that Encode a bytestring ...  0.260450  \n",
       "3   Generate Pyhton code that Add the arguments fo...  0.343548  \n",
       "4   Generate Pyhton code that Locking should inclu...  0.227213  \n",
       "..                                                ...       ...  \n",
       "95  Generate Pyhton code that Return the wind_spee...  0.250377  \n",
       "96  Generate Pyhton code that Return the visibilit...  0.195274  \n",
       "97  Generate Pyhton code that Whitelister.clean sh...  0.298197  \n",
       "98  Generate Pyhton code that Building a Job twice...  0.286331  \n",
       "99  Generate Pyhton code that Return whether *poin...  0.269592  \n",
       "\n",
       "[100 rows x 61 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docstring_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from pathlib import Path\n",
    "params_dict = param_default()\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\", datefmt=\"%m/%d/%Y %H:%M:%S\", level=logging.INFO, filename=params_dict['log_path']\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "docst_codebleu_df = pd.DataFrame()\n",
    "samples=30\n",
    "for i in range(0,samples):\n",
    "    bleuScore, codebleuScore = calculate_bleu_codeBleu(lang,params,docstring_df,'ground_truth','outcome_'+str(i),keywords)\n",
    "    docst_codebleu_df.loc[i,'docst_bleu'] = bleuScore\n",
    "    docst_codebleu_df.loc[i,'docst_codebleu'] = codebleuScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "docst_bleu        0.178065\n",
       "docst_codebleu    0.355168\n",
       "dtype: float64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docst_codebleu_df.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,samples):\n",
    "    bleuScore, codebleuScore = calculate_bleu_codeBleu(lang,params,lev_rand_code_df,'ground_truth','outcome_'+str(i),keywords)\n",
    "    docst_codebleu_df.loc[i,'rancut_bleu'] = bleuScore\n",
    "    docst_codebleu_df.loc[i,'rancut_codebleu'] = codebleuScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,samples):\n",
    "    bleuScore, codebleuScore = calculate_bleu_codeBleu(lang,params,lev_signature_df,'ground_truth','outcome_'+str(i),keywords)\n",
    "    docst_codebleu_df.loc[i,'doc_sig_bleu'] = bleuScore\n",
    "    docst_codebleu_df.loc[i,'doc_sig_codebleu'] = codebleuScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,samples):\n",
    "    bleuScore, codebleuScore = calculate_bleu_codeBleu(lang,params,lev_rand_df,'ground_truth','outcome_'+str(i),keywords)\n",
    "    docst_codebleu_df.loc[i,'doc_ran_bleu'] = bleuScore\n",
    "    docst_codebleu_df.loc[i,'doc_ran_codebleu'] = codebleuScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "docst_bleu          0.178065\n",
       "docst_codebleu      0.355168\n",
       "rancut_bleu         0.318113\n",
       "rancut_codebleu     0.531193\n",
       "doc_sig_bleu        0.230144\n",
       "doc_sig_codebleu    0.429848\n",
       "doc_ran_bleu        0.358623\n",
       "doc_ran_codebleu    0.573638\n",
       "dtype: float64"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docst_codebleu_df.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
