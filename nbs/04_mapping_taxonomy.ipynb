{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mapping Function\n",
    "> Labels a given token following a tailored taxonomy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (2.0.2) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n",
      "2023-07-17 13:37:14.601094: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-17 13:37:14.754445: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer\n",
    "import json\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from code_rationales.loader import download_grammars\n",
    "from tree_sitter import Language, Parser\n",
    "import code_rationales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Java Taxonomy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Programming Language Taxonomy\n",
    "def pl_taxonomy_java() -> dict:\n",
    "    return {\n",
    "  \"parenthesis\": { #Category-Level Label\n",
    "    \"<{>\": \"{\", #Token-Level Label\n",
    "    \"<}>\": \"}\",\n",
    "    \"<[>\": \"[\",\n",
    "    \"<]>\": \"]\",\n",
    "    \"<(>\": \"(\",\n",
    "    \"<)>\": \")\"    \n",
    "  },\n",
    "  \"semi_colon\":{\n",
    "    \"<;>\": \";\",\n",
    "    \"<:>\": \":\"\n",
    "  },\n",
    "  \"comma_dot\":{\n",
    "    \"<,>\": \",\",\n",
    "    \"<.>\": \".\",\n",
    "    \"<...>\": \"...\"\n",
    "  },\n",
    "  \"exceptions\": {\n",
    "    \"<catch>\": \"catch\",\n",
    "    \"<try>\": \"try\",\n",
    "    \"<finally>\": \"finally\",\n",
    "    \"<throw>\": \"throw\",\n",
    "    \"<throws>\": \"throws\"\n",
    "  },\n",
    "  \"oop\": {\n",
    "    \"<class>\": \"class\",\n",
    "    \"<instanceof>\": \"instanceof\",\n",
    "    \"<interface>\": \"interface\",\n",
    "    \"<private>\": \"private\",\n",
    "    \"<protected>\": \"protected\",\n",
    "    \"<public>\": \"public\",\n",
    "    \"<abstract>\": \"abstract\",\n",
    "    \"<extends>\": \"extends\",\n",
    "    \"<package>\": \"package\",\n",
    "    \"<this>\": \"this\",\n",
    "    \"<implements>\": \"implements\",\n",
    "    \"<import>\": \"import\",\n",
    "    \"<new>\": \"new\",\n",
    "    \"<super>\": \"super\"\n",
    "  },\n",
    "  \"asserts\": {\n",
    "    \"<assert>\": \"assert\"\n",
    "  },\n",
    "  \"types\": {\n",
    "    \"<native>\": \"native\",\n",
    "    \"<static>\": \"static\",\n",
    "    \"<synchronized>\": \"synchronized\",\n",
    "    \"<transient>\": \"transient\",\n",
    "    \"<volatile>\": \"volatile\",\n",
    "    \"<void>\": \"void\",\n",
    "    \"<final>\": \"final\",\n",
    "    \"<enum>\": \"enum\",\n",
    "    \"<byte>\": \"byte\",\n",
    "    \"<char>\": \"char\",\n",
    "    \"<float>\": \"float\",\n",
    "    \"<boolean>\": \"boolean\",\n",
    "    \"<double>\": \"double\",\n",
    "    \"<int>\": \"int\",\n",
    "    \"<long>\": \"long\",\n",
    "    \"<short>\": \"short\",\n",
    "    \"<strictfp>\": \"strictfp\"\n",
    "  },\n",
    "  \"conditionals\": {\n",
    "    \"<else>\": \"else\",\n",
    "    \"<if>\": \"if\",\n",
    "    \"<switch>\": \"switch\",\n",
    "    \"<case>\": \"case\",\n",
    "    \"<default>\": \"default\"\n",
    "  },\n",
    "  \"loops\": {\n",
    "    \"<break>\": \"break\",\n",
    "    \"<do>\": \"do\",\n",
    "    \"<for>\": \"for\",\n",
    "    \"<while>\": \"while\",\n",
    "    \"<continue>\": \"continue\"\n",
    "  },\n",
    "  \"operators\": {\n",
    "    \"<=>\": \"=\",\n",
    "    \"<+>\": \"+\",\n",
    "    \"<->\": \"-\",\n",
    "    \"<*>\": \"*\",\n",
    "    \"</>\": \"/\",\n",
    "    \"<%>\": \"%\",\n",
    "    \"<++>\": \"++\",\n",
    "    \"<-->\": \"--\",\n",
    "    \"<!>\": \"!\",\n",
    "    \"<==>\": \"==\",\n",
    "    \"<!=>\": \"!=\",\n",
    "    \"<greater_equal>\": \">=\",\n",
    "    \"<lesser_equal>\": \"<=\",\n",
    "    \"<&&>\": \"&&\",\n",
    "    \"<||>\": \"||\",\n",
    "    \"<?>\": \"?\",\n",
    "    \"<:>\": \":\",\n",
    "    \"<~>\": \"~\",\n",
    "    \"<double_lesser>\": \"<<\",\n",
    "    \"<double_greater>\": \">>\",\n",
    "    \"<triple_greater>\": \">>>\",\n",
    "    \"<&>\": \"&\",\n",
    "    \"<^>\": \"^\",\n",
    "    \"<|>\": \"|\"\n",
    "  },\n",
    "  \"newline\": {\n",
    "    \"<n>\": \"\\n\"\n",
    "  },\n",
    "  \"tab\": {\n",
    "    \"<t>\": \"\\t\"\n",
    "  },\n",
    "  \"ampersand\": {\n",
    "    \"<@>\": \"@\"\n",
    "  },\n",
    "  \"bool\": {\n",
    "    \"<true>\": \"true\",\n",
    "    \"<false>\": \"false\",\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_from_preprocessed_java( preprocessed_sequence ) -> list:\n",
    "    a = preprocessed_sequence\n",
    "    return list(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Taxonomy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Programming Language Taxonomy\n",
    "def pl_taxonomy_python() -> dict:\n",
    "    return {\n",
    "  \"parenthesis\": ['{', '}', '[', ']', '(', ')'], \n",
    "  \"exceptions\": ['raise_statement','catch', 'try', 'finally', 'throw', 'throws', 'except'],\n",
    "  \"oop\": ['def','class','instanceof','interface','private','protected','public','abstract','extends','package','this','implements','import','new','super'],\n",
    "  \"asserts\": ['assert'],\n",
    "  \"types\": ['pair','subscript','type','none','dictionary','integer','native','static','synchronized','transient','volatile','void','final','enum','byte','char','float','boolean','double','int','long','short','strictfp'],\n",
    "  \"conditionals\": ['else', 'if', 'switch', 'case', 'default'],\n",
    "  \"loops\": ['break', 'do', 'for', 'while', 'continue'],\n",
    "  \"operators\": ['or','not','**','slice','%','+','<','>','=','+','-','*','/','%','++','--','!','==','!=','>=','<=','&&','||','?',':','~','<<','>>','>>>','&','^','|'],\n",
    "  \"newline\": ['\\n'],\n",
    "  \"tab\": ['\\t'],\n",
    "  \"ampersand\": ['@'],\n",
    "  \"bool\": ['true', 'false'], \n",
    "  \"string\": ['string'], \n",
    "  \"punctuation\" : ['\\\"', ',', '.', '...',']', ';', ':'], \n",
    "  \"with\" : ['with'], \n",
    "  \"structural\" : ['module', 'argument_list', 'lambda_parameters'],\n",
    "  \"statements\" : ['return']\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'pair' in pl_taxonomy_python()['types'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AST Mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_rationals = pd.read_csv('/workspaces/code-rationales/data/rationales/gpt/testing/[t_100]_[max_tgt_44]_[exp:0]_.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_rationals = df_rationals[df_rationals['from_seq_id'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Retrieve the generated output\n",
    "#initial_token = eval(df_rationals['typesets_tgt'][0])[0][0]\n",
    "#code = initial_token + ''.join(df_rationals['goal_token'])\n",
    "#code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Add Span column\n",
    "#calculate_left_span = lambda index : len(initial_token + ''.join(df_rationals['goal_token'][:index]))\n",
    "#calculate_right_span = lambda left_span, token : len(left_span) + len(token)\n",
    "#span_col = list(map(lambda tuple: (tuple[0],tuple[0]+len(tuple[1])),[(calculate_left_span(index),token) for index, token in df_rationals['goal_token'].items()]))\n",
    "#df_rationals.insert(loc=df_rationals.columns.get_loc('goal_token')+1, column='span', value=span_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_rationals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map Tokens with Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/code_rationales/grammars\n"
     ]
    }
   ],
   "source": [
    "languages=['python', 'java']\n",
    "download_grammars(languages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unroll_node_types(\n",
    "    nested_node_types: dict  # node_types from tree-sitter\n",
    ") -> list: # list of node types\n",
    "    def iterate_and_unroll_dict(nested_node_types: dict, all_node_types: set):\n",
    "        for key, value in nested_node_types.items():\n",
    "            if key == 'type' and type(value) == str:\n",
    "                all_node_types.add(value)\n",
    "            if type(value) == dict:\n",
    "                iterate_and_unroll_dict(value, all_node_types)\n",
    "            if type(value) == list:\n",
    "                for element in value:\n",
    "                    iterate_and_unroll_dict(element, all_node_types) \n",
    "    all_node_types = set()\n",
    "    for dictionary in nested_node_types:\n",
    "        iterate_and_unroll_dict(dictionary, all_node_types)\n",
    "    all_node_types.add('ERROR')\n",
    "    return list(all_node_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_parser(lang: str):\n",
    "    # Grab the node types from the tree-sitter language\n",
    "    language = Language(f\"{code_rationales.__path__[0]}/grammars/tree-sitter-languages.so\", lang)\n",
    "    node_path = f\"{code_rationales.__path__[0]}/grammars/tree-sitter-{lang}/src/node-types.json\"\n",
    "    with open(node_path) as f:\n",
    "            node_types = json.load(f)\n",
    "    node_types = unroll_node_types(node_types)\n",
    "    # Create a parser for the language\n",
    "    parser = Parser()\n",
    "    parser.set_language(language)\n",
    "    return parser, node_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def traverse(\n",
    "    node,       # tree-sitter node\n",
    ") -> None:\n",
    "    \"\"\"Traverse in a recursive way, a tree-sitter node and append results to a list.\"\"\"\n",
    "    results = []\n",
    "    def traverse_tree(node, results):\n",
    "        if node.type == 'string':\n",
    "            results.append(node)\n",
    "            return\n",
    "        for n in node.children:\n",
    "            traverse_tree(n, results)\n",
    "        if not node.children:\n",
    "            results.append(node)\n",
    "    traverse_tree(node, results)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_offset(\n",
    "    point,              #point to convert\n",
    "    lines: list         #list of lines in the source code\n",
    "    ):\n",
    "        \"\"\"Convert the point to an offset\"\"\"\n",
    "        row, column = point\n",
    "        chars_in_rows = sum(map(len, lines[:row])) + row\n",
    "        chars_in_columns = len(lines[row][:column])\n",
    "        offset = chars_in_rows + chars_in_columns\n",
    "        return offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_node_span(node, lines):\n",
    "    \"\"\"Get the span position of the node in the code string\"\"\"\n",
    "    start_span = convert_to_offset(node.start_point, lines)\n",
    "    end_span = convert_to_offset(node.end_point, lines)\n",
    "    return start_span, end_span\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_type(\n",
    "    tok_span: tuple, # (start, end) position of a token in tokenizer\n",
    "    nodes: list,     # list of tree-sitter nodes\n",
    "    lines: list,     # list of lines in the code\n",
    ") -> tuple: # (parent_type, token_type) of the token\n",
    "    \"\"\"Get the parent AST type and token AST type of a token.\"\"\"\n",
    "    node_spans = [get_node_span(node, lines) for node in nodes]\n",
    "    for i, span in enumerate(node_spans):\n",
    "        if (span[0] <= tok_span[0] and tok_span[0] < span[1]) or (span[0] < tok_span[1] and tok_span[1] <= span[1]):\n",
    "            return nodes[i].parent.type, nodes[i].type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_nodes(\n",
    "    tok_span: tuple, # (start, end) position of a token in tokenizer\n",
    "    node,            # tree-sitter node\n",
    "    lines: list,     # list of lines in the code\n",
    ") -> list: \n",
    "    \"\"\"Get all AST types for the given token span\"\"\"\n",
    "    results = []\n",
    "    def traverse_and_get_types(tok_span, node, lines, results) -> None:\n",
    "        node_span = get_node_span(node, lines)\n",
    "        if (node_span[0] <= tok_span[0] and tok_span[0] < node_span[1]) or (node_span[0] < tok_span[1] and tok_span[1] <= node_span[1]):\n",
    "            results.append(node.type)\n",
    "        for n in node.children:\n",
    "            traverse_and_get_types(tok_span, n, lines, results)\n",
    "    traverse_and_get_types(tok_span, node, lines, results)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parser, node_types = create_parser('python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nodes = traverse(parser.parse(bytes(code, 'utf8')).root_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(get_token_type(df_rationals['span'][40], nodes, code.split(\"\\n\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(get_token_nodes(df_rationals['span'][42], parser.parse(bytes(code, 'utf8')).root_node, code.split(\"\\n\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(eval(df_rationals['rationale_pos_tgt'][2]))\n",
    "#print(eval(df_rationals['rationale_prob_tgt'][2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(df_rationals['goal_token'][eval(df_rationals['rationale_pos_tgt'][2])[0]-1])\n",
    "#print(df_rationals['span'][eval(df_rationals['rationale_pos_tgt'][2])[0]-1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Rational Global Aggregates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def param_default():\n",
    "    return {\n",
    "        #'dataset' : 'code_completion_random_cut_5k_30_512_tokens',\n",
    "        'dataset' : 'code_completion_docstring_random_cut_3.8k_30_150_tokens',\n",
    "        #'dataset' : 'code_completion_docstring_signature_3.8k_30_150_tokens',\n",
    "        #'dataset' : 'code_completion_docstring_5k_30_150_tokens',\n",
    "        'rational_results': '/workspaces/code-rationales/data/rationales/gpt',\n",
    "        'global_results': '/workspaces/code-rationales/data/global_results/gpt',\n",
    "        'num_samples' : 100, \n",
    "        'size_samples' : 146,\n",
    "        'num_experiments': 30, \n",
    "        'bootstrapping' : 500\n",
    "    }\n",
    "params = param_default()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_experiment_path =  lambda samples, size, exp: params['rational_results'] + '/' + params['dataset'] + '/' + '[t_'+str(samples)+']_[max_tgt_'+str(size)+']_[exp:'+str(exp)+']_.csv'\n",
    "calculate_left_span = lambda index, initial_token, df_rationals : len(str(initial_token) + ''.join(str(df_rationals['goal_token'][:index])))\n",
    "calculate_right_span = lambda left_span, token : len(left_span) + len(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Retrieve experiments\n",
    "experiment_paths = [get_experiment_path(params['num_samples'], params['size_samples'], exp) for exp in range(params['num_experiments'])]\n",
    "### Define parser\n",
    "parser, node_types = create_parser('python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_rationals(experiment_paths: list, parser, node_types: list):\n",
    "    global_results = {node_type : {node_type : [] for node_type in node_types} for node_type in node_types}\n",
    "    for exp_idx, experiment_path in enumerate(experiment_paths):\n",
    "        df_experiment = pd.read_csv(experiment_path, index_col=0)\n",
    "        experiment_rational_results = [df_experiment[(df_experiment['from_seq_id'] == sample_idx) | (df_experiment['from_seq_id'] == str(sample_idx))].reset_index() for sample_idx in range(params['num_samples'])]\n",
    "        print('*'*10 +'Aggregating rationales for exp: ' +str(exp_idx) + '*'*10)\n",
    "        for experiment_rational_result in experiment_rational_results:\n",
    "            initial_token = eval(experiment_rational_result['typesets_tgt'][0])[0][0]\n",
    "            experiment_rational_result.insert(loc=experiment_rational_result.columns.get_loc('goal_token')+1, column='span', value=list(map(lambda tuple: (tuple[0],tuple[0]+len(tuple[1])),[(calculate_left_span(index, initial_token, experiment_rational_result), str(token)) for index, token in experiment_rational_result['goal_token'].items()])))\n",
    "            target_code = eval(experiment_rational_result['typesets_tgt'][0])[0][0] + ''.join(str(experiment_rational_result['goal_token']))\n",
    "            target_ast = parser.parse(bytes(target_code, 'utf8')).root_node\n",
    "            for target_token_idx in range(len(experiment_rational_result['span'])):\n",
    "                target_node_types = get_token_nodes(experiment_rational_result['span'][target_token_idx], target_ast, target_code.split(\"\\n\"))\n",
    "                for rational_idx, rational_pos in enumerate(eval(experiment_rational_result['rationale_pos_tgt'][target_token_idx])):\n",
    "                    if eval(experiment_rational_result['rationale_pos_tgt'][target_token_idx])[rational_idx] > 0: #rational 1 position.\n",
    "                        try:\n",
    "                            rational_node_types = get_token_nodes(experiment_rational_result['span'][eval(experiment_rational_result['rationale_pos_tgt'][target_token_idx])[rational_idx]-1], target_ast, target_code.split(\"\\n\"))\n",
    "                            [global_results[target_node_type][rational_node_type].append(eval(experiment_rational_result['rationale_prob_tgt'][target_token_idx])[rational_idx]) for target_node_type in target_node_types for rational_node_type in rational_node_types]\n",
    "                        except Exception as e:\n",
    "                            print('rational pos out of range')\n",
    "    return global_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_global_results(global_results):\n",
    "    def clean_dictonary(result_dict):\n",
    "        clean_dict = result_dict.copy()\n",
    "        for key, value in result_dict.items():\n",
    "            if not value:\n",
    "                clean_dict.pop(key)\n",
    "        return clean_dict\n",
    "    for key, value in global_results.items():\n",
    "        global_results[key] = clean_dictonary(value)\n",
    "    return clean_dictonary(global_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrapping( np_data, np_func, size ):\n",
    "    \"\"\"Create a bootstrap sample given data and a function\n",
    "    For instance, a bootstrap sample of means, or mediands. \n",
    "    The bootstrap replicates are a long as the original size\n",
    "    we can choose any observation more than once (resampling with replacement:np.random.choice)\n",
    "    \"\"\"\n",
    "    \n",
    "    #Cleaning NaNs\n",
    "    #np_data_clean = np_data[ np.logical_not( np.isnan(np_data) ) ] \n",
    "    \n",
    "    #The size of the bootstrap replicate is as big as size\n",
    "    #Creating the boostrap replicates as long as the orignal data size\n",
    "    #This strategy might work as imputation \n",
    "    bootstrap_repl = [ np_func( np.random.choice( np_data, size=len(np_data) ) ) for i in range( size ) ]\n",
    "    \n",
    "    #logging.info(\"Covariate: \" + cov) #Empirical Mean\n",
    "    #logging.info(\"Empirical Mean: \" + str(np.mean(np_data_clean))) #Empirical Mean\n",
    "    #logging.info(\"Bootstrapped Mean: \" + str( np.mean(bootstrap_repl) ) ) #Bootstrapped Mean\n",
    "    \n",
    "    return np.array( bootstrap_repl )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_samples_global_results(global_results: dict, size: int):\n",
    "    for target_type, target_value in global_results.items():\n",
    "        for source_type, source_value in target_value.items():\n",
    "            global_results[target_type][source_type] = bootstrapping(source_value, np.mean, size).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********Aggregating rationales for exp: 0**********\n"
     ]
    }
   ],
   "source": [
    "### WARNING TAKES TIME\n",
    "global_results = clean_global_results(aggregate_rationals(experiment_paths, parser, node_types))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### WARNING TAKES TIME\n",
    "bootstrap_samples_global_results(global_results, params['bootstrapping'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open(params['global_results'] + '/' + params['dataset'] + '.txt', 'w') as file:\n",
    "#    file.write(json.dumps(global_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(params['global_results'] + '/' + params['dataset'] + '.txt', 'r') as file:\n",
    "    global_results = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">\n",
      "for\n",
      "except\n",
      "integer\n",
      "subscript\n",
      "*\n",
      "from\n",
      "dictionary\n",
      "<\n",
      "string\n",
      "\"\n",
      ":\n",
      "none\n",
      "+\n",
      "%\n",
      "if\n",
      "type\n",
      "slice\n",
      "raise_statement\n",
      ";\n",
      "with\n",
      "dictionary_splat\n",
      "def\n",
      "**\n",
      "assert\n",
      "|\n",
      "=\n",
      "<=\n",
      "not\n",
      ")\n",
      "module\n",
      "list_splat\n",
      "pair\n",
      "argument_list\n",
      "lambda_parameters\n",
      "{\n",
      "return\n",
      "or\n",
      "with_item\n",
      "with_statement\n",
      "block\n",
      "boolean_operator\n",
      "return_statement\n",
      "]\n",
      "comparison_operator\n",
      "escape_sequence\n",
      "false\n",
      "==\n",
      "list\n",
      "as_pattern\n",
      "set\n",
      "as_pattern_target\n",
      "}\n",
      "and\n",
      "ERROR\n",
      "binary_operator\n",
      "with_clause\n",
      "assert_statement\n",
      "in\n",
      "@\n",
      "[\n",
      "true\n",
      "unary_operator\n",
      "is\n",
      "exec\n",
      ",\n",
      "async\n",
      "yield\n",
      "class\n",
      "/\n",
      "tuple\n",
      "expression_statement\n",
      "while_statement\n",
      ".\n",
      "call\n",
      "ellipsis\n",
      "(\n",
      "as\n",
      "break_statement\n",
      "assignment\n",
      "//\n",
      "-\n",
      "parenthesized_expression\n",
      "pattern_list\n",
      "break\n",
      "identifier\n",
      "try\n",
      "comment\n",
      "while\n",
      "class_definition\n",
      "raise\n",
      "lambda\n",
      "attribute\n"
     ]
    }
   ],
   "source": [
    "#print(len(global_results.keys()))\n",
    "for key in global_results.keys():\n",
    "    print(key)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
