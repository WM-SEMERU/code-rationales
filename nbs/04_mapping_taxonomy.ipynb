{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mapping Function\n",
    "> Labels a given token following a tailored taxonomy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (2.0.2) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n",
      "2023-10-16 18:13:42.182123: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-16 18:13:42.338469: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from code_rationales.loader import download_grammars\n",
    "from tree_sitter import Language, Parser\n",
    "import code_rationales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package tagsets to /root/nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('tagsets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def param_default():\n",
    "    return {\n",
    "        #'dataset' : 'code_completion_random_cut_5k_30_512_tokens',\n",
    "        'dataset' : 'code_completion_docstring_random_cut_3.8k_30_150_tokens',\n",
    "        #'dataset' : 'code_completion_docstring_signature_3.8k_30_150_tokens',\n",
    "        #'dataset' : 'code_completion_docstring_5k_30_150_tokens',\n",
    "        'rational_results': '/workspaces/code-rationales/data/rationales/gpt',\n",
    "        'global_ast_results': '/workspaces/code-rationales/data/global_ast_results/gpt',\n",
    "        'global_taxonomy_results': '/workspaces/code-rationales/data/global_taxonomy_results/gpt',\n",
    "        'delimiter_sequence': 'and code starts with',\n",
    "        'num_samples' : 100, \n",
    "        'size_samples' : 146,\n",
    "        'num_experiments': 1, \n",
    "        'bootstrapping' : 500\n",
    "    }\n",
    "params = param_default()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taxonomy Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Programming Language Taxonomy\n",
    "def pl_taxonomy_python() -> dict:\n",
    "    return {\n",
    "  \"punctuation\": ['{', '}', '[', ']', '(', ')','\\\"', ',', '.', '...', ';', ':'], #NO SEMANTIC\n",
    "  \"exceptions\": ['raise_statement','catch', 'try', 'finally', 'throw', 'throws', 'except'], #SEMANTIC\n",
    "  \"oop\": ['def','class','instanceof','interface','private','protected','public','abstract','extends','package','this','implements','import','new','super'], #SEMANTIC\n",
    "  \"asserts\": ['assert'], #SEMANTIC\n",
    "  \"types\": ['tuple','set','list','pair','subscript','type','none','dictionary','integer','native','static','synchronized','transient','volatile','void','final','enum','byte','char','float','boolean','double','int','long','short','strictfp'], #SEMANTIC\n",
    "  \"conditionals\": ['else', 'if', 'switch', 'case', 'default'], #SEMANTIC\n",
    "  \"loops\": ['break', 'do', 'for', 'while', 'continue'], #SEMANTIC\n",
    "  \"operators\": ['as','yield','is','@','in','and','or','not','**','slice','%','+','<','>','=','+','-','*','/','%','++','--','!','==','!=','>=','<=','&&','||','?',':','~','<<','>>','>>>','&','^','|','//'],#NO SEMANTIC\n",
    "  \"indentation\": ['\\n','\\t', 'identation'],#NO SEMANTIC\n",
    "  \"bool\": ['true', 'false'], #SEMANTIC\n",
    "  \"functional\":['lambda','lambda_parameters'],#NO SEMANTIC\n",
    "  \"with\" : ['with','with_item','with_statement','with_clause'], #SEMANTIC\n",
    "  \"return\" :['return'],  #NO SEMANTIC\n",
    "  \"structural\" : ['attribute', 'argument_list','parenthesized_expression','pattern_list','class_definition','function_definition','block'], #SEMANTIC\n",
    "  \"statements\" : ['return_statement','break_statement','assignment','while_statement','expression_statement','assert_statement'],#SEMANTIC\n",
    "  \"expression\": ['call','exec','async','ellipsis','unary_operator','binary_operator','as_pattern_target','boolean_operator','as_pattern','comparison_operator','conditional_expression','named_expression','not_operator','primary_expression','as_pattern'], #NO SEMANTIC\n",
    "  \"errors\": [\"ERROR\"], #ERROR\n",
    "  \"identifier\":[\"identifier\"],  #NL\n",
    "  \"comment\":[\"comment\"], #NL\n",
    "  \"string\": ['string','interpolation','string_content','string_end','string_start','escape_sequence'], #NL\n",
    "  \"excluded\": ['module'], ### EXCLUDED CATEGORY\n",
    "  \"unknown\": []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nl_pos_taxonomy() -> dict: return {\n",
    "    \"nl_verb\" : ['VBN', 'VBG', 'VBZ', 'VBP', 'VBD', 'VB'],\n",
    "    \"nl_noun\" : ['NN', 'NNPS', 'NNS', 'NNP'],\n",
    "    \"nl_pronoun\" : ['WP', 'PRP', 'PRP$', 'WP','WP$'], \n",
    "    \"nl_adverb\" : ['RBS','RBR', 'RB', 'WRB'], \n",
    "    \"nl_adjetive\" : ['JJR', 'JJS', 'JJ'], \n",
    "    \"nl_determiner\" : ['DT','WDT','PDT'], \n",
    "    \"nl_preposition\" : ['IN', 'TO'],\n",
    "    \"nl_particle\" : ['RP'],\n",
    "    \"nl_modal\" : ['MD'],\n",
    "    \"nl_conjunction\" : ['CC'],\n",
    "    \"nl_cardinal\" : ['CD'],\n",
    "    \"nl_other\" : ['FW', 'EX', 'SYM' , 'UH', 'POS', \"''\", '--',':', '(', ')', '.', ',', '``', '$', 'LS']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LS:\n",
      "('list item marker', 'A A. B B. C C. D E F First G H I J K One SP-44001 SP-44002 SP-44005 SP-44007 Second Third Three Two * a b c d first five four one six three two ')\n",
      "Description: list item marker\n",
      "Example: A\n",
      "\n",
      "TO:\n",
      "('\"to\" as preposition or infinitive marker', 'to ')\n",
      "Description: \"to\" as preposition or infinitive marker\n",
      "Example: t\n",
      "\n",
      "VBN:\n",
      "('verb, past participle', 'multihulled dilapidated aerosolized chaired languished panelized used experimented flourished imitated reunifed factored condensed sheared unsettled primed dubbed desired ... ')\n",
      "Description: verb, past participle\n",
      "Example: m\n",
      "\n",
      "'':\n",
      "('closing quotation mark', \"' '' \")\n",
      "Description: closing quotation mark\n",
      "Example: '\n",
      "\n",
      "WP:\n",
      "('WH-pronoun', 'that what whatever whatsoever which who whom whosoever ')\n",
      "Description: WH-pronoun\n",
      "Example: t\n",
      "\n",
      "UH:\n",
      "('interjection', 'Goodbye Goody Gosh Wow Jeepers Jee-sus Hubba Hey Kee-reist Oops amen huh howdy uh dammit whammo shucks heck anyways whodunnit honey golly man baby diddle hush sonuvabitch ... ')\n",
      "Description: interjection\n",
      "Example: G\n",
      "\n",
      "VBG:\n",
      "('verb, present participle or gerund', \"telegraphing stirring focusing angering judging stalling lactating hankerin' alleging veering capping approaching traveling besieging encrypting interrupting erasing wincing ... \")\n",
      "Description: verb, present participle or gerund\n",
      "Example: t\n",
      "\n",
      "JJ:\n",
      "('adjective or numeral, ordinal', 'third ill-mannered pre-war regrettable oiled calamitous first separable ectoplasmic battery-powered participatory fourth still-to-be-named multilingual multi-disciplinary ... ')\n",
      "Description: adjective or numeral, ordinal\n",
      "Example: t\n",
      "\n",
      "VBZ:\n",
      "('verb, present tense, 3rd person singular', 'bases reconstructs marks mixes displeases seals carps weaves snatches slumps stretches authorizes smolders pictures emerges stockpiles seduces fizzes uses bolsters slaps speaks pleads ... ')\n",
      "Description: verb, present tense, 3rd person singular\n",
      "Example: b\n",
      "\n",
      "--:\n",
      "('dash', '-- ')\n",
      "Description: dash\n",
      "Example: -\n",
      "\n",
      "VBP:\n",
      "('verb, present tense, not 3rd person singular', 'predominate wrap resort sue twist spill cure lengthen brush terminate appear tend stray glisten obtain comprise detest tease attract emphasize mold postpone sever return wag ... ')\n",
      "Description: verb, present tense, not 3rd person singular\n",
      "Example: p\n",
      "\n",
      "NN:\n",
      "('noun, common, singular or mass', 'common-carrier cabbage knuckle-duster Casino afghan shed thermostat investment slide humour falloff slick wind hyena override subhumanity machinist ... ')\n",
      "Description: noun, common, singular or mass\n",
      "Example: c\n",
      "\n",
      "DT:\n",
      "('determiner', 'all an another any both del each either every half la many much nary neither no some such that the them these this those ')\n",
      "Description: determiner\n",
      "Example: a\n",
      "\n",
      "PRP:\n",
      "('pronoun, personal', 'hers herself him himself hisself it itself me myself one oneself ours ourselves ownself self she thee theirs them themselves they thou thy us ')\n",
      "Description: pronoun, personal\n",
      "Example: h\n",
      "\n",
      "::\n",
      "('colon or ellipsis', ': ; ... ')\n",
      "Description: colon or ellipsis\n",
      "Example: :\n",
      "\n",
      "WP$:\n",
      "('WH-pronoun, possessive', 'whose ')\n",
      "Description: WH-pronoun, possessive\n",
      "Example: w\n",
      "\n",
      "NNPS:\n",
      "('noun, proper, plural', 'Americans Americas Amharas Amityvilles Amusements Anarcho-Syndicalists Andalusians Andes Andruses Angels Animals Anthony Antilles Antiques Apache Apaches Apocrypha ... ')\n",
      "Description: noun, proper, plural\n",
      "Example: A\n",
      "\n",
      "PRP$:\n",
      "('pronoun, possessive', 'her his mine my our ours their thy your ')\n",
      "Description: pronoun, possessive\n",
      "Example: h\n",
      "\n",
      "WDT:\n",
      "('WH-determiner', 'that what whatever which whichever ')\n",
      "Description: WH-determiner\n",
      "Example: t\n",
      "\n",
      "(:\n",
      "('opening parenthesis', '( [ { ')\n",
      "Description: opening parenthesis\n",
      "Example: (\n",
      "\n",
      "):\n",
      "('closing parenthesis', ') ] } ')\n",
      "Description: closing parenthesis\n",
      "Example: )\n",
      "\n",
      ".:\n",
      "('sentence terminator', '. ! ? ')\n",
      "Description: sentence terminator\n",
      "Example: .\n",
      "\n",
      ",:\n",
      "('comma', ', ')\n",
      "Description: comma\n",
      "Example: ,\n",
      "\n",
      "``:\n",
      "('opening quotation mark', '` `` ')\n",
      "Description: opening quotation mark\n",
      "Example: `\n",
      "\n",
      "$:\n",
      "('dollar', '$ -$ --$ A$ C$ HK$ M$ NZ$ S$ U.S.$ US$ ')\n",
      "Description: dollar\n",
      "Example: $\n",
      "\n",
      "RB:\n",
      "('adverb', 'occasionally unabatingly maddeningly adventurously professedly stirringly prominently technologically magisterially predominately swiftly fiscally pitilessly ... ')\n",
      "Description: adverb\n",
      "Example: o\n",
      "\n",
      "RBR:\n",
      "('adverb, comparative', 'further gloomier grander graver greater grimmer harder harsher healthier heavier higher however larger later leaner lengthier less-perfectly lesser lonelier longer louder lower more ... ')\n",
      "Description: adverb, comparative\n",
      "Example: f\n",
      "\n",
      "RBS:\n",
      "('adverb, superlative', 'best biggest bluntest earliest farthest first furthest hardest heartiest highest largest least less most nearest second tightest worst ')\n",
      "Description: adverb, superlative\n",
      "Example: b\n",
      "\n",
      "VBD:\n",
      "('verb, past tense', 'dipped pleaded swiped regummed soaked tidied convened halted registered cushioned exacted snubbed strode aimed adopted belied figgered speculated wore appreciated contemplated ... ')\n",
      "Description: verb, past tense\n",
      "Example: d\n",
      "\n",
      "IN:\n",
      "('preposition or conjunction, subordinating', 'astride among uppon whether out inside pro despite on by throughout below within for towards near behind atop around if like until below next into if beside ... ')\n",
      "Description: preposition or conjunction, subordinating\n",
      "Example: a\n",
      "\n",
      "FW:\n",
      "('foreign word', \"gemeinschaft hund ich jeux habeas Haementeria Herr K'ang-si vous lutihaw alai je jour objets salutaris fille quibusdam pas trop Monte terram fiche oui corporis ... \")\n",
      "Description: foreign word\n",
      "Example: g\n",
      "\n",
      "RP:\n",
      "('particle', 'aboard about across along apart around aside at away back before behind by crop down ever fast for forth from go high i.e. in into just later low more off on open out over per pie raising start teeth that through under unto up up-pp upon whole with you ')\n",
      "Description: particle\n",
      "Example: a\n",
      "\n",
      "JJR:\n",
      "('adjective, comparative', 'bleaker braver breezier briefer brighter brisker broader bumper busier calmer cheaper choosier cleaner clearer closer colder commoner costlier cozier creamier crunchier cuter ... ')\n",
      "Description: adjective, comparative\n",
      "Example: b\n",
      "\n",
      "JJS:\n",
      "('adjective, superlative', 'calmest cheapest choicest classiest cleanest clearest closest commonest corniest costliest crassest creepiest crudest cutest darkest deadliest dearest deepest densest dinkiest ... ')\n",
      "Description: adjective, superlative\n",
      "Example: c\n",
      "\n",
      "PDT:\n",
      "('pre-determiner', 'all both half many quite such sure this ')\n",
      "Description: pre-determiner\n",
      "Example: a\n",
      "\n",
      "MD:\n",
      "('modal auxiliary', \"can cannot could couldn't dare may might must need ought shall should shouldn't will would \")\n",
      "Description: modal auxiliary\n",
      "Example: c\n",
      "\n",
      "VB:\n",
      "('verb, base form', 'ask assemble assess assign assume atone attention avoid bake balkanize bank begin behold believe bend benefit bevel beware bless boil bomb boost brace break bring broil brush build ... ')\n",
      "Description: verb, base form\n",
      "Example: a\n",
      "\n",
      "WRB:\n",
      "('Wh-adverb', 'how however whence whenever where whereby whereever wherein whereof why ')\n",
      "Description: Wh-adverb\n",
      "Example: h\n",
      "\n",
      "NNP:\n",
      "('noun, proper, singular', 'Motown Venneboerger Czestochwa Ranzer Conchita Trumplane Christos Oceanside Escobar Kreisler Sawyer Cougar Yvette Ervin ODI Darryl CTCA Shannon A.K.C. Meltex Liverpool ... ')\n",
      "Description: noun, proper, singular\n",
      "Example: M\n",
      "\n",
      "EX:\n",
      "('existential there', 'there ')\n",
      "Description: existential there\n",
      "Example: t\n",
      "\n",
      "NNS:\n",
      "('noun, common, plural', 'undergraduates scotches bric-a-brac products bodyguards facets coasts divestitures storehouses designs clubs fragrances averages subjectivists apprehensions muses factory-jobs ... ')\n",
      "Description: noun, common, plural\n",
      "Example: u\n",
      "\n",
      "SYM:\n",
      "('symbol', \"% & ' '' ''. ) ). * + ,. < = > @ A[fj] U.S U.S.S.R * ** *** \")\n",
      "Description: symbol\n",
      "Example: %\n",
      "\n",
      "CC:\n",
      "('conjunction, coordinating', \"& 'n and both but either et for less minus neither nor or plus so therefore times v. versus vs. whether yet \")\n",
      "Description: conjunction, coordinating\n",
      "Example: &\n",
      "\n",
      "CD:\n",
      "('numeral, cardinal', \"mid-1890 nine-thirty forty-two one-tenth ten million 0.5 one forty-seven 1987 twenty '79 zero two 78-degrees eighty-four IX '60s .025 fifteen 271,124 dozen quintillion DM2,000 ... \")\n",
      "Description: numeral, cardinal\n",
      "Example: m\n",
      "\n",
      "POS:\n",
      "('genitive marker', \"' 's \")\n",
      "Description: genitive marker\n",
      "Example: '\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#### EXAMPLES FOR POS TAGGINGS\n",
    "# Load the UPenn tagset\n",
    "tagset = nltk.data.load('help/tagsets/upenn_tagset.pickle')\n",
    "\n",
    "# Display examples for each tag\n",
    "for tag, details in tagset.items():\n",
    "    print(f\"{tag}:\")\n",
    "    print(details)\n",
    "    print(f\"Description: {details[0]}\")\n",
    "    print(f\"Example: {details[1][0]}\\n\")  # Taking the first example for brevity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AST Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unroll_node_types(\n",
    "    nested_node_types: dict  # node_types from tree-sitter\n",
    ") -> list: # list of node types\n",
    "    def iterate_and_unroll_dict(nested_node_types: dict, all_node_types: set):\n",
    "        for key, value in nested_node_types.items():\n",
    "            if key == 'type' and type(value) == str:\n",
    "                all_node_types.add(value)\n",
    "            if type(value) == dict:\n",
    "                iterate_and_unroll_dict(value, all_node_types)\n",
    "            if type(value) == list:\n",
    "                for element in value:\n",
    "                    iterate_and_unroll_dict(element, all_node_types) \n",
    "    all_node_types = set()\n",
    "    for dictionary in nested_node_types:\n",
    "        iterate_and_unroll_dict(dictionary, all_node_types)\n",
    "    all_node_types.add('ERROR')\n",
    "    return list(all_node_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_parser(lang: str):\n",
    "    # Grab the node types from the tree-sitter language\n",
    "    language = Language(f\"{code_rationales.__path__[0]}/grammars/tree-sitter-languages.so\", lang)\n",
    "    node_path = f\"{code_rationales.__path__[0]}/grammars/tree-sitter-{lang}/src/node-types.json\"\n",
    "    with open(node_path) as f:\n",
    "            node_types = json.load(f)\n",
    "    node_types = unroll_node_types(node_types)\n",
    "    # Create a parser for the language\n",
    "    parser = Parser()\n",
    "    parser.set_language(language)\n",
    "    return parser, node_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def traverse(\n",
    "    node,       # tree-sitter node\n",
    ") -> None:\n",
    "    \"\"\"Traverse in a recursive way, a tree-sitter node and append results to a list.\"\"\"\n",
    "    results = []\n",
    "    def traverse_tree(node, results):\n",
    "        if node.type == 'string':\n",
    "            results.append(node)\n",
    "            return\n",
    "        for n in node.children:\n",
    "            traverse_tree(n, results)\n",
    "        if not node.children:\n",
    "            results.append(node)\n",
    "    traverse_tree(node, results)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_offset(\n",
    "    point,              #point to convert\n",
    "    lines: list         #list of lines in the source code\n",
    "    ):\n",
    "        \"\"\"Convert the point to an offset\"\"\"\n",
    "        row, column = point\n",
    "        chars_in_rows = sum(map(len, lines[:row])) + row\n",
    "        chars_in_columns = len(lines[row][:column])\n",
    "        offset = chars_in_rows + chars_in_columns\n",
    "        return offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_node_span(node, lines):\n",
    "    \"\"\"Get the span position of the node in the code string\"\"\"\n",
    "    start_span = convert_to_offset(node.start_point, lines)\n",
    "    end_span = convert_to_offset(node.end_point, lines)\n",
    "    return start_span, end_span\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_token_span_in_node_span(tok_span, token: str, node_span, node_text: str):\n",
    "    return (node_span[0] <= tok_span[0] and tok_span[1] <= node_span[1]) or \\\n",
    "            (node_span[0]-1 <= tok_span[0] and tok_span[1] <= node_span[1] and node_text in token) or \\\n",
    "            (tok_span[0] <= node_span[0] and node_span[1] <= tok_span[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_type(\n",
    "    tok_span: tuple, # (start, end) position of a token in tokenizer\n",
    "    token: str,   # token value\n",
    "    nodes: list,     # list of tree-sitter nodes\n",
    "    lines: list,     # list of lines in the code\n",
    ") -> tuple: # (parent_type, token_type) of the token\n",
    "    \"\"\"Get the parent AST type and token AST type of a token.\"\"\"\n",
    "    for i, node in enumerate(nodes):\n",
    "        if is_token_span_in_node_span(tok_span, token, get_node_span(node, lines), node.text.decode('utf-8')):\n",
    "            return nodes[i].parent.type, nodes[i].type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_nodes(\n",
    "    tok_span: tuple, # (start, end) position of a token in tokenizer\n",
    "    token: str,      #actual token\n",
    "    lines: list,     # list of lines in the code, \n",
    "    nodes_information: dict # dict with augmented information of each ast node\n",
    ") -> list: \n",
    "    \"\"\"Get all AST types for the given token span\"\"\"\n",
    "    results = []\n",
    "    for node_id, node_info in nodes_information.items():\n",
    "        if is_token_span_in_node_span(tok_span, token, node_info['span'], node_info['node'].text.decode('utf-8')):\n",
    "            results.append(node_info['node'])   \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_node_height(node):\n",
    "    if not node.children: \n",
    "        return 0\n",
    "    children_heights = []\n",
    "    for child in node.children:\n",
    "        children_heights.append(get_node_height(child))\n",
    "    return max(children_heights) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_ast(node, lines):\n",
    "    \"\"\"Get an array with additional infor for each node in the AST, Appends the height and span\"\"\"\n",
    "    information = {}\n",
    "    def traverse_and_append_info(node, lines, information):\n",
    "        information[node.id] = {'height': get_node_height(node), 'span': get_node_span(node, lines), 'node': node}\n",
    "        for child in node.children:\n",
    "            traverse_and_append_info(child, lines, information)\n",
    "    traverse_and_append_info(node, lines, information)\n",
    "    return information "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nodes_by_type(\n",
    "    node, \n",
    "    node_types: list\n",
    ") -> list :\n",
    "    def traverse_and_search(node, node_types, results):\n",
    "        if node.type in node_types:\n",
    "            results.append(node)\n",
    "        for n in node.children:\n",
    "            traverse_and_search(n, node_types ,results)\n",
    "    results = []\n",
    "    traverse_and_search(node, node_types, results)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identation Mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_identation_spans(source_code:str):\n",
    "    pattern = '\\s+(?=\\w)|\\t|\\n'\n",
    "    return [(m.start(0), m.end(0)-1) for m in re.finditer(pattern, source_code)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taxonomy Mapping "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_results(global_results):\n",
    "    def clean_dictonary(result_dict):\n",
    "        clean_dict = result_dict.copy()\n",
    "        for key, value in result_dict.items():\n",
    "            if not value: \n",
    "                clean_dict.pop(key)\n",
    "        return clean_dict\n",
    "    for key, value in global_results.items():\n",
    "        global_results[key] = clean_dictonary(value)\n",
    "    return global_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_category_by_token(taxonomy_dict: dict, token_type: str):\n",
    "    for key, value in taxonomy_dict.items():\n",
    "        if token_type in value:\n",
    "            return key\n",
    "    return 'unknown'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_to_taxonomy(sc_taxonomy_dict:dict, nl_taxonomy_dict: dict, result_dict: dict):\n",
    "    result_dict = result_dict.copy()\n",
    "    mappings = {token: {category : [] for category in {**sc_taxonomy_dict, **nl_taxonomy_dict}.keys()} for token in {**sc_taxonomy_dict, **nl_taxonomy_dict}.keys()}\n",
    "    for target_token, value in result_dict.items():\n",
    "        for source_token, rationales_values in value.items():\n",
    "            try: \n",
    "                if source_token[:2] == 'sc':\n",
    "                    mappings[search_category_by_token(sc_taxonomy_dict, target_token.split('_|_')[1])][search_category_by_token(sc_taxonomy_dict, source_token.split('_|_')[1])] += rationales_values \n",
    "                elif source_token[:2] == 'nl':\n",
    "                    mappings[search_category_by_token(sc_taxonomy_dict, target_token.split('_|_')[1])][search_category_by_token(nl_taxonomy_dict, source_token.split('_|_')[1])] += rationales_values\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "    return clean_results(mappings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_global_results_to_taxonomy(sc_taxonomy_dict:dict, nl_taxonomy_dict:dict, results: dict):\n",
    "    return dict(zip(results.keys(), map(lambda aggegrations: map_to_taxonomy(sc_taxonomy_dict, nl_taxonomy_dict, aggegrations), results.values())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Rationals Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_right_span = lambda start_idx, end_idx, df : len(''.join(map(str, df.loc[start_idx:end_idx, 'goal_token'].tolist())))\n",
    "calculate_span = lambda right_span, token : (right_span-len(str(token)), right_span)\n",
    "delete_leading_spaces = lambda string: re.sub(r'^\\s+', '', string)\n",
    "delete_leading_breaks = lambda string: re.sub(r'^\\n+', '', string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_first_token_row(df):\n",
    "    df.loc[-1] = [df['typesets_tgt'][0][0][0], df['from_seq_id'][0], None, None, None]\n",
    "    df.index = df.index + 1\n",
    "    df = df.sort_index()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_auxiliary_columns_to_experiment_result(df, delimiter_sequence: str):\n",
    "    df.insert(0, 'rational_pos', [i for i in range(len(df))])\n",
    "    initial_token = df['goal_token'][0]\n",
    "    ### TOKEN TYPE COLUMN\n",
    "    token_type_column = ['src'] * len(df)\n",
    "    sequence = initial_token\n",
    "    for idx, goal_token in enumerate(df['goal_token']):\n",
    "        if delimiter_sequence not in sequence:\n",
    "            token_type_column[idx] = 'nl'\n",
    "            sequence+=goal_token\n",
    "    df['token_type'] = token_type_column\n",
    "    src_initial_token_idx = df[df['token_type'] == 'src'].first_valid_index()\n",
    "    df['span'] = [None] * len(df[:src_initial_token_idx]) + [calculate_span(calculate_right_span(src_initial_token_idx, index, df), token) for index, token in df[src_initial_token_idx:]['goal_token'].items()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_nl_tags_in_experiment_result(df, nl_ast_types, nl_pos_types, parser):\n",
    "    #initial_token = df['typesets_tgt'][0][0][0] if df[df['token_type'] == 'src'].first_valid_index() == 0 else ''\n",
    "    ##### POS TAGS FOR NL PART\n",
    "    target_nl = ''.join(df[df['token_type'] == 'nl']['goal_token'].map(lambda value: str(value)))\n",
    "    pos_tags = nltk.pos_tag(nltk.word_tokenize(target_nl))\n",
    "    for idx in range(df[df['token_type']== 'src'].first_valid_index()):\n",
    "        nl_tags = list(map(lambda tag: tag[1] if tag[1] in nl_pos_types else None, filter(lambda tag: tag[0] in str(df['goal_token'][idx]), pos_tags)))\n",
    "        if nl_tags: df.at[idx, 'tags'] = df['tags'][idx] + [('nl',nl_tags[-1],0)]\n",
    "    ##### POS TAGS FOR CODE PART\n",
    "    target_code = ''.join(df[df['token_type'] == 'src']['goal_token'].map(lambda value: str(value)))\n",
    "    nl_target_nodes = get_nodes_by_type(parser.parse(bytes(target_code, 'utf8')).root_node, nl_ast_types)\n",
    "    for token_idx in range(df[df['token_type'] == 'src'].first_valid_index(), len(df['span'])):\n",
    "                for nl_target_node in nl_target_nodes:\n",
    "                    if is_token_span_in_node_span(df['span'][token_idx], df['goal_token'][token_idx], get_node_span(nl_target_node, target_code.split(\"\\n\")), nl_target_node.text.decode('utf-8')) and \\\n",
    "                            (str(df['goal_token'][token_idx]) in nl_target_node.text.decode('utf-8') or nl_target_node.text.decode('utf-8') in str(df['goal_token'][token_idx])):\n",
    "                            tagged_token_list = list(filter(lambda tagged_token: str(tagged_token[0]).replace(' ','') in str(df['goal_token'][token_idx]).replace(' ','') or str(df['goal_token'][token_idx]).replace(' ','') in str(tagged_token[0]).replace(' ',''), \\\n",
    "                                                        nltk.pos_tag( nltk.word_tokenize(nl_target_node.text.decode('utf-8')))))\n",
    "                            if len(tagged_token_list)>0 and tagged_token_list[0][1] in nl_pos_types and tagged_token_list[0][1] not in df['tags'][token_idx]: \n",
    "                                    df.at[token_idx, 'tags'] = df['tags'][token_idx] + [('nl', tagged_token_list[0][1],0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_ast_tags_in_experiment_result(df, parser):\n",
    "    target_code = ''.join(df[df['token_type'] == 'src']['goal_token'].map(lambda value: str(value)))\n",
    "    src_initial_token_idx = df[df['token_type'] == 'src'].first_valid_index()\n",
    "    target_ast = parser.parse(bytes(target_code, 'utf8')).root_node\n",
    "    nodes_information = augment_ast(target_ast, target_code.split(\"\\n\"))\n",
    "    identation_spans = get_identation_spans(target_code)\n",
    "    for token_idx in range(src_initial_token_idx, len(df)):\n",
    "        df.at[token_idx, 'tags'] = df['tags'][token_idx] + list(map(lambda node: ('sc', node.type, nodes_information[node.id]['height']), \n",
    "                                                                    get_token_nodes(df['span'][token_idx], df['goal_token'][token_idx], target_code.split(\"\\n\"), nodes_information)))\n",
    "        df.at[token_idx, 'tags'] = df['tags'][token_idx] + list(map(lambda iden_span: ('sc','identation', 0), filter(lambda iden_span: is_token_span_in_node_span(df['span'][token_idx],  df['goal_token'][token_idx], iden_span, target_code[iden_span[0]:iden_span[1]+1]), identation_spans)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_rationals(experiment_paths: list, nl_ast_types: list, nl_pos_types: list, delimiter_sequence: str, parser):\n",
    "    experiments = {}\n",
    "    for exp_idx, experiment_path in enumerate(experiment_paths):\n",
    "        experiment_results = []\n",
    "        df_experiment = pd.read_csv(experiment_path, index_col=0)\n",
    "        experiment_rational_results = [df_experiment[(df_experiment['from_seq_id'] == sample_idx) | \\\n",
    "                                                     (df_experiment['from_seq_id'] == str(sample_idx))].reset_index() \\\n",
    "                                                    for sample_idx in range(params['num_samples'])]\n",
    "        print('*'*10 +'Tagging rationals for exp: ' +str(exp_idx) + '*'*10)\n",
    "        for experiment_rational_result in experiment_rational_results:\n",
    "            experiment_rational_result = experiment_rational_result.drop('index', axis=1)\n",
    "            experiment_rational_result = add_first_token_row(experiment_rational_result)\n",
    "            add_auxiliary_columns_to_experiment_result(experiment_rational_result, delimiter_sequence)\n",
    "            experiment_rational_result['tags'] = [[]]*len(experiment_rational_result)\n",
    "            fill_nl_tags_in_experiment_result(experiment_rational_result, nl_ast_types, nl_pos_types, parser)\n",
    "            fill_ast_tags_in_experiment_result(experiment_rational_result, parser)\n",
    "            experiment_results.append(experiment_rational_result)\n",
    "        experiments[exp_idx] = experiment_results\n",
    "    return experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rationals Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_rationals(global_tagged_results: dict, ast_node_types: list, nl_pos_types: list):\n",
    "    aggregation_results = {exp_id: None for exp_id in global_tagged_results.keys()}\n",
    "    for exp_idx, experiment_results in global_tagged_results.items():\n",
    "        experiment_aggregation_results = {node_type: {**{ 'sc'+'_|_'+node_type : [] for node_type in ast_node_types }, **{ 'nl'+'_|_'+node_type: [] for node_type in nl_pos_types }} \n",
    "                                          for node_type in {**{ 'sc'+'_|_'+node_type : {'values': [], 'rationales': []} for node_type in ast_node_types }, **{ 'nl'+'_|_'+node_type : {'values': [], 'rationales': []} for node_type in nl_pos_types }}}\n",
    "        print('*'*10 +'Aggregrating rationals for exp: ' +str(exp_idx) + '*'*10)\n",
    "        for result_idx, experiment_result in enumerate(experiment_results):\n",
    "            for target_idx in range(len(experiment_result)):\n",
    "                if target_idx > 0: # INITIAL TOKEN IS IGNORED\n",
    "                    for rational_idx, rational_pos in enumerate(eval(experiment_result['rationale_pos_tgt'][target_idx])):\n",
    "                        try:\n",
    "                            [experiment_aggregation_results[target_tag[0]+'_|_'+target_tag[1]][rational_tag[0]+'_|_'+rational_tag[1]].append(eval(experiment_result['rationale_prob_tgt'][target_idx])[rational_idx]) if target_tag[1] and rational_tag[1] else None \\\n",
    "                            for rational_tag in experiment_result['tags'][rational_pos] for target_tag in experiment_result['tags'][target_idx]]\n",
    "                        except Exception as e:\n",
    "                            print('An Error Occurred')\n",
    "        aggregation_results[exp_idx] = clean_results(experiment_aggregation_results)\n",
    "    return aggregation_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bootstrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrapping( np_data, np_func, size ):\n",
    "    \"\"\"Create a bootstrap sample given data and a function\n",
    "    For instance, a bootstrap sample of means, or mediands. \n",
    "    The bootstrap replicates are a long as the original size\n",
    "    we can choose any observation more than once (resampling with replacement:np.random.choice)\n",
    "    \"\"\"\n",
    "    \n",
    "    #Cleaning NaNs\n",
    "    #np_data_clean = np_data[ np.logical_not( np.isnan(np_data) ) ] \n",
    "    \n",
    "    #The size of the bootstrap replicate is as big as size\n",
    "    #Creating the boostrap replicates as long as the orignal data size\n",
    "    #This strategy might work as imputation \n",
    "    bootstrap_repl = [ np_func( np.random.choice( np_data, size=len(np_data) ) ) for i in range( size ) ]\n",
    "    \n",
    "    #logging.info(\"Covariate: \" + cov) #Empirical Mean\n",
    "    #logging.info(\"Empirical Mean: \" + str(np.mean(np_data_clean))) #Empirical Mean\n",
    "    #logging.info(\"Bootstrapped Mean: \" + str( np.mean(bootstrap_repl) ) ) #Bootstrapped Mean\n",
    "    \n",
    "    return np.array( bootstrap_repl )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_samples_global_results(global_results: dict, size: int):\n",
    "    for exp_id in global_results.keys():\n",
    "        experiment_result = global_results[exp_id]\n",
    "        for target_type, target_value in global_results[exp_id].items():\n",
    "            for source_type, source_value in target_value.items():\n",
    "                experiment_result[target_type][source_type] = bootstrapping(source_value, np.mean, size).tolist()\n",
    "        global_results[exp_id] = experiment_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Retrieve experiments\n",
    "get_experiment_path =  lambda samples, size, exp: params['rational_results'] + '/' + params['dataset'] + '/' + '[t_'+str(samples)+']_[max_tgt_'+str(size)+']_[exp:'+str(exp)+'].csv'\n",
    "experiment_paths = [get_experiment_path(params['num_samples'], params['size_samples'], exp) for exp in range(params['num_experiments'])][:1]\n",
    "### Define parser\n",
    "parser, node_types = create_parser('python')\n",
    "node_types += ['identation']\n",
    "### Defines pos tags \n",
    "pos_types = list(nltk.data.load('help/tagsets/upenn_tagset.pickle'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********Tagging rationals for exp: 0**********\n"
     ]
    }
   ],
   "source": [
    "###TAG EXPERIMENTS RESULTS - TAKES TIME\n",
    "nl_ast_types = ['comment','identifier','string']\n",
    "global_tagged_results = tag_rationals(experiment_paths, nl_ast_types, pos_types, params['delimiter_sequence'], parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********Aggregrating rationals for exp: 0**********\n"
     ]
    }
   ],
   "source": [
    "###AGGREGATE RATIONALS - TAKES TIME\n",
    "global_aggregated_results = aggregate_rationals(global_tagged_results, node_types, pos_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "###GROUP AGGREGATES BY TAXONOMY\n",
    "global_taxonomy_results = map_global_results_to_taxonomy(pl_taxonomy_python(), nl_pos_taxonomy(), global_aggregated_results.copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storing Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for exp_id, exp_aggregation in global_aggregated_results.items():\n",
    "    with open(params['global_ast_results'] + '/' + params['dataset'] +'_exp_' + str(exp_id) +'.txt', 'w') as file:\n",
    "        file.write(json.dumps(exp_aggregation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for exp_id, exp_aggregation in global_taxonomy_results.items():\n",
    "    with open(params['global_taxonomy_results'] + '/' + params['dataset'] +'_exp_' + str(exp_id) +'.txt', 'w') as file:\n",
    "        file.write(json.dumps(exp_aggregation))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
