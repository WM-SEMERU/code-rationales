{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mapping Function\n",
    "> Labels a given token following a tailored taxonomy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer\n",
    "import json\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from code_rationales.loader import download_grammars\n",
    "from tree_sitter import Language, Parser\n",
    "import code_rationales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package tagsets to /root/nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('tagsets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def param_default():\n",
    "    return {\n",
    "        #'dataset' : 'code_completion_random_cut_5k_30_512_tokens',\n",
    "        'dataset' : 'code_completion_docstring_random_cut_3.8k_30_150_tokens',\n",
    "        #'dataset' : 'code_completion_docstring_signature_3.8k_30_150_tokens',\n",
    "        #'dataset' : 'code_completion_docstring_5k_30_150_tokens',\n",
    "        'rational_results': '/workspaces/code-rationales/data/rationales/gpt',\n",
    "        'global_ast_results': '/workspaces/code-rationales/data/global_ast_results/gpt',\n",
    "        'global_taxonomy_results': '/workspaces/code-rationales/data/global_taxonomy_results/gpt',\n",
    "        'num_samples' : 100, \n",
    "        'size_samples' : 146,\n",
    "        'num_experiments': 30, \n",
    "        'bootstrapping' : 500\n",
    "    }\n",
    "params = param_default()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Taxonomy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Programming Language Taxonomy\n",
    "def pl_taxonomy_python() -> dict:\n",
    "    return {\n",
    "  \"punctuation\": ['{', '}', '[', ']', '(', ')','\\\"', ',', '.', '...', ';', ':'], \n",
    "  \"exceptions\": ['raise_statement','catch', 'try', 'finally', 'throw', 'throws', 'except'],\n",
    "  \"oop\": ['def','class','instanceof','interface','private','protected','public','abstract','extends','package','this','implements','import','new','super'],\n",
    "  \"asserts\": ['assert'],\n",
    "  \"types\": ['tuple','set','list','pair','subscript','type','none','dictionary','integer','native','static','synchronized','transient','volatile','void','final','enum','byte','char','float','boolean','double','int','long','short','strictfp'],\n",
    "  \"conditionals\": ['else', 'if', 'switch', 'case', 'default'],\n",
    "  \"loops\": ['break', 'do', 'for', 'while', 'continue'],\n",
    "  \"operators\": ['as','yield','is','@','in','and','or','not','**','slice','%','+','<','>','=','+','-','*','/','%','++','--','!','==','!=','>=','<=','&&','||','?',':','~','<<','>>','>>>','&','^','|','//'],\n",
    "  \"indentation\": ['\\n','\\t'],\n",
    "  \"bool\": ['true', 'false'], \n",
    "  \"functional\":['lambda','lambda_parameters'],\n",
    "  \"with\" : ['with','with_item','with_statement','with_clause'], \n",
    "  \"return\" :['return'],\n",
    "  \"structural\" : ['attribute','module', 'argument_list','parenthesized_expression','pattern_list','class_definition','function_definition','block'],\n",
    "  \"statements\" : ['return_statement','break_statement','assignment','while_statement','expression_statement','assert_statement'],\n",
    "  \"expression\": ['call','exec','async','ellipsis','unary_operator','binary_operator','as_pattern_target','boolean_operator','as_pattern','comparison_operator','conditional_expression','named_expression','not_operator','primary_expression','as_pattern'],\n",
    "  \"errors\": [\"ERROR\"],\n",
    "  \"identifier\":[\"identifier\"],  \n",
    "  \"comment\":[\"comment\"],\n",
    "  \"string\": ['string','interpolation','string_content','string_end','string_start','escape_sequence'], \n",
    "  \"unknown\": []\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NL POS Taxonomy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nl_pos_taxonomy() -> dict: return {\n",
    "    \"nl_verb\" : ['VBN', 'VBG', 'VBZ', 'VBP', 'VBD', 'VB'],\n",
    "    \"nl_noun\" : ['NN', 'NNPS', 'NNS', 'NNP'],\n",
    "    \"nl_pronoun\" : ['WP', 'PRP', 'PRP$', 'WP','WP$'], \n",
    "    \"nl_adverb\" : ['RBS','RBR', 'RB', 'WRB'], \n",
    "    \"nl_adjetive\" : ['JJR', 'JJS', 'JJ'], \n",
    "    \"nl_determier\" : ['DT','WDT','PDT'], \n",
    "    \"nl_preposition\" : ['IN', 'TO'],\n",
    "    \"nl_particle\" : ['RP'],\n",
    "    \"nl_modal\" : ['MD'],\n",
    "    \"nl_conjunction\" : ['CC'],\n",
    "    \"nl_cardinal\" : ['CD'],\n",
    "    \"nl_list\": ['LS'],\n",
    "    \"nl_other\" : ['FW', 'EX', 'SYM' , 'UH', 'POS', \"''\", '--',':', '(', ')', '.', ',', '``', '$']\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AST Mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_rationals = pd.read_csv('/workspaces/code-rationales/data/rationales/gpt/testing/[t_100]_[max_tgt_44]_[exp:0]_.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_rationals = df_rationals[df_rationals['from_seq_id'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Retrieve the generated output\n",
    "#initial_token = eval(df_rationals['typesets_tgt'][0])[0][0]\n",
    "#code = initial_token + ''.join(df_rationals['goal_token'])\n",
    "#code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Add Span column\n",
    "#calculate_left_span = lambda index : len(initial_token + ''.join(df_rationals['goal_token'][:index]))\n",
    "#calculate_right_span = lambda left_span, token : len(left_span) + len(token)\n",
    "#span_col = list(map(lambda tuple: (tuple[0],tuple[0]+len(tuple[1])),[(calculate_left_span(index),token) for index, token in df_rationals['goal_token'].items()]))\n",
    "#df_rationals.insert(loc=df_rationals.columns.get_loc('goal_token')+1, column='span', value=span_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_rationals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map Tokens with Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/code_rationales/grammars\n"
     ]
    }
   ],
   "source": [
    "languages=['python', 'java']\n",
    "download_grammars(languages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unroll_node_types(\n",
    "    nested_node_types: dict  # node_types from tree-sitter\n",
    ") -> list: # list of node types\n",
    "    def iterate_and_unroll_dict(nested_node_types: dict, all_node_types: set):\n",
    "        for key, value in nested_node_types.items():\n",
    "            if key == 'type' and type(value) == str:\n",
    "                all_node_types.add(value)\n",
    "            if type(value) == dict:\n",
    "                iterate_and_unroll_dict(value, all_node_types)\n",
    "            if type(value) == list:\n",
    "                for element in value:\n",
    "                    iterate_and_unroll_dict(element, all_node_types) \n",
    "    all_node_types = set()\n",
    "    for dictionary in nested_node_types:\n",
    "        iterate_and_unroll_dict(dictionary, all_node_types)\n",
    "    all_node_types.add('ERROR')\n",
    "    return list(all_node_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_parser(lang: str):\n",
    "    # Grab the node types from the tree-sitter language\n",
    "    language = Language(f\"{code_rationales.__path__[0]}/grammars/tree-sitter-languages.so\", lang)\n",
    "    node_path = f\"{code_rationales.__path__[0]}/grammars/tree-sitter-{lang}/src/node-types.json\"\n",
    "    with open(node_path) as f:\n",
    "            node_types = json.load(f)\n",
    "    node_types = unroll_node_types(node_types)\n",
    "    # Create a parser for the language\n",
    "    parser = Parser()\n",
    "    parser.set_language(language)\n",
    "    return parser, node_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def traverse(\n",
    "    node,       # tree-sitter node\n",
    ") -> None:\n",
    "    \"\"\"Traverse in a recursive way, a tree-sitter node and append results to a list.\"\"\"\n",
    "    results = []\n",
    "    def traverse_tree(node, results):\n",
    "        if node.type == 'string':\n",
    "            results.append(node)\n",
    "            return\n",
    "        for n in node.children:\n",
    "            traverse_tree(n, results)\n",
    "        if not node.children:\n",
    "            results.append(node)\n",
    "    traverse_tree(node, results)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_offset(\n",
    "    point,              #point to convert\n",
    "    lines: list         #list of lines in the source code\n",
    "    ):\n",
    "        \"\"\"Convert the point to an offset\"\"\"\n",
    "        row, column = point\n",
    "        chars_in_rows = sum(map(len, lines[:row])) + row\n",
    "        chars_in_columns = len(lines[row][:column])\n",
    "        offset = chars_in_rows + chars_in_columns\n",
    "        return offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_node_span(node, lines):\n",
    "    \"\"\"Get the span position of the node in the code string\"\"\"\n",
    "    start_span = convert_to_offset(node.start_point, lines)\n",
    "    end_span = convert_to_offset(node.end_point, lines)\n",
    "    return start_span, end_span\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_token_span_in_node_span(tok_span, node_span):\n",
    "    if (node_span[0] <= tok_span[0] and tok_span[0] < node_span[1]) or (node_span[0] < tok_span[1] and tok_span[1] <= node_span[1]):\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_type(\n",
    "    tok_span: tuple, # (start, end) position of a token in tokenizer\n",
    "    nodes: list,     # list of tree-sitter nodes\n",
    "    lines: list,     # list of lines in the code\n",
    ") -> tuple: # (parent_type, token_type) of the token\n",
    "    \"\"\"Get the parent AST type and token AST type of a token.\"\"\"\n",
    "    node_spans = [get_node_span(node, lines) for node in nodes]\n",
    "    for i, span in enumerate(node_spans):\n",
    "        if is_token_span_in_node_span(tok_span, span):\n",
    "            return nodes[i].parent.type, nodes[i].type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_nodes(\n",
    "    tok_span: tuple, # (start, end) position of a token in tokenizer\n",
    "    node,            # tree-sitter node\n",
    "    lines: list,     # list of lines in the code\n",
    ") -> list: \n",
    "    \"\"\"Get all AST types for the given token span\"\"\"\n",
    "    results = []\n",
    "    def traverse_and_get_types(tok_span, node, lines, results) -> None:\n",
    "        node_span = get_node_span(node, lines)\n",
    "        if (node_span[0] <= tok_span[0] and tok_span[0] < node_span[1]) or (node_span[0] < tok_span[1] and tok_span[1] <= node_span[1]):\n",
    "            results.append(node)\n",
    "        for n in node.children:\n",
    "            traverse_and_get_types(tok_span, n, lines, results)\n",
    "    traverse_and_get_types(tok_span, node, lines, results)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nodes_by_type(\n",
    "    node, \n",
    "    node_types: list\n",
    ") -> list :\n",
    "    def traverse_and_search(node, node_types, results):\n",
    "        if node.type in node_types:\n",
    "            results.append(node)\n",
    "            return\n",
    "        for n in node.children:\n",
    "            traverse_and_search(n, node_types ,results)\n",
    "    results = []\n",
    "    traverse_and_search(node, node_types, results)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parser, node_types = create_parser('python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nodes = traverse(parser.parse(bytes(code, 'utf8')).root_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(get_token_type(df_rationals['span'][40], nodes, code.split(\"\\n\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(get_token_nodes(df_rationals['span'][42], parser.parse(bytes(code, 'utf8')).root_node, code.split(\"\\n\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(eval(df_rationals['rationale_pos_tgt'][2]))\n",
    "#print(eval(df_rationals['rationale_prob_tgt'][2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(df_rationals['goal_token'][eval(df_rationals['rationale_pos_tgt'][2])[0]-1])\n",
    "#print(df_rationals['span'][eval(df_rationals['rationale_pos_tgt'][2])[0]-1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taxonomy Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_results(global_results):\n",
    "    def clean_dictonary(result_dict):\n",
    "        clean_dict = result_dict.copy()\n",
    "        for key, value in result_dict.items():\n",
    "            if not value:\n",
    "                clean_dict.pop(key)\n",
    "        return clean_dict\n",
    "    for key, value in global_results.items():\n",
    "        global_results[key] = clean_dictonary(value)\n",
    "    return clean_dictonary(global_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_category_by_token(taxonomy_dict: dict, token_type: str):\n",
    "    for key, value in taxonomy_dict.items():\n",
    "        if token_type in value:\n",
    "            return key\n",
    "    return 'unknown'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_to_taxonomy(taxonomy_dict: dict, result_dict: dict):\n",
    "    result_dict = result_dict.copy()\n",
    "    mappings = {token: {category : [] for category in taxonomy_dict.keys()} for token in taxonomy_dict.keys()}\n",
    "    for target_token, value in result_dict.items():\n",
    "        for source_token, probs in value.items():\n",
    "            mappings[search_category_by_token(taxonomy_dict, target_token)][search_category_by_token(taxonomy_dict, source_token)]+=probs\n",
    "    return clean_results(mappings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Rational Global Aggregates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_experiment_path =  lambda samples, size, exp: params['rational_results'] + '/' + params['dataset'] + '/' + '[t_'+str(samples)+']_[max_tgt_'+str(size)+']_[exp:'+str(exp)+']_.csv'\n",
    "calculate_left_span = lambda index, initial_token, df_rationals : len(str(initial_token) + ''.join(str(df_rationals['goal_token'][:index])))\n",
    "calculate_right_span = lambda left_span, token : len(left_span) + len(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback\n",
    "\n",
    "def aggregate_rationals(experiment_paths: list, parser, node_types: list, pos_types: list, nl_ast_types: list):\n",
    "\n",
    "    global_results = {node_type : {node_type : [] for node_type in node_types + pos_types} for node_type in node_types + pos_types}\n",
    "\n",
    "    for exp_idx, experiment_path in enumerate(experiment_paths):\n",
    "        df_experiment = pd.read_csv(experiment_path, index_col=0)\n",
    "        experiment_rational_results = [df_experiment[(df_experiment['from_seq_id'] == sample_idx) | (df_experiment['from_seq_id'] == str(sample_idx))].reset_index() for sample_idx in range(params['num_samples'])]\n",
    "        print('*'*10 +'Aggregating rationales for exp: ' +str(exp_idx) + '*'*10)\n",
    "        \n",
    "        for experiment_rational_result in experiment_rational_results:\n",
    "            initial_token = eval(experiment_rational_result['typesets_tgt'][0])[0][0]\n",
    "            experiment_rational_result.insert(loc=experiment_rational_result.columns.get_loc('goal_token')+1, column='span', value=list(map(lambda tuple: (tuple[0],tuple[0]+len(tuple[1])),[(calculate_left_span(index, initial_token, experiment_rational_result), str(token)) for index, token in experiment_rational_result['goal_token'].items()])))\n",
    "            target_code = eval(experiment_rational_result['typesets_tgt'][0])[0][0] + ''.join(experiment_rational_result['goal_token'].map(lambda value: str(value)))\n",
    "            target_ast = parser.parse(bytes(target_code, 'utf8')).root_node\n",
    "            nl_target_nodes = get_nodes_by_type(target_ast, nl_ast_types)\n",
    "            nl_pos_column = [None for i in range(len(experiment_rational_result))]\n",
    "\n",
    "            ###### NL SAMPLE POS TAGGING\n",
    "            for target_token_idx in range(len(experiment_rational_result['span'])):\n",
    "                for nl_target_node in nl_target_nodes:\n",
    "                    if is_token_span_in_node_span(experiment_rational_result['span'][target_token_idx], get_node_span(nl_target_node, target_code.split(\"\\n\"))) and \\\n",
    "                            str(experiment_rational_result['goal_token'][target_token_idx]) in nl_target_node.text.decode('utf-8'):\n",
    "                            tagged_token_list = list(filter(lambda tagged_token: str(experiment_rational_result['goal_token'][target_token_idx]) in tagged_token[0] or \\\n",
    "                                                       tagged_token[0] in str(experiment_rational_result['goal_token'][target_token_idx]), \\\n",
    "                                                        nltk.pos_tag( nltk.word_tokenize(nl_target_node.text.decode('utf-8')))))\n",
    "                            if len(tagged_token_list)>0 and tagged_token_list[0][1] in pos_types: nl_pos_column[target_token_idx] = tagged_token_list[0][1]\n",
    "            experiment_rational_result['nl_pos'] = nl_pos_column      \n",
    "            \n",
    "            ###### GLOBAL MAPPING\n",
    "            for target_token_idx in range(len(experiment_rational_result['span'])):\n",
    "                target_nodes = get_token_nodes(experiment_rational_result['span'][target_token_idx], target_ast, target_code.split(\"\\n\"))\n",
    "                target_node_types = list(map(lambda node: node.type, target_nodes))\n",
    "                for rational_idx, rational_pos in enumerate(eval(experiment_rational_result['rationale_pos_tgt'][target_token_idx])):\n",
    "                    if eval(experiment_rational_result['rationale_pos_tgt'][target_token_idx])[rational_idx] > 0: #rational 1 position.\n",
    "                        try:\n",
    "                            #########################AST rationales binding: AST -> AST\n",
    "                            rational_prob = eval(experiment_rational_result['rationale_prob_tgt'][target_token_idx])[rational_idx]\n",
    "                            rational_nodes = get_token_nodes(experiment_rational_result['span'][eval(experiment_rational_result['rationale_pos_tgt'][target_token_idx])[rational_idx]-1], target_ast, target_code.split(\"\\n\"))\n",
    "                            rational_node_types = list(map(lambda node: node.type, rational_nodes))\n",
    "                            [global_results[target_node_type][rational_node_type].append(rational_prob) for target_node_type in target_node_types for rational_node_type in rational_node_types]\n",
    "\n",
    "                            #######################NL bindings\n",
    "                            target_nl_type = experiment_rational_result['nl_pos'][target_token_idx]\n",
    "                            rational_nl_type = experiment_rational_result['nl_pos'][eval(experiment_rational_result['rationale_pos_tgt'][target_token_idx])[rational_idx]-1]\n",
    "                            #NL rationales binding: AST -> NL\n",
    "                            for target_node_type in target_node_types:\n",
    "                                global_results[target_node_type][rational_nl_type].append(rational_prob) if rational_nl_type else None\n",
    "                            #NL rationales binding: NL -> AST\n",
    "                            if target_nl_type:\n",
    "                                [global_results[target_nl_type][rational_node_type].append(rational_prob) for rational_node_type in rational_node_types]\n",
    "                                #NL rationales binding: NL -> NL\n",
    "                                global_results[target_nl_type][rational_nl_type].append(rational_prob) if rational_nl_type else None\n",
    "                        except Exception as e:\n",
    "                            print(\"An error occurred:\", e)\n",
    "    return global_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback\n",
    "\n",
    "def aggregate_rationals(experiment_paths: list, parser, node_types: list, pos_types: list, nl_ast_types: list):\n",
    "\n",
    "    global_results = {node_type : {node_type : [] for node_type in node_types + pos_types} for node_type in node_types + pos_types}\n",
    "\n",
    "    for exp_idx, experiment_path in enumerate(experiment_paths):\n",
    "        df_experiment = pd.read_csv(experiment_path, index_col=0)\n",
    "        experiment_rational_results = [df_experiment[(df_experiment['from_seq_id'] == sample_idx) | (df_experiment['from_seq_id'] == str(sample_idx))].reset_index() for sample_idx in range(params['num_samples'])]\n",
    "        print('*'*10 +'Aggregating rationales for exp: ' +str(exp_idx) + '*'*10)\n",
    "        \n",
    "        for experiment_rational_result in experiment_rational_results:\n",
    "            initial_token = eval(experiment_rational_result['typesets_tgt'][0])[0][0]\n",
    "            experiment_rational_result.insert(loc=experiment_rational_result.columns.get_loc('goal_token')+1, column='span', value=list(map(lambda tuple: (tuple[0],tuple[0]+len(tuple[1])),[(calculate_left_span(index, initial_token, experiment_rational_result), str(token)) for index, token in experiment_rational_result['goal_token'].items()])))\n",
    "            target_code = eval(experiment_rational_result['typesets_tgt'][0])[0][0] + ''.join(experiment_rational_result['goal_token'].map(lambda value: str(value)))\n",
    "            target_ast = parser.parse(bytes(target_code, 'utf8')).root_node\n",
    "            nl_target_nodes = get_nodes_by_type(target_ast, nl_ast_types)\n",
    "            nl_pos_column = [None for i in range(len(experiment_rational_result))]\n",
    "\n",
    "            ###### NL SAMPLE POS TAGGING\n",
    "            for target_token_idx in range(len(experiment_rational_result['span'])):\n",
    "                for nl_target_node in nl_target_nodes:\n",
    "                    if is_token_span_in_node_span(experiment_rational_result['span'][target_token_idx], get_node_span(nl_target_node, target_code.split(\"\\n\"))) and \\\n",
    "                            str(experiment_rational_result['goal_token'][target_token_idx]) in nl_target_node.text.decode('utf-8'):\n",
    "                            tagged_token_list = list(filter(lambda tagged_token: str(experiment_rational_result['goal_token'][target_token_idx]) in tagged_token[0] or \\\n",
    "                                                       tagged_token[0] in str(experiment_rational_result['goal_token'][target_token_idx]), \\\n",
    "                                                        nltk.pos_tag( nltk.word_tokenize(nl_target_node.text.decode('utf-8')))))\n",
    "                            if len(tagged_token_list)>0 and tagged_token_list[0][1] in pos_types: nl_pos_column[target_token_idx] = tagged_token_list[0][1]\n",
    "            experiment_rational_result['nl_pos'] = nl_pos_column      \n",
    "            \n",
    "            ###### GLOBAL MAPPING\n",
    "            for target_token_idx in range(len(experiment_rational_result['span'])):\n",
    "                target_nodes = get_token_nodes(experiment_rational_result['span'][target_token_idx], target_ast, target_code.split(\"\\n\"))\n",
    "                target_node_types = list(map(lambda node: node.type, target_nodes))\n",
    "                for rational_idx, rational_pos in enumerate(eval(experiment_rational_result['rationale_pos_tgt'][target_token_idx])):\n",
    "                    if eval(experiment_rational_result['rationale_pos_tgt'][target_token_idx])[rational_idx] > 0: #rational 1 position.\n",
    "                        try:\n",
    "                            #########################AST rationales binding: AST -> AST\n",
    "                            rational_prob = eval(experiment_rational_result['rationale_prob_tgt'][target_token_idx])[rational_idx]\n",
    "                            rational_nodes = get_token_nodes(experiment_rational_result['span'][eval(experiment_rational_result['rationale_pos_tgt'][target_token_idx])[rational_idx]-1], target_ast, target_code.split(\"\\n\"))\n",
    "                            rational_node_types = list(map(lambda node: node.type, rational_nodes))\n",
    "                            [global_results[target_node_type][rational_node_type].append(rational_prob) for target_node_type in target_node_types for rational_node_type in rational_node_types]\n",
    "\n",
    "                            #######################NL bindings\n",
    "                            target_nl_type = experiment_rational_result['nl_pos'][target_token_idx]\n",
    "                            rational_nl_type = experiment_rational_result['nl_pos'][eval(experiment_rational_result['rationale_pos_tgt'][target_token_idx])[rational_idx]-1]\n",
    "                            #NL rationales binding: AST -> NL\n",
    "                            for target_node_type in target_node_types:\n",
    "                                global_results[target_node_type][rational_nl_type].append(rational_prob) if rational_nl_type else None\n",
    "                            #NL rationales binding: NL -> AST\n",
    "                            if target_nl_type:\n",
    "                                [global_results[target_nl_type][rational_node_type].append(rational_prob) for rational_node_type in rational_node_types]\n",
    "                                #NL rationales binding: NL -> NL\n",
    "                                global_results[target_nl_type][rational_nl_type].append(rational_prob) if rational_nl_type else None\n",
    "                        except Exception as e:\n",
    "                            print(\"An error occurred:\", e)\n",
    "    return global_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrapping( np_data, np_func, size ):\n",
    "    \"\"\"Create a bootstrap sample given data and a function\n",
    "    For instance, a bootstrap sample of means, or mediands. \n",
    "    The bootstrap replicates are a long as the original size\n",
    "    we can choose any observation more than once (resampling with replacement:np.random.choice)\n",
    "    \"\"\"\n",
    "    \n",
    "    #Cleaning NaNs\n",
    "    #np_data_clean = np_data[ np.logical_not( np.isnan(np_data) ) ] \n",
    "    \n",
    "    #The size of the bootstrap replicate is as big as size\n",
    "    #Creating the boostrap replicates as long as the orignal data size\n",
    "    #This strategy might work as imputation \n",
    "    bootstrap_repl = [ np_func( np.random.choice( np_data, size=len(np_data) ) ) for i in range( size ) ]\n",
    "    \n",
    "    #logging.info(\"Covariate: \" + cov) #Empirical Mean\n",
    "    #logging.info(\"Empirical Mean: \" + str(np.mean(np_data_clean))) #Empirical Mean\n",
    "    #logging.info(\"Bootstrapped Mean: \" + str( np.mean(bootstrap_repl) ) ) #Bootstrapped Mean\n",
    "    \n",
    "    return np.array( bootstrap_repl )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_samples_global_results(global_results: dict, size: int):\n",
    "    for target_type, target_value in global_results.items():\n",
    "        for source_type, source_value in target_value.items():\n",
    "            global_results[target_type][source_type] = bootstrapping(source_value, np.mean, size).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Retrieve experiments\n",
    "experiment_paths = [get_experiment_path(params['num_samples'], params['size_samples'], exp) for exp in range(params['num_experiments'])][:1]\n",
    "### Define parser\n",
    "parser, node_types = create_parser('python')\n",
    "### Defines pos tags \n",
    "pos_types = list(nltk.data.load('help/tagsets/upenn_tagset.pickle'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********Aggregating rationales for exp: 0**********\n"
     ]
    }
   ],
   "source": [
    "###AGGREGATES BY AST\n",
    "nl_ast_types = ['comment','identifier','string']\n",
    "global_ast_results = clean_results(aggregate_rationals(experiment_paths, parser, node_types, pos_types, nl_ast_types))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "###AGGREGATES BY TAXONOMY\n",
    "taxonomy = {**pl_taxonomy_python(), **nl_pos_taxonomy()}\n",
    "global_taxonomy_results = map_to_taxonomy(taxonomy, global_ast_results.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### WARNING TAKES TIME\n",
    "bootstrap_samples_global_results(global_taxonomy_results, params['bootstrapping'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storing Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open(params['global_ast_results'] + '/' + params['dataset'] + '.txt', 'w') as file:\n",
    "#    file.write(json.dumps(global_ast_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open(params['global_taxonomy_results'] + '/' + params['dataset'] + '.txt', 'w') as file:\n",
    "#    file.write(json.dumps(global_taxonomy_results))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
