{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local Aggregations Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def param_default():\n",
    "    return {\n",
    "        'model_name' : '/workspaces/code-rationales/data/codeparrot-small/checkpoints/checkpoint-29000', \n",
    "        'cache_dir': '/workspaces/code-rationales/datax/df_cache_dir',\n",
    "        'delimiter_sequence': '' ### BE VERY CAREFULL HERE ALWAYS VERIFY -> VERY IMPORTANT\n",
    "    }\n",
    "prompts = [\n",
    "        \"\"\"Generate Pyhton code that Test symlink when the target file is a relative path\n",
    "    Should throw a SaltInvocationError : it shouldn't fix this\n",
    "    (although it is done for this emulation.\n",
    "    If the caller works as with the following exception, that might\n",
    "    cause a performance read from the (i.e., with `ServiceException` and supporting the luck process.\n",
    "\n",
    "    :param block_point_on_error: the procurement that is only related to it.\n",
    "\n",
    "    \\\"\\\"\\\"\n",
    "\n",
    "    @private\n",
    "    def best_open_context(self, reqections: Iterable[Name]:\n",
    "        self.usage(\n",
    "            'rame.once is installed in Python. There is no-optimal...\n",
    "    \\\"\\\"\\\"\n",
    "        if not hasattr(self.collection.name):\"\"\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = param_default()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CORE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import csv\n",
    "import seaborn as sns; sns.set_theme()\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import functools\n",
    "import json\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "pd.options.display.float_format = '{:.2f}'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from code_rationales.loader import download_grammars\n",
    "from tree_sitter import Language, Parser\n",
    "import code_rationales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (2.0.2) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n",
      "2024-06-05 13:47:55.538896: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-05 13:47:55.769594: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset, load_from_disk\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import importlib\n",
    "from matplotlib import colors\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, '/workspaces/code-rationales/sequential-rationales/huggingface')\n",
    "from rationalization import rationalize_lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from code_rationales.taxonomies import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package tagsets to /root/nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('tagsets')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AST Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unroll_node_types(\n",
    "    nested_node_types: dict  # node_types from tree-sitter\n",
    ") -> list: # list of node types\n",
    "    def iterate_and_unroll_dict(nested_node_types: dict, all_node_types: set):\n",
    "        for key, value in nested_node_types.items():\n",
    "            if key == 'type' and type(value) == str:\n",
    "                all_node_types.add(value)\n",
    "            if type(value) == dict:\n",
    "                iterate_and_unroll_dict(value, all_node_types)\n",
    "            if type(value) == list:\n",
    "                for element in value:\n",
    "                    iterate_and_unroll_dict(element, all_node_types) \n",
    "    all_node_types = set()\n",
    "    for dictionary in nested_node_types:\n",
    "        iterate_and_unroll_dict(dictionary, all_node_types)\n",
    "    all_node_types.add('ERROR')\n",
    "    return list(all_node_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_parser(lang: str):\n",
    "    # Grab the node types from the tree-sitter language\n",
    "    language = Language(f\"{code_rationales.__path__[0]}/grammars/tree-sitter-languages.so\", lang)\n",
    "    node_path = f\"{code_rationales.__path__[0]}/grammars/tree-sitter-{lang}/src/node-types.json\"\n",
    "    with open(node_path) as f:\n",
    "            node_types = json.load(f)\n",
    "    node_types = unroll_node_types(node_types)\n",
    "    # Create a parser for the language\n",
    "    parser = Parser()\n",
    "    parser.set_language(language)\n",
    "    return parser, node_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def traverse(\n",
    "    node,       # tree-sitter node\n",
    ") -> None:\n",
    "    \"\"\"Traverse in a recursive way, a tree-sitter node and append results to a list.\"\"\"\n",
    "    results = []\n",
    "    def traverse_tree(node, results):\n",
    "        if node.type == 'string':\n",
    "            results.append(node)\n",
    "            return\n",
    "        for n in node.children:\n",
    "            traverse_tree(n, results)\n",
    "        if not node.children:\n",
    "            results.append(node)\n",
    "    traverse_tree(node, results)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_offset(\n",
    "    point,              #point to convert\n",
    "    lines: list         #list of lines in the source code\n",
    "    ):\n",
    "        \"\"\"Convert the point to an offset\"\"\"\n",
    "        row, column = point\n",
    "        chars_in_rows = sum(map(len, lines[:row])) + row\n",
    "        chars_in_columns = len(lines[row][:column])\n",
    "        offset = chars_in_rows + chars_in_columns\n",
    "        return offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_node_span(node, lines):\n",
    "    \"\"\"Get the span position of the node in the code string\"\"\"\n",
    "    start_span = convert_to_offset(node.start_point, lines)\n",
    "    end_span = convert_to_offset(node.end_point, lines)\n",
    "    return start_span, end_span\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_token_span_in_node_span(tok_span, token: str, node_span, node_text: str):\n",
    "    return (node_span[0] <= tok_span[0] and tok_span[1] <= node_span[1]) or \\\n",
    "            (node_span[0]-1 <= tok_span[0] and tok_span[1] <= node_span[1] and node_text in token) or \\\n",
    "            (tok_span[0] <= node_span[0] and node_span[1] <= tok_span[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_type(\n",
    "    tok_span: tuple, # (start, end) position of a token in tokenizer\n",
    "    token: str,   # token value\n",
    "    nodes: list,     # list of tree-sitter nodes\n",
    "    lines: list,     # list of lines in the code\n",
    ") -> tuple: # (parent_type, token_type) of the token\n",
    "    \"\"\"Get the parent AST type and token AST type of a token.\"\"\"\n",
    "    for i, node in enumerate(nodes):\n",
    "        if is_token_span_in_node_span(tok_span, token, get_node_span(node, lines), node.text.decode('utf-8')):\n",
    "            return nodes[i].parent.type, nodes[i].type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_nodes(\n",
    "    tok_span: tuple, # (start, end) position of a token in tokenizer\n",
    "    token: str,      #actual token\n",
    "    lines: list,     # list of lines in the code, \n",
    "    nodes_information: dict # dict with augmented information of each ast node\n",
    ") -> list: \n",
    "    \"\"\"Get all AST types for the given token span\"\"\"\n",
    "    results = []\n",
    "    for node_id, node_info in nodes_information.items():\n",
    "        if is_token_span_in_node_span(tok_span, token, node_info['span'], node_info['node'].text.decode('utf-8')):\n",
    "            results.append(node_info['node'])   \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_node_height(node):\n",
    "    if not node.children: \n",
    "        return 0\n",
    "    children_heights = []\n",
    "    for child in node.children:\n",
    "        children_heights.append(get_node_height(child))\n",
    "    return max(children_heights) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_ast(node, lines):\n",
    "    \"\"\"Get an array with additional infor for each node in the AST, Appends the height and span\"\"\"\n",
    "    information = {}\n",
    "    def traverse_and_append_info(node, lines, information):\n",
    "        information[node.id] = {'height': get_node_height(node), 'span': get_node_span(node, lines), 'node': node}\n",
    "        for child in node.children:\n",
    "            traverse_and_append_info(child, lines, information)\n",
    "    traverse_and_append_info(node, lines, information)\n",
    "    return information "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nodes_by_type(\n",
    "    node, \n",
    "    node_types: list\n",
    ") -> list :\n",
    "    def traverse_and_search(node, node_types, results):\n",
    "        if node.type in node_types:\n",
    "            results.append(node)\n",
    "        for n in node.children:\n",
    "            traverse_and_search(n, node_types ,results)\n",
    "    results = []\n",
    "    traverse_and_search(node, node_types, results)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identation Mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_identation_spans(source_code:str):\n",
    "    pattern = '\\s+(?=\\w)|\\t|\\n'\n",
    "    return [(m.start(0), m.end(0)-1) for m in re.finditer(pattern, source_code)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taxonomy Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_results(global_results):\n",
    "    def clean_dictonary(result_dict):\n",
    "        clean_dict = result_dict.copy()\n",
    "        for key, value in result_dict.items():\n",
    "            if not value or not value['values']: \n",
    "                clean_dict.pop(key)\n",
    "        return clean_dict\n",
    "    for key, value in global_results.items():\n",
    "        global_results[key] = clean_dictonary(value)\n",
    "    return global_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_category_by_token(taxonomy_dict: dict, token_type: str):\n",
    "    for key, value in taxonomy_dict.items():\n",
    "        if token_type in value:\n",
    "            return key\n",
    "    return 'unknown'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_to_taxonomy(sc_taxonomy_dict:dict, nl_taxonomy_dict: dict, result_dict: dict):\n",
    "    result_dict = result_dict.copy()\n",
    "    mappings = {token: {category : {'values': [], 'rationales': []} for category in {**sc_taxonomy_dict, **nl_taxonomy_dict}.keys()} for token in result_dict.keys()}\n",
    "    for target_token, value in result_dict.items():\n",
    "        for source_token, props in value.items():\n",
    "            source_key = search_category_by_token(sc_taxonomy_dict, source_token.split('_|_')[1]) if source_token[:2] == 'sc' else search_category_by_token(nl_taxonomy_dict, source_token.split('_|_')[1])\n",
    "            mappings[target_token][source_key]['values'].append(props['values'])\n",
    "            mappings[target_token][source_key]['rationales'].append(props['rationales'])\n",
    "    return clean_results(mappings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_local_results_to_taxonomy(sc_taxonomy_dict:dict, nl_taxonomy_dict:dict, local_results: dict):\n",
    "    return dict(zip(local_results.keys(), map(lambda aggegrations: map_to_taxonomy(sc_taxonomy_dict, nl_taxonomy_dict, aggegrations), local_results.values())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Sampling Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_sampled_generation(\n",
    "        df_sampled_code, \n",
    "        model,\n",
    "        tokenizer,\n",
    "        number_samples_generation = 1,\n",
    "        max_gen_tok = 100, \n",
    "        top_k = 0\n",
    "    ):\n",
    "    dict_generated_code = {i: [] for i in range(number_samples_generation)}\n",
    "    for idx_prompt, prompt in enumerate(df_sampled_code['prompt']):\n",
    "        input = tokenizer([prompt], return_tensors=\"pt\")\n",
    "        input.to(model.device)\n",
    "        outputs = model.generate(**input, do_sample=True,\n",
    "                                 max_length=len(df_sampled_code['input_ids'][idx_prompt]), ##Force rationalization\n",
    "                                 top_k=top_k, \n",
    "                                 num_return_sequences=number_samples_generation, \n",
    "                                 pad_token_id=tokenizer.eos_token_id)\n",
    "        for index, output in enumerate(outputs):\n",
    "            dict_generated_code[index].append(output.tolist())\n",
    "    df_temp = pd.DataFrame().from_dict(data=dict_generated_code) # DataFrame from Generation\n",
    "    df_temp = pd.concat([df_sampled_code.reset_index(), df_temp ], axis=1) #Index before concating\n",
    "    return df_temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Rationales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If the model is not fine-tuned or compatible, it will rise an error\n",
    "#This function works for one tensor of source token and one tensor of target tokens\n",
    "def rationalize_model(model, tokenizer, input_ids, max_token_size: int, verbose=True):\n",
    "    torch.cuda.empty_cache() #Cleaning Cache\n",
    "    all_rationales, log = rationalize_lm(\n",
    "        model = model,\n",
    "        input_ids = input_ids[:max_token_size],\n",
    "        tokenizer = tokenizer,\n",
    "        verbose = verbose,\n",
    "        max_steps=1024 #Max number of steps for greedy rationalization\n",
    "    )\n",
    "    return all_rationales, log "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_multiple_rational(\n",
    "    model,\n",
    "    tokenizer, \n",
    "    arr_target_tokens, \n",
    "    seq_id, #mapping sequence id\n",
    "    max_token_size,\n",
    "    verbose=True\n",
    "):\n",
    "    arr_log = []\n",
    "    for index, val in enumerate(arr_target_tokens):\n",
    "        all_rationales, log = rationalize_model(\n",
    "            model=model, \n",
    "            tokenizer=tokenizer, \n",
    "            input_ids=val,\n",
    "            max_token_size=max_token_size,\n",
    "            verbose=False\n",
    "        )\n",
    "        arr_log.append(log)\n",
    "    arr_code_rationales = [ log['rationalization'] for log in arr_log ] #extracting just rationalizations\n",
    "    arr_from_sentence = [ list(np.full( len(val), seq_id[arr_i] )) #arr_i maps to the real sequence id\n",
    "                            for arr_i, val in enumerate(arr_code_rationales)]\n",
    "    arr_code_rationales = sum( arr_code_rationales, [] ) #flatting\n",
    "    arr_from_sentence = sum( arr_from_sentence, [] ) #flatting\n",
    "    return arr_code_rationales, arr_from_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pandas_rationales( arr_code_rationales, arr_from_sentence ):\n",
    "    #Creating pandas_1 {p_rationale}\n",
    "    rational = lambda list_log,typeset: [ (dict_tok['added_token_text'],round(dict_tok['true_token_prob'],6)) for dict_tok in list_log if dict_tok['from']==typeset]\n",
    "    log = lambda log_row: [(log_dict['added_token_text'],log_dict['true_token_prob']) for log_dict in log_row] #Typeset\n",
    "\n",
    "    log_position = lambda log_row: [log_dict['added_token_position'] for log_dict in log_row] #Position of the Rationale\n",
    "    log_prediction = lambda log_row: [log_dict['true_token_prob'] for log_dict in log_row] #Rationale Prob\n",
    "\n",
    "    p_rationale = pd.DataFrame()\n",
    "\n",
    "    p_rationale['goal_token'] = [dict_token['goal_word'] for dict_token in arr_code_rationales]\n",
    "    p_rationale['from_seq_id'] = arr_from_sentence\n",
    "\n",
    "    p_rationale['typesets_tgt'] = [ log(log_row) for log_row in [dict_token['log'] for dict_token in arr_code_rationales]]\n",
    "    \n",
    "    p_rationale['rationale_pos_tgt'] = [ log_position(log_row) for log_row in [dict_token['log'] for dict_token in arr_code_rationales]]\n",
    "    p_rationale['rationale_prob_tgt'] = [ log_prediction(log_row) for log_row in [dict_token['log'] for dict_token in arr_code_rationales]]\n",
    "\n",
    "\n",
    "    return p_rationale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Running Rationalization\n",
    "def run_code_rational( \n",
    "        df_generated_input,\n",
    "        tensor_size, #Control the size of the experiment, \n",
    "        model,\n",
    "        tokenizer,\n",
    "        experiment = '5',\n",
    "        batch_size = 100, \n",
    "        max_token_size = 44,\n",
    "        verbose = True \n",
    "    ):\n",
    "\n",
    "    arr_rationals = []\n",
    "    arr_from_seq = []\n",
    "\n",
    "    for i in range( 0 , tensor_size , batch_size ):\n",
    "        print('************************' + str(i) + '************************')\n",
    "        t_generated_input = df_generated_input[experiment].values[i:i+batch_size]\n",
    "        t_generated_input = [ torch.tensor(s).to(model.device) for s in t_generated_input]\n",
    "\n",
    "        t_arr_rationals,t_arr_from_seq = run_multiple_rational(\n",
    "            model = model,\n",
    "            tokenizer = tokenizer,\n",
    "            arr_target_tokens =  t_generated_input, \n",
    "            seq_id = list(range(i,i+batch_size)),\n",
    "            max_token_size = len(t_generated_input[0]),\n",
    "            verbose = verbose\n",
    "        )\n",
    "\n",
    "        arr_rationals = arr_rationals + t_arr_rationals\n",
    "        arr_from_seq = arr_from_seq + t_arr_from_seq\n",
    "\n",
    "        torch.cuda.empty_cache() #Cleaning Cache\n",
    "        \n",
    "    print(\"Experiment Finished: \" + str(experiment))\n",
    "    return pandas_rationales( arr_rationals, arr_from_seq )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_code_rational_all_set(exp, df_generated_input, model, tokenizer, tensor_n = 100, BATCH = 10): #When Tensor_n and batch differs then 'from_seq_id' is lost\n",
    "    torch.cuda.empty_cache() #Cleaning Cache\n",
    "    EXP = exp\n",
    "    test_arr_rationals = run_code_rational( \n",
    "            df_generated_input,\n",
    "            tensor_n,\n",
    "            model, \n",
    "            tokenizer,\n",
    "            experiment = EXP,\n",
    "            batch_size = BATCH,\n",
    "            verbose = False \n",
    "        )\n",
    "    #Saving process\n",
    "    return test_arr_rationals\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rationales Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_right_span = lambda start_idx, end_idx, df : len(''.join(map(str, df.loc[start_idx:end_idx, 'goal_token'].tolist())))\n",
    "calculate_span = lambda right_span, token : (right_span-len(str(token)), right_span)\n",
    "delete_leading_spaces = lambda string: re.sub(r'^\\s+', '', string)\n",
    "delete_leading_breaks = lambda string: re.sub(r'^\\n+', '', string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_first_token_row(df):\n",
    "    df.loc[-1] = [df['typesets_tgt'][0][0][0], df['from_seq_id'][0], None, None, None, df['exp'][0]]\n",
    "    df.index = df.index + 1\n",
    "    df = df.sort_index()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_auxiliary_columns_to_experiment_result(df, delimiter_sequence: str):\n",
    "    df.insert(0, 'rational_pos', [i for i in range(len(df))])\n",
    "    initial_token = df['goal_token'][0]\n",
    "    ### TOKEN TYPE COLUMN\n",
    "    token_type_column = ['src'] * len(df)\n",
    "    sequence = initial_token\n",
    "    for idx, goal_token in enumerate(df['goal_token']):\n",
    "        if delimiter_sequence not in sequence:\n",
    "            token_type_column[idx] = 'nl'\n",
    "            sequence+=str(goal_token)\n",
    "    df['token_type'] = token_type_column\n",
    "    src_initial_token_idx = df[df['token_type'] == 'src'].first_valid_index()\n",
    "    df['span'] = [None] * len(df[:src_initial_token_idx]) + [calculate_span(calculate_right_span(src_initial_token_idx, index, df), token) for index, token in df[src_initial_token_idx:]['goal_token'].items()] if src_initial_token_idx is not None else [None] * len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_nl_tags_in_experiment_result(df, nl_ast_types, nl_pos_types, parser):\n",
    "    #initial_token = df['typesets_tgt'][0][0][0] if df[df['token_type'] == 'src'].first_valid_index() == 0 else ''\n",
    "    ##### POS TAGS FOR NL PART IN PROMPT\n",
    "    target_nl = ''.join(df[df['token_type'] == 'nl']['goal_token'].map(lambda value: str(value)))\n",
    "    pos_tags = nltk.pos_tag(nltk.word_tokenize(target_nl))\n",
    "    first_src_token_index = df[df['token_type']== 'src'].first_valid_index()\n",
    "    nl_stop_index = first_src_token_index if first_src_token_index is not None else len(df)\n",
    "    for idx in range(nl_stop_index):\n",
    "        nl_tags = list(map(lambda tag: tag[1] if tag[1] in nl_pos_types else None, filter(lambda tag: tag[0] in str(df['goal_token'][idx]), pos_tags)))\n",
    "        if nl_tags: df.at[idx, 'tags'] = df['tags'][idx] + [('nl',nl_tags[-1],0)]\n",
    "    ##### POS TAGS FOR CODE PART\n",
    "    target_code = ''.join(df[df['token_type'] == 'src']['goal_token'].map(lambda value: str(value)))\n",
    "    nl_target_nodes = get_nodes_by_type(parser.parse(bytes(target_code, 'utf8')).root_node, nl_ast_types)\n",
    "    if first_src_token_index is not None:\n",
    "        for token_idx in range(first_src_token_index, len(df['span'])):\n",
    "                    for nl_target_node in nl_target_nodes:\n",
    "                        if is_token_span_in_node_span(df['span'][token_idx], df['goal_token'][token_idx], get_node_span(nl_target_node, target_code.split(\"\\n\")), nl_target_node.text.decode('utf-8')) and \\\n",
    "                                (str(df['goal_token'][token_idx]) in nl_target_node.text.decode('utf-8') or nl_target_node.text.decode('utf-8') in str(df['goal_token'][token_idx])):\n",
    "                                tagged_token_list = list(filter(lambda tagged_token: str(tagged_token[0]).replace(' ','') in str(df['goal_token'][token_idx]).replace(' ','') or str(df['goal_token'][token_idx]).replace(' ','') in str(tagged_token[0]).replace(' ',''), \\\n",
    "                                                        nltk.pos_tag( nltk.word_tokenize(nl_target_node.text.decode('utf-8')))))\n",
    "                                if len(tagged_token_list)>0 and tagged_token_list[0][1] in nl_pos_types and tagged_token_list[0][1] not in df['tags'][token_idx]: \n",
    "                                        df.at[token_idx, 'tags'] = df['tags'][token_idx] + [('nl', tagged_token_list[0][1],0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_ast_tags_in_experiment_result(df, parser):\n",
    "    target_code = ''.join(df[df['token_type'] == 'src']['goal_token'].map(lambda value: str(value)))\n",
    "    src_initial_token_idx = df[df['token_type'] == 'src'].first_valid_index()\n",
    "    target_ast = parser.parse(bytes(target_code, 'utf8')).root_node\n",
    "    nodes_information = augment_ast(target_ast, target_code.split(\"\\n\"))\n",
    "    identation_spans = get_identation_spans(target_code)\n",
    "    if src_initial_token_idx is not None:\n",
    "        for token_idx in range(src_initial_token_idx, len(df)):\n",
    "            df.at[token_idx, 'tags'] = df['tags'][token_idx] + list(map(lambda node: ('sc', node.type, nodes_information[node.id]['height']), \n",
    "                                                                    get_token_nodes(df['span'][token_idx], df['goal_token'][token_idx], target_code.split(\"\\n\"), nodes_information)))\n",
    "            df.at[token_idx, 'tags'] = df['tags'][token_idx] + list(map(lambda iden_span: ('sc','identation', 0), filter(lambda iden_span: is_token_span_in_node_span(df['span'][token_idx],  df['goal_token'][token_idx], iden_span, target_code[iden_span[0]:iden_span[1]+1]), identation_spans)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_rationals(experiment_results: list, nl_ast_types: list, nl_pos_types: list, delimiter_sequence: str, parser):\n",
    "    experiments = {}\n",
    "    for exp_idx, df_experiment in enumerate(experiment_results):\n",
    "        experiment_results = []\n",
    "        experiment_rational_results = [df_experiment[(df_experiment['from_seq_id'] == sample_idx) | \\\n",
    "                                                     (df_experiment['from_seq_id'] == str(sample_idx))].reset_index() \\\n",
    "                                                    for sample_idx in range(len(prompts))]\n",
    "        print('*'*10 +'Tagging rationals for exp: ' +str(exp_idx) + '*'*10)\n",
    "        for experiment_rational_result in experiment_rational_results:\n",
    "            experiment_rational_result = experiment_rational_result.drop('index', axis=1)\n",
    "            experiment_rational_result = add_first_token_row(experiment_rational_result)\n",
    "            add_auxiliary_columns_to_experiment_result(experiment_rational_result, delimiter_sequence)\n",
    "            experiment_rational_result['tags'] = [[]]*len(experiment_rational_result)\n",
    "            fill_nl_tags_in_experiment_result(experiment_rational_result, nl_ast_types, nl_pos_types, parser)\n",
    "            fill_ast_tags_in_experiment_result(experiment_rational_result, parser)\n",
    "            experiment_results.append(experiment_rational_result)\n",
    "        experiments[exp_idx] = experiment_results\n",
    "    return experiments\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rationales Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_rationals(global_tagged_results: dict, ast_node_types: list, nl_pos_types: list, number_samples: int):\n",
    "    aggregation_results = {sample_id: None  for sample_id in range(number_samples)}\n",
    "    for exp_idx, experiment_results in global_tagged_results.items():\n",
    "        print('*'*10 +'Aggregrating rationals for exp: ' +str(exp_idx) + '*'*10)\n",
    "        for experiment_result in experiment_results:\n",
    "            ### GET INFORMATION OF FIRST TOKEN\n",
    "            #sample_results = {str(pos+1)+'['+str(token)+']' : {node_type : {'values': [], 'rationales': []} for node_type in ast_node_types + nl_pos_types} for pos, token in enumerate(experiment_result['goal_token'].tolist())}\n",
    "            sample_results = {str(token_pos)+'['+str(experiment_result['goal_token'][token_pos])+']' : \n",
    "                              {**{ 'sc'+'_|_'+node_type : {'values': [], 'rationales': []} for node_type in ast_node_types }, **{ 'nl'+'_|_'+node_type: {'values': [], 'rationales': []} for node_type in nl_pos_types }}\n",
    "                              for token_pos in range(1, len(experiment_result['rational_pos']))}\n",
    "            for target_idx, target_token in enumerate(experiment_result['goal_token'].tolist()): \n",
    "                if target_idx > 0: # INITIAL TOKEN IS IGNORED\n",
    "                    for rational_idx, rational_pos in enumerate(experiment_result['rationale_pos_tgt'][target_idx]):\n",
    "                        for rational_tag in experiment_result['tags'][rational_pos]: \n",
    "                            if rational_tag[1]:\n",
    "                                try:\n",
    "                                    sample_results[str(target_idx)+'['+str(target_token)+']'][rational_tag[0]+'_|_'+rational_tag[1]]['values'].append(experiment_result['rationale_prob_tgt'][target_idx][rational_idx])\n",
    "                                    sample_results[str(target_idx)+'['+str(target_token)+']'][rational_tag[0]+'_|_'+rational_tag[1]]['rationales'].append(str(rational_pos)+'['+str(experiment_result['goal_token'][rational_pos])+']')\n",
    "                                except Exception as e:\n",
    "                                    print('An Error Occurred')\n",
    "            aggregation_results[experiment_result['from_seq_id'].unique()[0]] = clean_results(sample_results)\n",
    "    return aggregation_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOCAL EXPERIMENT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define parser\n",
    "parser, node_types = create_parser('python')\n",
    "node_types += ['identation']\n",
    "### Defines pos tags \n",
    "pos_types = list(nltk.data.load('help/tagsets/upenn_tagset.pickle'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model, Tokenizer Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(32768, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=32768, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(params['model_name'], cache_dir=params['cache_dir'])\n",
    "tokenizer = AutoTokenizer.from_pretrained(params['model_name'])\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local Dataset Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sampled_code = pd.DataFrame(prompts, columns=['prompt'])\n",
    "df_sampled_code['input_ids'] = tokenizer(df_sampled_code['prompt'].tolist())['input_ids']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input length of input_ids is 157, but ``max_length`` is set to 157.This can lead to unexpected behavior. You should consider increasing ``config.max_length`` or ``max_length``.\n"
     ]
    }
   ],
   "source": [
    "### SAMPLING GENERATION \n",
    "df_generated_input = df_sampled_generation(\n",
    "    df_sampled_code=df_sampled_code, \n",
    "    model=model,\n",
    "    tokenizer=tokenizer, \n",
    "    number_samples_generation=1,\n",
    "    max_gen_tok=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************0************************\n",
      "Experiment Finished: 0\n"
     ]
    }
   ],
   "source": [
    "### GET RATIONALES\n",
    "experiment_results = []\n",
    "for i in df_generated_input.columns[3:]: #Only Generated Sequences \n",
    "    experiment_result = run_code_rational_all_set(df_generated_input=df_generated_input, exp=i, tensor_n=df_generated_input.shape[0],model=model, tokenizer=tokenizer, BATCH=10)\n",
    "    experiment_result['exp'] = i\n",
    "    experiment_results.append(experiment_result)\n",
    "df_experiment_results = pd.concat(experiment_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_experiment_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********Tagging rationals for exp: 0**********\n"
     ]
    }
   ],
   "source": [
    "###TAG EXPERIMENTS RESULTS - TAKES TIME\n",
    "nl_ast_types = ['comment','identifier','string']\n",
    "tagged_results = tag_rationals([df_experiment_results], nl_ast_types, pos_types, params['delimiter_sequence'], parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rational_pos</th>\n",
       "      <th>goal_token</th>\n",
       "      <th>from_seq_id</th>\n",
       "      <th>typesets_tgt</th>\n",
       "      <th>rationale_pos_tgt</th>\n",
       "      <th>rationale_prob_tgt</th>\n",
       "      <th>exp</th>\n",
       "      <th>token_type</th>\n",
       "      <th>span</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>90</td>\n",
       "      <td>the</td>\n",
       "      <td>0</td>\n",
       "      <td>[(:, 0.0013765129260718822), (param, 0.0272987...</td>\n",
       "      <td>[89, 81, 79, 82]</td>\n",
       "      <td>[0.0013765129260718822, 0.0272987000644207, 0....</td>\n",
       "      <td>0</td>\n",
       "      <td>src</td>\n",
       "      <td>(390, 394)</td>\n",
       "      <td>[(nl, DT, 0), (sc, ERROR, 6), (sc, ERROR, 1), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>91</td>\n",
       "      <td>procurement</td>\n",
       "      <td>0</td>\n",
       "      <td>[( the, 5.867495474376483e-06), ( throw, 1.358...</td>\n",
       "      <td>[90, 18, 73, 50, 72, 56, 34, 55, 19, 33, 21, 5...</td>\n",
       "      <td>[5.867495474376483e-06, 1.3583382497017737e-05...</td>\n",
       "      <td>0</td>\n",
       "      <td>src</td>\n",
       "      <td>(394, 406)</td>\n",
       "      <td>[(nl, NN, 0), (sc, ERROR, 6), (sc, ERROR, 1), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>92</td>\n",
       "      <td>that</td>\n",
       "      <td>0</td>\n",
       "      <td>[( procurement, 0.000968215346802026), ( this,...</td>\n",
       "      <td>[91, 39, 25, 57, 54, 33, 35, 34, 36, 30, 38, 7...</td>\n",
       "      <td>[0.000968215346802026, 0.005966015160083771, 0...</td>\n",
       "      <td>0</td>\n",
       "      <td>src</td>\n",
       "      <td>(406, 411)</td>\n",
       "      <td>[(nl, IN, 0), (sc, ERROR, 6), (sc, ERROR, 1), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>93</td>\n",
       "      <td>is</td>\n",
       "      <td>0</td>\n",
       "      <td>[( that, 0.04233383387327194), ( is, 0.1007973...</td>\n",
       "      <td>[92, 12, 91, 89]</td>\n",
       "      <td>[0.04233383387327194, 0.1007973700761795, 0.13...</td>\n",
       "      <td>0</td>\n",
       "      <td>src</td>\n",
       "      <td>(411, 414)</td>\n",
       "      <td>[(sc, ERROR, 6), (sc, is, 0), (sc, identation,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>94</td>\n",
       "      <td>only</td>\n",
       "      <td>0</td>\n",
       "      <td>[( is, 0.0022375662811100483), ( emulation, 0....</td>\n",
       "      <td>[93, 40, 25, 69, 71, 72, 63, 33, 31, 38, 39, 3...</td>\n",
       "      <td>[0.0022375662811100483, 0.008678463287651539, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>src</td>\n",
       "      <td>(414, 419)</td>\n",
       "      <td>[(nl, RB, 0), (sc, ERROR, 6), (sc, identifier,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>95</td>\n",
       "      <td>related</td>\n",
       "      <td>0</td>\n",
       "      <td>[( only, 0.00027346322895027697), (param, 0.00...</td>\n",
       "      <td>[94, 81, 8, 60, 25, 91, 93, 58, 50, 14, 12, 57...</td>\n",
       "      <td>[0.00027346322895027697, 0.0008514222572557628...</td>\n",
       "      <td>0</td>\n",
       "      <td>src</td>\n",
       "      <td>(419, 427)</td>\n",
       "      <td>[(nl, JJ, 0), (sc, ERROR, 6), (sc, ERROR, 2), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>96</td>\n",
       "      <td>to</td>\n",
       "      <td>0</td>\n",
       "      <td>[( related, 0.013893939554691315), ( is, 0.050...</td>\n",
       "      <td>[95, 93, 33]</td>\n",
       "      <td>[0.013893939554691315, 0.050838764756917953, 0...</td>\n",
       "      <td>0</td>\n",
       "      <td>src</td>\n",
       "      <td>(427, 430)</td>\n",
       "      <td>[(nl, TO, 0), (sc, ERROR, 6), (sc, ERROR, 2), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>97</td>\n",
       "      <td>it</td>\n",
       "      <td>0</td>\n",
       "      <td>[( to, 0.006652590818703175), ( it, 0.02278597...</td>\n",
       "      <td>[96, 35, 26, 48, 52, 93, 49, 72, 62, 67, 1, 11...</td>\n",
       "      <td>[0.006652590818703175, 0.02278597466647625, 0....</td>\n",
       "      <td>0</td>\n",
       "      <td>src</td>\n",
       "      <td>(430, 433)</td>\n",
       "      <td>[(nl, PRP, 0), (sc, ERROR, 6), (sc, ERROR, 2),...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>98</td>\n",
       "      <td>.</td>\n",
       "      <td>0</td>\n",
       "      <td>[( it, 0.013379665091633797), ( to, 0.13006784...</td>\n",
       "      <td>[97, 96]</td>\n",
       "      <td>[0.013379665091633797, 0.130067840218544]</td>\n",
       "      <td>0</td>\n",
       "      <td>src</td>\n",
       "      <td>(433, 434)</td>\n",
       "      <td>[(sc, ERROR, 6), (sc, ERROR, 2), (sc, ., 0), (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>99</td>\n",
       "      <td>\\n\\n</td>\n",
       "      <td>0</td>\n",
       "      <td>[(., 0.004827948287129402), ( to, 0.0953347086...</td>\n",
       "      <td>[98, 96, 66]</td>\n",
       "      <td>[0.004827948287129402, 0.09533470869064331, 0....</td>\n",
       "      <td>0</td>\n",
       "      <td>src</td>\n",
       "      <td>(434, 439)</td>\n",
       "      <td>[(sc, ERROR, 6), (sc, ERROR, 2), (sc, identati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>100</td>\n",
       "      <td>\"\"\"</td>\n",
       "      <td>0</td>\n",
       "      <td>[(\\n\\n   , 0.03844781965017319), ( to, 0.08647...</td>\n",
       "      <td>[99, 96, 70]</td>\n",
       "      <td>[0.03844781965017319, 0.08647187799215317, 0.1...</td>\n",
       "      <td>0</td>\n",
       "      <td>src</td>\n",
       "      <td>(439, 443)</td>\n",
       "      <td>[(sc, ERROR, 6), (sc, ERROR, 0), (sc, string, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>101</td>\n",
       "      <td>\\n\\n</td>\n",
       "      <td>0</td>\n",
       "      <td>[( \"\"\", 0.04540449380874634), ( supporting, 0....</td>\n",
       "      <td>[100, 73]</td>\n",
       "      <td>[0.04540449380874634, 0.23099170625209808]</td>\n",
       "      <td>0</td>\n",
       "      <td>src</td>\n",
       "      <td>(443, 448)</td>\n",
       "      <td>[(sc, ERROR, 6), (sc, identation, 0), (sc, ide...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>102</td>\n",
       "      <td>@</td>\n",
       "      <td>0</td>\n",
       "      <td>[(\\n\\n   , 0.06158827245235443), ( procurement...</td>\n",
       "      <td>[101, 91, 41]</td>\n",
       "      <td>[0.06158827245235443, 0.21221791207790375, 0.3...</td>\n",
       "      <td>0</td>\n",
       "      <td>src</td>\n",
       "      <td>(448, 450)</td>\n",
       "      <td>[(sc, ERROR, 6), (sc, @, 0)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>103</td>\n",
       "      <td>private</td>\n",
       "      <td>0</td>\n",
       "      <td>[( @, 0.0009735914063639939), ( it, 0.00280330...</td>\n",
       "      <td>[102, 97, 66, 7, 33, 17, 25, 5, 78, 62, 85, 65...</td>\n",
       "      <td>[0.0009735914063639939, 0.0028033016715198755,...</td>\n",
       "      <td>0</td>\n",
       "      <td>src</td>\n",
       "      <td>(450, 457)</td>\n",
       "      <td>[(nl, JJ, 0), (sc, ERROR, 6), (sc, identifier,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>104</td>\n",
       "      <td>\\n</td>\n",
       "      <td>0</td>\n",
       "      <td>[(private, 0.0012458181008696556), ( @, 0.0371...</td>\n",
       "      <td>[103, 102, 1]</td>\n",
       "      <td>[0.0012458181008696556, 0.037113577127456665, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>src</td>\n",
       "      <td>(457, 461)</td>\n",
       "      <td>[(sc, ERROR, 6), (sc, identation, 0)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>105</td>\n",
       "      <td>def</td>\n",
       "      <td>0</td>\n",
       "      <td>[(\\n   , 0.026085834950208664), ( @, 0.5138683...</td>\n",
       "      <td>[104, 102]</td>\n",
       "      <td>[0.026085834950208664, 0.5138683915138245]</td>\n",
       "      <td>0</td>\n",
       "      <td>src</td>\n",
       "      <td>(461, 465)</td>\n",
       "      <td>[(nl, NN, 0), (sc, ERROR, 6), (sc, identifier,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>106</td>\n",
       "      <td>best</td>\n",
       "      <td>0</td>\n",
       "      <td>[( def, 1.8622939023771323e-05), ( the, 0.0001...</td>\n",
       "      <td>[105, 9, 102, 58, 42, 50, 25, 55, 63, 24, 66, ...</td>\n",
       "      <td>[1.8622939023771323e-05, 0.0001347129436908289...</td>\n",
       "      <td>0</td>\n",
       "      <td>src</td>\n",
       "      <td>(465, 470)</td>\n",
       "      <td>[(sc, ERROR, 6), (sc, ERROR, 1), (sc, identati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>107</td>\n",
       "      <td>_</td>\n",
       "      <td>0</td>\n",
       "      <td>[( best, 0.2493283599615097)]</td>\n",
       "      <td>[106]</td>\n",
       "      <td>[0.2493283599615097]</td>\n",
       "      <td>0</td>\n",
       "      <td>src</td>\n",
       "      <td>(470, 471)</td>\n",
       "      <td>[(nl, NN, 0), (sc, ERROR, 6), (sc, ERROR, 1), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>108</td>\n",
       "      <td>open</td>\n",
       "      <td>0</td>\n",
       "      <td>[(_, 0.0003458334249444306), ( symlink, 0.0009...</td>\n",
       "      <td>[107, 7, 48, 67, 1, 11, 46, 93, 16, 75, 59, 57...</td>\n",
       "      <td>[0.0003458334249444306, 0.0009618357289582491,...</td>\n",
       "      <td>0</td>\n",
       "      <td>src</td>\n",
       "      <td>(471, 475)</td>\n",
       "      <td>[(nl, NN, 0), (sc, ERROR, 6), (sc, ERROR, 1), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>109</td>\n",
       "      <td>_</td>\n",
       "      <td>0</td>\n",
       "      <td>[(open, 0.2500033378601074)]</td>\n",
       "      <td>[108]</td>\n",
       "      <td>[0.2500033378601074]</td>\n",
       "      <td>0</td>\n",
       "      <td>src</td>\n",
       "      <td>(475, 476)</td>\n",
       "      <td>[(nl, NN, 0), (sc, ERROR, 6), (sc, ERROR, 1), ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     rational_pos    goal_token  from_seq_id  \\\n",
       "90             90           the            0   \n",
       "91             91   procurement            0   \n",
       "92             92          that            0   \n",
       "93             93            is            0   \n",
       "94             94          only            0   \n",
       "95             95       related            0   \n",
       "96             96            to            0   \n",
       "97             97            it            0   \n",
       "98             98             .            0   \n",
       "99             99       \\n\\n               0   \n",
       "100           100           \"\"\"            0   \n",
       "101           101       \\n\\n               0   \n",
       "102           102             @            0   \n",
       "103           103       private            0   \n",
       "104           104         \\n               0   \n",
       "105           105           def            0   \n",
       "106           106          best            0   \n",
       "107           107             _            0   \n",
       "108           108          open            0   \n",
       "109           109             _            0   \n",
       "\n",
       "                                          typesets_tgt  \\\n",
       "90   [(:, 0.0013765129260718822), (param, 0.0272987...   \n",
       "91   [( the, 5.867495474376483e-06), ( throw, 1.358...   \n",
       "92   [( procurement, 0.000968215346802026), ( this,...   \n",
       "93   [( that, 0.04233383387327194), ( is, 0.1007973...   \n",
       "94   [( is, 0.0022375662811100483), ( emulation, 0....   \n",
       "95   [( only, 0.00027346322895027697), (param, 0.00...   \n",
       "96   [( related, 0.013893939554691315), ( is, 0.050...   \n",
       "97   [( to, 0.006652590818703175), ( it, 0.02278597...   \n",
       "98   [( it, 0.013379665091633797), ( to, 0.13006784...   \n",
       "99   [(., 0.004827948287129402), ( to, 0.0953347086...   \n",
       "100  [(\\n\\n   , 0.03844781965017319), ( to, 0.08647...   \n",
       "101  [( \"\"\", 0.04540449380874634), ( supporting, 0....   \n",
       "102  [(\\n\\n   , 0.06158827245235443), ( procurement...   \n",
       "103  [( @, 0.0009735914063639939), ( it, 0.00280330...   \n",
       "104  [(private, 0.0012458181008696556), ( @, 0.0371...   \n",
       "105  [(\\n   , 0.026085834950208664), ( @, 0.5138683...   \n",
       "106  [( def, 1.8622939023771323e-05), ( the, 0.0001...   \n",
       "107                      [( best, 0.2493283599615097)]   \n",
       "108  [(_, 0.0003458334249444306), ( symlink, 0.0009...   \n",
       "109                       [(open, 0.2500033378601074)]   \n",
       "\n",
       "                                     rationale_pos_tgt  \\\n",
       "90                                    [89, 81, 79, 82]   \n",
       "91   [90, 18, 73, 50, 72, 56, 34, 55, 19, 33, 21, 5...   \n",
       "92   [91, 39, 25, 57, 54, 33, 35, 34, 36, 30, 38, 7...   \n",
       "93                                    [92, 12, 91, 89]   \n",
       "94   [93, 40, 25, 69, 71, 72, 63, 33, 31, 38, 39, 3...   \n",
       "95   [94, 81, 8, 60, 25, 91, 93, 58, 50, 14, 12, 57...   \n",
       "96                                        [95, 93, 33]   \n",
       "97   [96, 35, 26, 48, 52, 93, 49, 72, 62, 67, 1, 11...   \n",
       "98                                            [97, 96]   \n",
       "99                                        [98, 96, 66]   \n",
       "100                                       [99, 96, 70]   \n",
       "101                                          [100, 73]   \n",
       "102                                      [101, 91, 41]   \n",
       "103  [102, 97, 66, 7, 33, 17, 25, 5, 78, 62, 85, 65...   \n",
       "104                                      [103, 102, 1]   \n",
       "105                                         [104, 102]   \n",
       "106  [105, 9, 102, 58, 42, 50, 25, 55, 63, 24, 66, ...   \n",
       "107                                              [106]   \n",
       "108  [107, 7, 48, 67, 1, 11, 46, 93, 16, 75, 59, 57...   \n",
       "109                                              [108]   \n",
       "\n",
       "                                    rationale_prob_tgt  exp token_type  \\\n",
       "90   [0.0013765129260718822, 0.0272987000644207, 0....    0        src   \n",
       "91   [5.867495474376483e-06, 1.3583382497017737e-05...    0        src   \n",
       "92   [0.000968215346802026, 0.005966015160083771, 0...    0        src   \n",
       "93   [0.04233383387327194, 0.1007973700761795, 0.13...    0        src   \n",
       "94   [0.0022375662811100483, 0.008678463287651539, ...    0        src   \n",
       "95   [0.00027346322895027697, 0.0008514222572557628...    0        src   \n",
       "96   [0.013893939554691315, 0.050838764756917953, 0...    0        src   \n",
       "97   [0.006652590818703175, 0.02278597466647625, 0....    0        src   \n",
       "98           [0.013379665091633797, 0.130067840218544]    0        src   \n",
       "99   [0.004827948287129402, 0.09533470869064331, 0....    0        src   \n",
       "100  [0.03844781965017319, 0.08647187799215317, 0.1...    0        src   \n",
       "101         [0.04540449380874634, 0.23099170625209808]    0        src   \n",
       "102  [0.06158827245235443, 0.21221791207790375, 0.3...    0        src   \n",
       "103  [0.0009735914063639939, 0.0028033016715198755,...    0        src   \n",
       "104  [0.0012458181008696556, 0.037113577127456665, ...    0        src   \n",
       "105         [0.026085834950208664, 0.5138683915138245]    0        src   \n",
       "106  [1.8622939023771323e-05, 0.0001347129436908289...    0        src   \n",
       "107                               [0.2493283599615097]    0        src   \n",
       "108  [0.0003458334249444306, 0.0009618357289582491,...    0        src   \n",
       "109                               [0.2500033378601074]    0        src   \n",
       "\n",
       "           span                                               tags  \n",
       "90   (390, 394)  [(nl, DT, 0), (sc, ERROR, 6), (sc, ERROR, 1), ...  \n",
       "91   (394, 406)  [(nl, NN, 0), (sc, ERROR, 6), (sc, ERROR, 1), ...  \n",
       "92   (406, 411)  [(nl, IN, 0), (sc, ERROR, 6), (sc, ERROR, 1), ...  \n",
       "93   (411, 414)  [(sc, ERROR, 6), (sc, is, 0), (sc, identation,...  \n",
       "94   (414, 419)  [(nl, RB, 0), (sc, ERROR, 6), (sc, identifier,...  \n",
       "95   (419, 427)  [(nl, JJ, 0), (sc, ERROR, 6), (sc, ERROR, 2), ...  \n",
       "96   (427, 430)  [(nl, TO, 0), (sc, ERROR, 6), (sc, ERROR, 2), ...  \n",
       "97   (430, 433)  [(nl, PRP, 0), (sc, ERROR, 6), (sc, ERROR, 2),...  \n",
       "98   (433, 434)  [(sc, ERROR, 6), (sc, ERROR, 2), (sc, ., 0), (...  \n",
       "99   (434, 439)  [(sc, ERROR, 6), (sc, ERROR, 2), (sc, identati...  \n",
       "100  (439, 443)  [(sc, ERROR, 6), (sc, ERROR, 0), (sc, string, ...  \n",
       "101  (443, 448)  [(sc, ERROR, 6), (sc, identation, 0), (sc, ide...  \n",
       "102  (448, 450)                       [(sc, ERROR, 6), (sc, @, 0)]  \n",
       "103  (450, 457)  [(nl, JJ, 0), (sc, ERROR, 6), (sc, identifier,...  \n",
       "104  (457, 461)              [(sc, ERROR, 6), (sc, identation, 0)]  \n",
       "105  (461, 465)  [(nl, NN, 0), (sc, ERROR, 6), (sc, identifier,...  \n",
       "106  (465, 470)  [(sc, ERROR, 6), (sc, ERROR, 1), (sc, identati...  \n",
       "107  (470, 471)  [(nl, NN, 0), (sc, ERROR, 6), (sc, ERROR, 1), ...  \n",
       "108  (471, 475)  [(nl, NN, 0), (sc, ERROR, 6), (sc, ERROR, 1), ...  \n",
       "109  (475, 476)  [(nl, NN, 0), (sc, ERROR, 6), (sc, ERROR, 1), ...  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged_results[0][0][90:110]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " done\n",
      "[('nl', 'VBN', 0), ('sc', 'ERROR', 6), ('sc', 'comparison_operator', 2), ('sc', 'identifier', 0), ('sc', 'identation', 0), ('sc', 'identation', 0)]\n"
     ]
    }
   ],
   "source": [
    "print(tagged_results[0][0]['goal_token'][37])\n",
    "print(tagged_results[0][0]['tags'][37])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********Aggregrating rationals for exp: 0**********\n"
     ]
    }
   ],
   "source": [
    "###AGGREGATE RATIONALS - AST\n",
    "local_ast_aggregated_results = aggregate_rationals(tagged_results, node_types, pos_types, len(prompts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "###AGGREGATE RATIONALS - TAXONOMY\n",
    "local_taxonomy_aggregated_results = map_local_results_to_taxonomy(pl_taxonomy_python(), nl_pos_taxonomy() ,local_ast_aggregated_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize - AST Aggregation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['1[ Py]', '2[ht]', '3[on]', '4[ code]', '5[ that]', '6[ Test]', '7[ symlink]', '8[ when]', '9[ the]', '10[ target]', '11[ file]', '12[ is]', '13[ a]', '14[ relative]', '15[ path]', '16[\\n   ]', '17[ Should]', '18[ throw]', '19[ a]', '20[ S]', '21[alt]', '22[Inv]', '23[oc]', '24[ationError]', '25[ :]', '26[ it]', '27[ shouldn]', \"28['t]\", '29[ fix]', '30[ this]', '31[\\n   ]', '32[ (]', '33[alth]', '34[ough]', '35[ it]', '36[ is]', '37[ done]', '38[ for]', '39[ this]', '40[ emulation]', '41[.]', '42[\\n   ]', '43[ If]', '44[ the]', '45[ caller]', '46[ works]', '47[ as]', '48[ with]', '49[ the]', '50[ following]', '51[ exception]', '52[,]', '53[ that]', '54[ might]', '55[\\n   ]', '56[ cause]', '57[ a]', '58[ performance]', '59[ read]', '60[ from]', '61[ the]', '62[ (]', '63[i]', '64[.]', '65[e]', '66[.,]', '67[ with]', '68[ `]', '69[Service]', '70[Exception]', '71[`]', '72[ and]', '73[ supporting]', '74[ the]', '75[ l]', '76[uck]', '77[ process]', '78[.]', '79[\\n\\n   ]', '80[ :]', '81[param]', '82[ block]', '83[_]', '84[point]', '85[_]', '86[on]', '87[_]', '88[error]', '89[:]', '90[ the]', '91[ procurement]', '92[ that]', '93[ is]', '94[ only]', '95[ related]', '96[ to]', '97[ it]', '98[.]', '99[\\n\\n   ]', '100[ \"\"\"]', '101[\\n\\n   ]', '102[ @]', '103[private]', '104[\\n   ]', '105[ def]', '106[ best]', '107[_]', '108[open]', '109[_]', '110[context]', '111[(]', '112[self]', '113[,]', '114[ req]', '115[e]', '116[ctions]', '117[:]', '118[ Iterable]', '119[[]', '120[Name]', '121[]:]', '122[\\n       ]', '123[ self]', '124[.]', '125[usage]', '126[(]', '127[\\n           ]', \"128[ ']\", '129[rame]', '130[.]', '131[once]', '132[ is]', '133[ installed]', '134[ in]', '135[ Python]', '136[.]', '137[ There]', '138[ is]', '139[ no]', '140[-]', '141[op]', '142[timal]', '143[...]', '144[\\n   ]', '145[ \"\"\"]', '146[\\n       ]', '147[ if]', '148[ not]', '149[ hasattr]', '150[(]', '151[self]', '152[.]', '153[collection]', '154[.]', '155[name]', '156[):]', '157[\\n           ]'])\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'114[ return]'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [51], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#local_ast_aggregated_results[<sample_id>][<pos[token]>] -> aggregated rationales\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(local_ast_aggregated_results[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mkeys()) \u001b[38;5;66;03m#target tokens\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mlocal_ast_aggregated_results\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m114[ return]\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mkeys()) \u001b[38;5;66;03m#rationales\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(local_ast_aggregated_results[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m114[ return]\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msc_|_identation\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrationales\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[0;31mKeyError\u001b[0m: '114[ return]'"
     ]
    }
   ],
   "source": [
    "#local_ast_aggregated_results[<sample_id>][<pos[token]>] -> aggregated rationales\n",
    "print(local_ast_aggregated_results[0].keys()) #target tokens\n",
    "print(local_ast_aggregated_results[0]['114[ return]'].keys()) #rationales\n",
    "print(local_ast_aggregated_results[0]['114[ return]']['sc_|_identation']['rationales']) #rationales values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize - Taxonomy Aggregation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['1[ base]', '2[64]', '3[\\n]', '4[import]', '5[ pickle]', '6[\\n]', '7[\\n]', '8[from]', '9[ flask]', '10[ import]', '11[ Flask]', '12[,]', '13[ request]', '14[\\n]', '15[app]', '16[ =]', '17[ Flask]', '18[(__]', '19[name]', '20[__)]', '21[\\n]', '22[\\n]', '23[@]', '24[app]', '25[.]', '26[route]', '27[(\"/]', '28[profile]', '29[\")]', '30[\\n]', '31[def]', '32[ profile]', '33[():]', '34[\\n   ]', \"35[ ''']\", '36[\\n   ]', '37[ Get]', '38[ raw]', '39[_]', '40[data]', '41[ from]', '42[ request]', '43[ with]', '44[ utf]', '45[-]', '46[8]', '47[ encoding]', '48[ and]', '49[ decode]', '50[ it]', '51[ with]', '52[ base]', '53[64]', '54[.]', '55[\\n   ]', '56[ De]', '57[serialize]', '58[ the]', '59[ decoded]', '60[ data]', '61[,]', '62[ store]', '63[ it]', '64[ in]', '65[ a]', '66[ variable]', '67[ and]', '68[ return]', '69[ it]', '70[\\n   ]', \"71[ ''']\", '72[\\n   ]', '73[ raw]', '74[_]', '75[data]', '76[ =]', '77[ request]', '78[.]', '79[args]', '80[.]', '81[get]', '82[(\"]', '83[data]', '84[\").]', '85[encode]', \"86[(']\", '87[utf]', '88[-]', '89[8]', \"90[')]\", '91[\\n   ]', '92[ data]', '93[ =]', '94[ base]', '95[64]', '96[.]', '97[decode]', '98[bytes]', '99[(]', '100[raw]', '101[_]', '102[data]', '103[)]', '104[\\n   ]', '105[ obj]', '106[ =]', '107[ pickle]', '108[.]', '109[loads]', '110[(]', '111[data]', '112[)]', '113[\\n   ]', '114[ return]', '115[ obj]', '116[\\n\\n]'])\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'2[,]'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [52], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#local_ast_aggregated_results[<sample_id>][<pos[token]>] -> aggregated rationales\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(local_taxonomy_aggregated_results[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mkeys()) \u001b[38;5;66;03m#target tokens\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mlocal_taxonomy_aggregated_results\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m2[,]\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mkeys()) \u001b[38;5;66;03m#rationales\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(local_taxonomy_aggregated_results[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2[,]\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnl_adverb\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrationales\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[0;31mKeyError\u001b[0m: '2[,]'"
     ]
    }
   ],
   "source": [
    "#local_ast_aggregated_results[<sample_id>][<pos[token]>] -> aggregated rationales\n",
    "print(local_taxonomy_aggregated_results[0].keys()) #target tokens\n",
    "print(local_taxonomy_aggregated_results[0]['2[,]'].keys()) #rationales\n",
    "print(local_taxonomy_aggregated_results[0]['2[,]']['nl_adverb']['rationales']) #rationales values\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
