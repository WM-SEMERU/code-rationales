{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis - Distribution of Semantic and Non Semantic top rationales\n",
    "> Prototyping B: Code Completion\n",
    "\n",
    "This notebook presents boxplots for each identified concept, highlighting both the least and most frequent occurrences. These concepts are grouped into semantically meaningful and non-meaningful categories (level_2). \n",
    "\n",
    "*Notebook Structure*\n",
    "- Notebook parameters\n",
    "- Source Code\n",
    "    - Imports\n",
    "    - Frequency Dataframes\n",
    "    - Data Loading\n",
    "    - Statistics\n",
    "    - Calculate statistics and frequencies\n",
    "- Experiment [1.2.1]\n",
    "- Experiment [1.2.2]\n",
    "\n",
    "*General Instructions*\n",
    "* Collapse cells by tittle to improve the navigation\n",
    "* Before running the experiments, it is required to have global_taxonomy_results for each dataset. global_taxonomy_results contains a the aggregations of rationales by level_1 taxonomomy. \n",
    "* Read description on each experiment for further instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def param_default():\n",
    "    return {\n",
    "        'model': 'codeparrot',\n",
    "        'modality' : 'sc',\n",
    "        'datasets' : {\n",
    "            'SG_BD' : 'code_completion_random_cut_5k_30_512_tokens', \n",
    "            'DC_SG_BD' : 'code_completion_docstring_random_cut_3.8k_30_150_tokens', \n",
    "            'DC_SG' : 'code_completion_docstring_signature_3.8k_30_150_tokens', \n",
    "            'DC': 'code_completion_docstring_5k_30_150_tokens'\n",
    "        },\n",
    "        ######## INPUT\n",
    "        'global_ast_results': '/workspaces/code-rationales/data/global_ast_results',\n",
    "        'grouping_results': '/workspaces/code-rationales/data/experiments/grouping_results', \n",
    "        'quantitative_results' : '/workspaces/code-rationales/data/experiments/quantitative_results',\n",
    "        ######## OUTPUT\n",
    "        'rationales_distributions': '/workspaces/code-rationales/data/experiments/rationales_distributions',\n",
    "        'num_experiments': 30,\n",
    "        'num_samples': 100,\n",
    "        'bootstrapping_size': 1000,\n",
    "    }\n",
    "    \n",
    "params = param_default()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from statistics import NormalDist\n",
    "from venn import venn\n",
    "\n",
    "import seaborn as sns; sns.set_theme()\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "\n",
    "pd.options.display.float_format = '{:.2f}'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from code_rationales.taxonomies import *\n",
    "import os.path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistic Treatments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrapping( np_data, np_func, size ):\n",
    "    \"\"\"Create a bootstrap sample given data and a function\n",
    "    For instance, a bootstrap sample of means, or mediands. \n",
    "    The bootstrap replicates are a long as the original size\n",
    "    we can choose any observation more than once (resampling with replacement:np.random.choice)\n",
    "    \"\"\"\n",
    "    \n",
    "    #Cleaning NaNs\n",
    "    #np_data_clean = np_data[ np.logical_not( np.isnan(np_data) ) ] \n",
    "    \n",
    "    #The size of the bootstrap replicate is as big as size\n",
    "    #Creating the boostrap replicates as long as the orignal data size\n",
    "    #This strategy might work as imputation \n",
    "    bootstrap_repl = [ np_func( np.random.choice( np_data, size=len(np_data) ) ) for i in range( size ) ]\n",
    "    \n",
    "    #logging.info(\"Covariate: \" + cov) #Empirical Mean\n",
    "    #logging.info(\"Empirical Mean: \" + str(np.mean(np_data_clean))) #Empirical Mean\n",
    "    #logging.info(\"Bootstrapped Mean: \" + str( np.mean(bootstrap_repl) ) ) #Bootstrapped Mean\n",
    "    \n",
    "    return np.array( bootstrap_repl )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_experiment_path =  lambda results_folder, dataset, exp: results_folder + '/' + dataset + '_exp_' + str(exp) +'.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ast_rationales_results(dataset_id: str) :\n",
    "    experiment_paths = [get_experiment_path(params['global_ast_results'] + '/' + params['model'] + '/' + params['modality'], params['datasets'][dataset_id], exp) for exp in range(params['num_experiments'])]\n",
    "    experiment_results = []\n",
    "    for experiment_path in experiment_paths:\n",
    "        with open(experiment_path, 'r') as file:\n",
    "            experiment_results.append(json.loads(file.read()))\n",
    "    return experiment_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_rationales_results(dataset_id: str, level: str) :\n",
    "    file_path = params['grouping_results'] + '/' + params['model'] + '/' + params['modality'] + '/' + params['datasets'][dataset_id] + '_' + level + '.json'\n",
    "    with open(file_path, 'r') as file:\n",
    "        return json.loads(file.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flat_rationales_probabilities(rationales_results):\n",
    "    rational_distributions = {}\n",
    "    for experiment_result in rationales_results:\n",
    "        experiments_rationales_values = {rational_key: [] for target_values in experiment_result.values() for rational_key in target_values.keys()}\n",
    "        [experiments_rationales_values[rational_key].extend(rational_values) for target_values in experiment_result.values() for rational_key, rational_values in target_values.items()]\n",
    "        for key, values in experiments_rationales_values.items(): rational_distributions.setdefault(key, []).extend(values)\n",
    "    #for rational_key in rational_distributions.keys(): rational_distributions[rational_key] = bootstrapping(rational_distributions[rational_key], np.mean, params['bootstrapping_size']) ## to reduce dimentionality\n",
    "    return rational_distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rationales_distribution(dataset_id: str, level: str, experiments: int):\n",
    "    rationales_results = load_ast_rationales_results(dataset_id)[:experiments] if level == 'level_0' else load_rationales_results(dataset_id, level)[:experiments]\n",
    "    dataset_rationales_values = flat_rationales_probabilities(rationales_results)\n",
    "    return dataset_rationales_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df_rationales_distributions(level, exp):\n",
    "    file_path = params['rationales_distributions'] + '/' + params['model'] + '/' + params['modality'] + '/' + 'by_category_' + level + '_exp_' + str(exp) + '_bootstrapping_' + str(params['bootstrapping_size']) + '.csv'\n",
    "    if os.path.isfile(file_path):\n",
    "          plot_data = pd.read_csv(file_path, index_col=[0])\n",
    "          return plot_data\n",
    "    rationales_distributions = {key: {} for key in params['datasets'].keys()}\n",
    "    for dataset_id in params['datasets'].keys(): rationales_distributions[dataset_id] = get_rationales_distribution(dataset_id, level, exp)\n",
    "    plot_df = pd.DataFrame(\n",
    "    [(dataset_id, category, value) for dataset_id, categories in rationales_distributions.items() for category, values in categories.items() for value in values],\n",
    "    columns=['dataset_id', 'rational_category', 'rational_value'])\n",
    "    plot_df.to_csv(file_path)\n",
    "    return plot_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment [1.2.1.0.1]\n",
    "**Level 1 most frequent rationales distribution plots**\n",
    "\n",
    "*Research Question: How is the distribution of level_1 most frequent rationales probabilities accross datasets?*\n",
    "\n",
    "General Instructions: Execute this experiment per dataset separately\n",
    "- Change ```model``` in ```param_default```, indicating the name of the model used for inference\n",
    "- Change ```modality``` in ```param_default```, indicating the id of the modality in the prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prototyping Methodology\n",
    "Here we expose the step by step to run this experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: read the dataframe of rationales frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequencies_df = pd.read_csv(params['quantitative_results'] + '/' + params['model'] + '/' + params['modality']  + '/' + params['datasets']['DC'] + '_frequencies_dataset' + '.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: create distributions dataframe from rationales frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rationales_distributions_level_1 = create_df_rationales_distributions('level_1', params['num_experiments'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Get the set of most Frequent rationales across datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_most_frequent = set()\n",
    "for dataset_id in params['datasets'].keys():\n",
    "    top_most_frequent.update(frequencies_df[frequencies_df['dataset_id'] == dataset_id].nlargest(10, 'total')['category'].tolist())\n",
    "top_most_frequent = list(top_most_frequent)\n",
    "top_most_frequent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.float_format', lambda x: '%7f' % x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: show the information in the boxplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_path = params['rationales_distributions'] + '/' + params['model'] + '/' + params['modality'] + '/' + 'by_category_' + 'level_1' + '_exp_' + str(params['num_experiments']) + '_bootstrapping_' + str(params['bootstrapping_size']) + '_most_frequent'\n",
    "with PdfPages(fig_path + '.pdf') as pdf:\n",
    "    fig, axes = plt.subplots(nrows=len(top_most_frequent)*2//2, ncols=2, figsize=(20, 2.5*(len(top_most_frequent)*2//2)))\n",
    "    concept_index = 0\n",
    "    for i in range(0,len(axes.flat),2):\n",
    "        category_df = df_rationales_distributions_level_1[df_rationales_distributions_level_1['rational_category']==top_most_frequent[concept_index]]\n",
    "        g = sns.kdeplot(\n",
    "            data=category_df, \n",
    "            x=\"rational_value\", \n",
    "            hue=\"dataset_id\",\n",
    "            palette = 'husl',\n",
    "            ax=axes.flat[i])\n",
    "        h = sns.boxplot(\n",
    "            data=category_df,\n",
    "            x=\"rational_value\", \n",
    "            y=\"dataset_id\", \n",
    "            fliersize = 2.5, \n",
    "            orient=\"h\",\n",
    "            ax=axes.flat[i+1]\n",
    "        )\n",
    "        g.set(xlabel=f\"[{top_most_frequent[concept_index]}] Density\", ylabel=\"Density\")\n",
    "        h.set(xlabel=f\"[{top_most_frequent[concept_index]}] Distribution\", ylabel=\"Dataset\")\n",
    "        concept_index += 1\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(fig_path + '.png')\n",
    "    pdf.savefig()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prototyping results\n",
    "The following subsections show the generated *level_1* bloxplots of most frequent rationales for all the four datasets, in the two modalitites. \n",
    "To generate the plots from scrach, please modify the following parameters in ```param_default``` at the beginning of the notebook and run the notebook until this point. \n",
    "- ```'model'```\n",
    "- ```'modality'```\n",
    "\n",
    "### Results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment [1.2.1.0.2]\n",
    "**Level 1 less frequent rationales distribution plots**\n",
    "\n",
    "*Research Question: How is the distribution of level_1 less frequent rationales probabilities accross datasets?*\n",
    "\n",
    "General Instructions: Execute this experiment per dataset separately\n",
    "- Change ```model``` in ```param_default```, indicating the name of the model used for inference\n",
    "- Change ```modality``` in ```param_default```, indicating the id of the modality in the prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prototyping Methodology\n",
    "Here we expose the step by step to run this experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Get the set of less Frequent rationales across datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_less_frequent = set()\n",
    "for dataset_id in params['datasets'].keys():\n",
    "    top_less_frequent.update(frequencies_df[frequencies_df['dataset_id'] == dataset_id].nsmallest(10, 'total')['category'].tolist())\n",
    "top_less_frequent = list(top_less_frequent)\n",
    "top_less_frequent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Get the set of less Frequent rationales across datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_path = params['rationales_distributions'] + '/' + params['model'] + '/' + params['modality'] + '/' + 'by_category_' + 'level_1' + '_exp_' + str(params['num_experiments']) + '_bootstrapping_' + str(params['bootstrapping_size']) + '_less_frequent'\n",
    "with PdfPages(fig_path + '.pdf') as pdf:\n",
    "    fig, axes = plt.subplots(nrows=len(top_less_frequent)*2//2, ncols=2, figsize=(20, 2.5*(len(top_less_frequent)*2//2)))\n",
    "    concept_index = 0\n",
    "    for i in range(0,len(axes.flat),2):\n",
    "        category_df = df_rationales_distributions_level_1[df_rationales_distributions_level_1['rational_category']==top_less_frequent[concept_index]]\n",
    "        g = sns.kdeplot(\n",
    "        data=category_df, \n",
    "        x=\"rational_value\", \n",
    "        hue=\"dataset_id\",\n",
    "        palette = 'husl',\n",
    "        ax=axes.flat[i])\n",
    "        h = sns.boxplot(\n",
    "        data=category_df,\n",
    "        x=\"rational_value\", \n",
    "        y=\"dataset_id\", \n",
    "        fliersize = 2.5, \n",
    "        orient=\"h\",\n",
    "        ax=axes.flat[i+1]\n",
    "        )\n",
    "        g.set(xlabel=f\"[{top_less_frequent[concept_index]}] Density\", ylabel=\"Density\")\n",
    "        h.set(xlabel=f\"[{top_less_frequent[concept_index]}] Distribution\", ylabel=\"Dataset\")\n",
    "        concept_index += 1\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(fig_path + '.png')\n",
    "    pdf.savefig()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prototyping results\n",
    "The following subsections show the generated *level_1* bloxplots of less frequent rationales for all the four datasets, in the two modalitites. \n",
    "To generate the plots from scrach, please modify the following parameters in ```param_default``` at the beginning of the notebook and run the notebook until this point. \n",
    "- ```'model'```\n",
    "- ```'modality'```\n",
    "\n",
    "### Results\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
