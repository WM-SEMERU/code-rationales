[
  {
    "objectID": "interpretableshapley.html",
    "href": "interpretableshapley.html",
    "title": "Exploratory Data Code Analysis",
    "section": "",
    "text": "from IPython.core.display import display, HTML\ndisplay(HTML(\"&lt;style&gt;.container { width:100% !important; }&lt;/style&gt;\"))\n\nDeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n  from IPython.core.display import display, HTML\n# ! pip install tokenizer #&lt;---- Unk\n# ! pip install tokenizers #&lt;--- HugginFace\n# ! pip install pandas\n# ! pip install matplotlib\n# ! pip install seaborn\nfrom pathlib import Path\nfrom tokenizers import ByteLevelBPETokenizer #HugginFace\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\npd.options.display.float_format = '{:.2f}'.format\n#export\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\nimport functools\nfrom collections import Counter\nsns.set_theme(style=\"darkgrid\")\n\nNameError: name 'sns' is not defined\ndef param_default():\n    corpus = 'fm_fc_ms_ff' #&lt;-- Scope\n    data_path = Path('../athena-datasets/' + corpus + '/')\n    data_path_raw = Path('../athena-datasets/' + corpus + '/raw/')\n    tokenizer_path = Path('../tokenizer/')\n    return {\n        'bpe_path' : tokenizer_path / 'universal_tokenizer/universal_tokenizer/roberta_aug_spaces',\n        'eval_raw': [data_path_raw / 'eval/input.methods.txt',\n                        data_path_raw / 'eval/output.tests.txt'],\n        'test_raw': [data_path_raw / 'test/input.methods.txt', \n                        data_path_raw / 'test/output.tests.txt'],\n        'train_raw': [data_path_raw / 'train/input.methods.txt', \n                        data_path_raw / 'train/output.tests.txt'],\n        'data_labels' : ['eval_raw','test_raw','train_raw'],\n        'output_pandas' : data_path / 'pandas/'\n    }\nparams = param_default()\nparams['eval_raw']\n\n[PosixPath('../athena-datasets/fm_fc_ms_ff/raw/eval/input.methods.txt'),\n PosixPath('../athena-datasets/fm_fc_ms_ff/raw/eval/output.tests.txt')]"
  },
  {
    "objectID": "interpretableshapley.html#universal-tokenizer",
    "href": "interpretableshapley.html#universal-tokenizer",
    "title": "Exploratory Data Code Analysis",
    "section": "Universal Tokenizer",
    "text": "Universal Tokenizer\n\ndef load_tokenizer( bpe_path ):\n    return ByteLevelBPETokenizer(str(bpe_path)+'-vocab.json',str(bpe_path)+'-merges.txt')\n\n\n#bpe_path = '/tufanodata/work/unit-test-gen/code/universal_tokenizer/universal_tokenizer/roberta_aug_spaces'\ntokenizer = load_tokenizer(params['bpe_path'])\ntokenizer\n\nTokenizer(vocabulary_size=50342, model=ByteLevelBPE, add_prefix_space=False, lowercase=False, dropout=None, unicode_normalizer=None, continuing_subword_prefix=None, end_of_word_suffix=None, trim_offsets=False)"
  },
  {
    "objectID": "interpretableshapley.html#generative-functions",
    "href": "interpretableshapley.html#generative-functions",
    "title": "Exploratory Data Code Analysis",
    "section": "Generative Functions",
    "text": "Generative Functions\n\ndef pretty_print_test(test):\n    print('Working on test number', test['id'])\n    print('The focal method being tested is:\\n',prettify_java(test['source']))\n    print('\\nThe generated unit test is:\\n',prettify_java(test['top-prediciton']))\n    print('\\nThe ground truth unit test is:\\n',prettify_java(test['target']),'\\n-----------------------------------------------------------------\\n')\n\n\ndef prettify_java(minified_java):\n    \"tries to undo Michele's minification. Works decently, although for loops and sets get newlines inserted, and there are no empty lines or comments\"\n    minified_java = minified_java.replace('{','{\\n').replace('}','}\\n').replace(';',';\\n')\n    num_indents = 0\n    pretty_java = ''\n    for line in minified_java.splitlines():\n        if line.lstrip().startswith('}'):\n            num_indents -= 1\n        pretty_java += num_indents*'    '+line+'\\n'\n        if line.endswith('{'):\n            num_indents += 1\n        if line.endswith('}') and not line.lstrip().startswith('}'):\n            num_indents -= 1\n    return pretty_java\n\n\ndef minify(java_code):\n    return ' '.join(java_code.split())\n\n\n#export\ndef method_size_vector( method_vector ):\n    '''Return the size of the tokens for a give method based on id\n        Assuming that method_vector is an array of tokens\n    '''\n    input_ids = [ len(mtd) for mtd in method_vector ]\n    return input_ids\n\n\ndef super_set_code():\n    data = {}\n    for label in params['data_labels']:\n        for val,path_data in enumerate(params[ label ]):\n            df = pd.read_csv( path_data, sep=\"\\n\", header=None, names=[label+str(val)]) #reading file\n            df[label+'_bpe'+str(val)] = [ enc.tokens for enc in tokenizer.encode_batch( df[label+str(val)].values ) ] #bpe\n            df['method_size'+str(val)] = method_size_vector( df[label+'_bpe'+str(val)].values ) #counting tokens\n            data[label+str(val)] =  df  \n        #data[-1].columns = [ label ]\n    return data\n\n\nsuper_data = super_set_code()\n\n\nsuper_data.keys()\n\ndict_keys(['eval_raw0', 'eval_raw1', 'test_raw0', 'test_raw1', 'train_raw0', 'train_raw1'])\n\n\n\nsuper_data['eval_raw0'].head(1)\n\n\n\n\n\n\n\n\neval_raw0\neval_raw_bpe0\nmethod_size0\n\n\n\n\n0\nRosetteAbstractProcessor extends AbstractProce...\n[Ros, ette, Abstract, Process, or, Ġextends, Ġ...\n220\n\n\n\n\n\n\n\n\n#tst\ntokenizer.encode( super_data['eval_raw0'].eval_raw0.values[0] )\n\nEncoding(num_tokens=220, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\n\n\n\n# [Checkpoint] Saving\ndef checkpoint_1(df_dict):\n    for key in df_dict.keys():\n        df = df_dict[key]\n        df.to_json( params['output_pandas'] / (key +'.json') ) #serialization\n    pass\n\n\ncheckpoint_1( super_data ) #&lt;--- Data Checkpoint!\n\n\n# Loading Json Sets\ndef load_checkpoint_1():\n    super_df = {}\n    for label in params['data_labels']:\n        for val, _ in enumerate(params[ label ]):\n            super_df[ label+str(val) ] = pd.read_json( params['output_pandas'] / (label+str(val) +'.json')  )\n            print(\"read:\",label+str(val))\n    return super_df"
  },
  {
    "objectID": "interpretableshapley.html#most-popular",
    "href": "interpretableshapley.html#most-popular",
    "title": "Exploratory Data Code Analysis",
    "section": "Most Popular",
    "text": "Most Popular\n\nTOK = 10\ndict_most = {}\n\nfor dct_cnt in dct_control:\n    dict_most[dct_cnt] = [\n        dict(dct_control[dct_cnt][0].most_common( TOK )) ,\n        dict(dct_control[dct_cnt][1].most_common( TOK ))    \n    ] #&lt;---Hyperparameter\n\n\n#printing most popular tokens\nfor key in dict_most:\n    fig, axs = plt.subplots( figsize=(20,4), ncols=2 )\n    print_token_distribution( [dict_most[key][0]], ax=axs[0], TOK=TOK, label=['inputs'], log=False, title= '['+key+']')\n    print_token_distribution( [dict_most[key][1]], ax=axs[1], TOK=TOK, label=['outputs'], log=False, title= '['+key+']')"
  },
  {
    "objectID": "interpretableshapley.html#special-tokens",
    "href": "interpretableshapley.html#special-tokens",
    "title": "Exploratory Data Code Analysis",
    "section": "Special Tokens",
    "text": "Special Tokens\n\nproposed_token = ['assert','private','public','abstract','static','if','else','for']\n\n\nfocused_tokens_non = { split :  count_by_taxonomy_list( dct_control[split], k_token = proposed_token )  for split in dct_control}\n\nĠpublic : 74951\nĠif : 121810\nĠfor : 31690\nĠstatic : 326130\nĠprivate : 13613\nĠelse : 32562\nfor : 1756\nĠabstract : 4408\nĠassert : 1324\nstatic : 51\nif : 2763\nelse : 641\npublic : 473\nprivate : 140\nassert : 222\nĠpublic : 74597\nĠassert : 127151\nif : 1058\nfor : 1941\nĠfor : 6613\nassert : 41381\nĠif : 3166\nĠelse : 864\nstatic : 134\nelse : 99\npublic : 239\nĠabstract : 238\nĠprivate : 549\nprivate : 214\nĠstatic : 171\nĠpublic : 76347\nĠstatic : 321229\nĠif : 132416\nĠelse : 31483\nĠprivate : 14301\npublic : 278\nprivate : 301\nĠfor : 29161\nif : 3888\nfor : 2401\nĠabstract : 5562\nelse : 363\nĠassert : 2138\nstatic : 280\nassert : 408\nĠpublic : 78445\nĠassert : 132251\nif : 1110\nprivate : 329\npublic : 376\nĠfor : 6152\nĠif : 2884\nĠelse : 835\nassert : 34718\nĠstatic : 174\nstatic : 93\nfor : 1938\nĠprivate : 364\nelse : 49\nĠabstract : 153\nĠpublic : 599563\nĠif : 943072\nĠelse : 241066\nĠfor : 218163\nelse : 2722\nĠstatic : 2578818\nĠprivate : 105628\nĠassert : 10443\nfor : 18392\nĠabstract : 33066\npublic : 2774\nassert : 3168\nstatic : 2217\nif : 25711\nprivate : 1194\nĠpublic : 613329\nĠassert : 1077402\nassert : 236556\nĠfor : 52982\nĠif : 23385\nif : 9502\nfor : 16323\npublic : 3035\nĠelse : 7727\nelse : 525\nĠprivate : 1951\nstatic : 1023\nĠabstract : 873\nĠstatic : 1033\nprivate : 1747\n\n\n\nfocused_tokens = { split :  count_by_taxonomy_list( dct_control[split], k_token = proposed_token , stringent=True )  for split in dct_control}\n#dict( count_by_taxonomy( dct_control, k_token = proposed_token, stringent=True ) )\n\nfor : 1756\nstatic : 51\nif : 2763\nelse : 641\npublic : 473\nprivate : 140\nassert : 222\nif : 1058\nfor : 1941\nassert : 41381\nstatic : 134\nelse : 99\npublic : 239\nprivate : 214\npublic : 278\nprivate : 301\nif : 3888\nfor : 2401\nelse : 363\nstatic : 280\nassert : 408\nif : 1110\nprivate : 329\npublic : 376\nassert : 34718\nstatic : 93\nfor : 1938\nelse : 49\nelse : 2722\nfor : 18392\npublic : 2774\nassert : 3168\nstatic : 2217\nif : 25711\nprivate : 1194\nassert : 236556\nif : 9502\nfor : 16323\npublic : 3035\nelse : 525\nstatic : 1023\nprivate : 1747\n\n\n\nfocused_tokens_non\n\n{'eval_raw': [{'assert': 1546,\n   'private': 13753,\n   'public': 75424,\n   'abstract': 4408,\n   'static': 326181,\n   'if': 124573,\n   'else': 33203,\n   'for': 33446},\n  {'assert': 168532,\n   'private': 763,\n   'public': 74836,\n   'abstract': 238,\n   'static': 305,\n   'if': 4224,\n   'else': 963,\n   'for': 8554}],\n 'test_raw': [{'assert': 2546,\n   'private': 14602,\n   'public': 76625,\n   'abstract': 5562,\n   'static': 321509,\n   'if': 136304,\n   'else': 31846,\n   'for': 31562},\n  {'assert': 166969,\n   'private': 693,\n   'public': 78821,\n   'abstract': 153,\n   'static': 267,\n   'if': 3994,\n   'else': 884,\n   'for': 8090}],\n 'train_raw': [{'assert': 13611,\n   'private': 106822,\n   'public': 602337,\n   'abstract': 33066,\n   'static': 2581035,\n   'if': 968783,\n   'else': 243788,\n   'for': 236555},\n  {'assert': 1313958,\n   'private': 3698,\n   'public': 616364,\n   'abstract': 873,\n   'static': 2056,\n   'if': 32887,\n   'else': 8252,\n   'for': 69305}]}\n\n\n\n#printing selected (interesting) tokens\n#print_token_distribution( [focused_tokens_non,focused_tokens] , label=['non-Stringent','Stringent'], log=True )\n\n#printing most popular tokens\nfor key in params['data_labels']:\n    fig, axs = plt.subplots( figsize=(20,4), ncols=2 )\n    print_token_distribution( [focused_tokens_non[key][0],focused_tokens[key][0]], ax=axs[0],  label=['inputs [non-Stringent]','inputs [Stringent]'], log=True, title='['+key+']' )\n    print_token_distribution( [focused_tokens_non[key][1],focused_tokens[key][1]], ax=axs[1],  label=['outputs [non-Stringent]', 'outputs [Stringent]'], log=True, title='['+key+']' )"
  },
  {
    "objectID": "interpretableshapley.html#taxonomy-countings",
    "href": "interpretableshapley.html#taxonomy-countings",
    "title": "Exploratory Data Code Analysis",
    "section": "Taxonomy Countings",
    "text": "Taxonomy Countings\n\n#export\ndef token_taxonomy() -&gt; dict:\n    return {\n  \"blocks\": {\n    \"&lt;{&gt;\": \"{\",\n    \"&lt;}&gt;\": \"}\",\n    \"&lt;[&gt;\": \"[\",\n    \"&lt;]&gt;\": \"]\",\n    \"&lt;(&gt;\": \"(\",\n    \"&lt;)&gt;\": \")\",\n    \"&lt;;&gt;\": \";\",\n    \"&lt;return&gt;\": \"return\"\n  },\n  \"exceptions\": {\n    \"&lt;catch&gt;\": \"catch\",\n    \"&lt;try&gt;\": \"try\",\n    \"&lt;finally&gt;\": \"finally\",\n    \"&lt;throw&gt;\": \"throw\",\n    \"&lt;throws&gt;\": \"throws\"\n  },\n  \"oop\": {\n    \"&lt;class&gt;\": \"class\",\n    \"&lt;instanceof&gt;\": \"instanceof\",\n    \"&lt;interface&gt;\": \"interface\",\n    \"&lt;private&gt;\": \"private\",\n    \"&lt;protected&gt;\": \"protected\",\n    \"&lt;public&gt;\": \"public\",\n    \"&lt;abstract&gt;\": \"abstract\",\n    \"&lt;extends&gt;\": \"extends\",\n    \"&lt;package&gt;\": \"package\",\n    \"&lt;this&gt;\": \"this\",\n    \"&lt;implements&gt;\": \"implements\",\n    \"&lt;import&gt;\": \"import\",\n    \"&lt;new&gt;\": \"new\",\n    \"&lt;super&gt;\": \"super\"\n  },\n  \"tests\": {\n    \"&lt;assert&gt;\": \"assert\"\n  },\n  \"declarations\": {\n    \"&lt;native&gt;\": \"native\",\n    \"&lt;static&gt;\": \"static\",\n    \"&lt;synchronized&gt;\": \"synchronized\",\n    \"&lt;transient&gt;\": \"transient\",\n    \"&lt;volatile&gt;\": \"volatile\",\n    \"&lt;void&gt;\": \"void\",\n    \"&lt;final&gt;\": \"final\",\n    \"&lt;enum&gt;\": \"enum\"\n  },\n  \"conditionals\": {\n    \"&lt;else&gt;\": \"else\",\n    \"&lt;if&gt;\": \"if\",\n    \"&lt;switch&gt;\": \"switch\",\n    \"&lt;case&gt;\": \"case\",\n    \"&lt;default&gt;\": \"default\"\n  },\n  \"loops\": {\n    \"&lt;break&gt;\": \"break\",\n    \"&lt;do&gt;\": \"do\",\n    \"&lt;for&gt;\": \"for\",\n    \"&lt;while&gt;\": \"while\",\n    \"&lt;continue&gt;\": \"continue\"\n  },\n  \"operators\": {\n    \"&lt;=&gt;\": \"=\",\n    \"&lt;+&gt;\": \"+\",\n    \"&lt;-&gt;\": \"-\",\n    \"&lt;*&gt;\": \"*\",\n    \"&lt;/&gt;\": \"/\",\n    \"&lt;%&gt;\": \"%\",\n    \"&lt;++&gt;\": \"++\",\n    \"&lt;--&gt;\": \"--\",\n    \"&lt;!&gt;\": \"!\",\n    \"&lt;==&gt;\": \"==\",\n    \"&lt;!=&gt;\": \"!=\",\n    \"&lt;greater_equal&gt;\": \"&gt;=\",\n    \"&lt;lesser_equal&gt;\": \"&lt;=\",\n    \"&lt;&&&gt;\": \"&&\",\n    \"&lt;||&gt;\": \"||\",\n    \"&lt;?&gt;\": \"?\",\n    \"&lt;:&gt;\": \":\",\n    \"&lt;~&gt;\": \"~\",\n    \"&lt;double_lesser&gt;\": \"&lt;&lt;\",\n    \"&lt;double_greater&gt;\": \"&gt;&gt;\",\n    \"&lt;triple_greater&gt;\": \"&gt;&gt;&gt;\",\n    \"&lt;&&gt;\": \"&\",\n    \"&lt;^&gt;\": \"^\",\n    \"&lt;|&gt;\": \"|\"\n  },\n  \"datatypes\": {\n    \"&lt;byte&gt;\": \"byte\",\n    \"&lt;char&gt;\": \"char\",\n    \"&lt;float&gt;\": \"float\",\n    \"&lt;boolean&gt;\": \"boolean\",\n    \"&lt;double&gt;\": \"double\",\n    \"&lt;int&gt;\": \"int\",\n    \"&lt;long&gt;\": \"long\",\n    \"&lt;short&gt;\": \"short\",\n    \"&lt;strictfp&gt;\": \"strictfp\"\n  },\n  \"extra_tokens\": {\n    \"&lt;@&gt;\": \"@\",\n    \"&lt;...&gt;\": \"...\",\n    \"&lt;null&gt;\": \"null\",\n    \"&lt;true&gt;\": \"true\",\n    \"&lt;false&gt;\": \"false\",\n    \"&lt;n&gt;\": \"\\n\"\n  }\n}\n\n\ntkn_taxonomy = token_taxonomy()\n\n\ntkn_taxonomy['extra_tokens'].values()\n\ndict_values(['@', '...', 'null', 'true', 'false', '\\n'])\n\n\n\ndct_control.keys()\n\ndict_keys(['eval_raw', 'test_raw', 'train_raw'])\n\n\n\ndef taxonomy_counting_v1( tkn_taxonomy, dct_control, stringent=False ):\n    ''''Hardcoded version of taxonomy counting'''\n    dict_tokens = {}\n    for split in dct_control:\n        to_flat_tax_dict = { tax :  count_by_taxonomy_list( dct_control[split], k_token = tkn_taxonomy[tax].values() , stringent=stringent )  for tax in tkn_taxonomy }\n        to_flat_tax_dict = { tax : [ functools.reduce(lambda a, b: a+b, list_dict.values() ) for list_dict in to_flat_tax_dict[tax] ] for tax in to_flat_tax_dict }\n        dict_tokens[split] = [ \n                { tax: to_flat_tax_dict[tax][0] for tax in to_flat_tax_dict }, #Hardcoded Input\n                { tax: to_flat_tax_dict[tax][1] for tax in to_flat_tax_dict } #Hardcoded Output\n            ]\n    return dict_tokens\n\n\nflat_dict_foc_tax_non = taxonomy_counting_v1( tkn_taxonomy, dct_control, stringent=False )\nflat_dict_foc_tax_str = taxonomy_counting_v1( tkn_taxonomy, dct_control, stringent=True )\n\n\n#flat_dict_foc_tax_str\n#print_token_distribution( [flat_dict_foc_tax_non,flat_dict_foc_tax] , label=['non-Stringent','Stringent'], log=True, TOK=15 )\n\nfor key in params['data_labels']:\n    fig, axs = plt.subplots( figsize=(30,4), ncols=2 )\n    print_token_distribution( [flat_dict_foc_tax_non[key][0],flat_dict_foc_tax_str[key][0]], ax=axs[0],  label=['inputs [non-Stringent]','inputs [Stringent]'], log=True, title='['+key+']' )\n    print_token_distribution( [flat_dict_foc_tax_non[key][1],flat_dict_foc_tax_str[key][1]], ax=axs[1],  label=['outputs [non-Stringent]', 'outputs [Stringent]'], log=True, title='['+key+']' )"
  },
  {
    "objectID": "grammar_loader.html",
    "href": "grammar_loader.html",
    "title": "Grammar Loading Module",
    "section": "",
    "text": "source\n\ndownload_grammars\n\n download_grammars (languages)\n\nDownload Tree-sitter grammars\n\n\n\n\n\n\n\n\nDetails\n\n\n\n\nlanguages\nlanguages: Param(“Languages to download”, str, nargs=“+”) = “all”,"
  },
  {
    "objectID": "bart_global_rationalization.html",
    "href": "bart_global_rationalization.html",
    "title": "Rationalization @ Global Granularity",
    "section": "",
    "text": "from pathlib import Path\nimport csv\nimport seaborn as sns; sns.set_theme()\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nimport functools\n\npd.options.display.float_format = '{:.2f}'.format\nfrom sacrebleu.metrics import BLEU\nfrom tokenizers import ByteLevelBPETokenizer\nimport torch\nimport importlib\nfrom fairseq.models.transformer import TransformerModel\nimport warnings\nfrom matplotlib import colors\nimport os\nfrom rationalization import rationalize_lm, rationalize_conditional_model\ndef param_default():\n    corpus = 'fm_fc_ms_ff' #&lt;-- Scope\n    data_path = Path('../athena-datasets/' + corpus + '/')\n    data_path_raw = Path('../athena-datasets/' + corpus + '/raw/')\n    tokenizer_path = Path('../tokenizer/')\n    return {\n        'bpe_path' : tokenizer_path / 'universal_tokenizer/universal_tokenizer/roberta_aug_spaces',\n        'eval_raw': [data_path_raw / 'eval/input.methods.txt',\n                        data_path_raw / 'eval/output.tests.txt'],\n        'test_raw': [data_path_raw / 'test/input.methods.txt', \n                        data_path_raw / 'test/output.tests.txt'],\n        'train_raw': [data_path_raw / 'train/input.methods.txt', \n                        data_path_raw / 'train/output.tests.txt'],\n        'data_labels' : ['test_raw'],#['eval_raw','test_raw','train_raw'], &lt;----- Just Test\n        'output_pandas' : data_path / 'pandas/',\n        'out_processed' : '/datasets/out_processed/',\n        'model_name_or_path' : 'models/checkpoint_dir_01/models/', #Model Path\n        'checkpoint_file': 'checkpoint_best.pt', #Model\n        'data_preprocessed':'/home/davidna/data/dummy/sequential-rationales/fairseq/fairseq/data-bin/bins/',\n        'output_results' : 'results/' \n    }\nparams = param_default()\nparams['output_results']\n\n'results/'"
  },
  {
    "objectID": "bart_global_rationalization.html#rationalizations-utilities",
    "href": "bart_global_rationalization.html#rationalizations-utilities",
    "title": "Rationalization @ Global Granularity",
    "section": "Rationalizations Utilities",
    "text": "Rationalizations Utilities\n\nrationalization = importlib.import_module(\"sequential-rationales.huggingface.rationalization\")\nrationalize = rationalization.rationalize_lm\nwarnings.filterwarnings(\"ignore\")"
  },
  {
    "objectID": "bart_global_rationalization.html#universal-tokenizer",
    "href": "bart_global_rationalization.html#universal-tokenizer",
    "title": "Rationalization @ Global Granularity",
    "section": "Universal Tokenizer",
    "text": "Universal Tokenizer\n\ndef lazy_decode(bpe_java):\n    return bpe_java.replace(' ','').replace('Ġ',' ').replace('Ċ','\\n')\n\n\ndef prettify_java(minified_java):\n    \"tries to undo Michele's minification. Works decently, although for loops and sets get newlines inserted, and there are no empty lines or comments\"\n    minified_java = minified_java.replace('{','{\\n').replace('}','}\\n').replace(';',';\\n')\n    num_indents = 0\n    pretty_java = ''\n    for line in minified_java.splitlines():\n        if line.lstrip().startswith('}'):\n            num_indents -= 1\n        pretty_java += num_indents*'    '+line+'\\n'\n        if line.endswith('{'):\n            num_indents += 1\n        if line.endswith('}') and not line.lstrip().startswith('}'):\n            num_indents -= 1\n    return pretty_java"
  },
  {
    "objectID": "bart_global_rationalization.html#model-loading-and-testing",
    "href": "bart_global_rationalization.html#model-loading-and-testing",
    "title": "Rationalization @ Global Granularity",
    "section": "Model Loading and Testing",
    "text": "Model Loading and Testing\n\n#Loading a pretrain model\nmodel = TransformerModel.from_pretrained(\n  model_name_or_path = params['model_name_or_path'],\n  checkpoint_file = params['checkpoint_file'],\n  #data_name_or_path = params['data_preprocessed']\n)\n\n\n#Setting experiments \n#! export CUDA_VISIBLE_DEVICES=\"0,1\"\n\n\n## Move model to GPU if available and trigger evaluation mode\ndef model_activate(model = model):\n  if torch.cuda.is_available():\n    model.cuda()\n    model.eval()\n    model.model = model.models[0]\n    model.device\n    print(\"Model Activated\")\n  pass"
  },
  {
    "objectID": "bart_global_rationalization.html#data-loading-and-testing",
    "href": "bart_global_rationalization.html#data-loading-and-testing",
    "title": "Rationalization @ Global Granularity",
    "section": "Data Loading and Testing",
    "text": "Data Loading and Testing\n\n#Loading Code Generation\ndf_generated_input = pd.read_json( params['output_results'] + '1_generation_[max:100]_02.json' )\n\n\ndf_generated_input.columns[4:] #Tensor Columns\n\nIndex(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12',\n       '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24',\n       '25', '26', '27', '28', '29', 'source_sampling'],\n      dtype='object')\n\n\n\nprint('df readit')\ndf_generated_input.head()\n\ndf readit\n\n\n\n\n\n\n\n\n\nindex\ntest_raw0\ntest_raw_bpe0\nmethod_size0\n0\n1\n2\n3\n4\n5\n...\n21\n22\n23\n24\n25\n26\n27\n28\n29\nsource_sampling\n\n\n\n\n0\n3610\nCitizenId implements Identity { @Override publ...\n[C, itizen, Id, Ġimplements, ĠIdentity, Ġ{, Ġ@...\n96\n[1039, 34603, 285, 13842, 1296, 20320, 32890, ...\n[1039, 34603, 285, 13842, 1296, 20320, 32890, ...\n[1039, 34603, 285, 13842, 28754, 1215, 17276, ...\n[1039, 34603, 285, 13842, 8218, 1215, 17276, 4...\n[1039, 34603, 285, 13842, 8218, 1215, 17276, 1...\n[1039, 34603, 285, 13842, 8218, 1215, 17276, 1...\n...\n[1039, 34603, 285, 13842, 8218, 1215, 17276, 4...\n[1039, 34603, 285, 13842, 28754, 1215, 17276, ...\n[1039, 34603, 285, 13842, 28754, 1215, 17276, ...\n[1039, 34603, 285, 13842, 8218, 1215, 17276, 4...\n[1039, 34603, 285, 13842, 8218, 1215, 17276, 1...\n[1039, 34603, 285, 13842, 1296, 20320, 32890, ...\n[1039, 34603, 285, 13842, 28754, 1215, 17276, ...\n[1039, 34603, 285, 13842, 1296, 20320, 32890, ...\n[1039, 34603, 285, 13842, 1296, 20320, 32890, ...\n[347, 31020, 28081, 36987, 30805, 25522, 787, ...\n\n\n1\n23609\nGetDMNMessagesCommand extends AbstractDMNResul...\n[Get, DM, NM, ess, ages, Command, Ġextends, ĠA...\n70\n[1039, 34603, 285, 13842, 1296, 46891, 4467, 4...\n[1039, 34603, 285, 13842, 1296, 46891, 4467, 4...\n[1039, 34603, 285, 13842, 1296, 46891, 4467, 4...\n[1039, 34603, 285, 13842, 1296, 46891, 4467, 4...\n[1039, 34603, 285, 13842, 1296, 46891, 4467, 4...\n[1039, 34603, 285, 13842, 1296, 46891, 4467, 4...\n...\n[1039, 34603, 285, 13842, 1296, 46891, 4467, 4...\n[1039, 34603, 285, 13842, 1296, 46891, 4467, 4...\n[1039, 34603, 285, 13842, 1296, 43048, 25522, ...\n[1039, 34603, 285, 13842, 1296, 43048, 25522, ...\n[1039, 34603, 285, 13842, 1296, 46891, 4467, 4...\n[1039, 34603, 285, 13842, 1296, 46891, 4467, 4...\n[1039, 34603, 285, 13842, 1296, 46891, 4467, 4...\n[1039, 34603, 285, 13842, 1296, 46891, 4467, 4...\n[1039, 34603, 285, 13842, 1296, 46891, 4467, 4...\n[14181, 25652, 31156, 3361, 3443, 46785, 14269...\n\n\n2\n45764\nCalculator { public double div(double firstOpe...\n[Cal, cul, ator, Ġ{, Ġpublic, Ġdouble, Ġdiv, (...\n95\n[1039, 34603, 285, 13842, 1296, 37165, 43048, ...\n[1039, 34603, 285, 13842, 1296, 37165, 43048, ...\n[1039, 34603, 285, 13842, 1296, 37165, 43048, ...\n[1039, 34603, 285, 13842, 14445, 43048, 25522,...\n[1039, 34603, 285, 13842, 1296, 37165, 43048, ...\n[1039, 34603, 285, 13842, 14445, 9058, 47395, ...\n...\n[1039, 34603, 285, 13842, 1296, 37165, 43048, ...\n[1039, 34603, 285, 13842, 1296, 37165, 43048, ...\n[1039, 34603, 285, 13842, 1296, 37165, 43048, ...\n[1039, 34603, 285, 13842, 14445, 43048, 25522,...\n[1039, 34603, 285, 13842, 1296, 37165, 43048, ...\n[1039, 34603, 285, 13842, 14445, 43048, 25522,...\n[1039, 34603, 285, 13842, 1296, 37165, 43048, ...\n[1039, 34603, 285, 13842, 14445, 43048, 25522,...\n[1039, 34603, 285, 13842, 14445, 43048, 25522,...\n[15117, 13300, 2630, 25522, 285, 1457, 14445, ...\n\n\n3\n6263\nAsyncSqsClientFactory { public AsyncSqsClient ...\n[Async, S, qs, Client, Factory, Ġ{, Ġpublic, Ġ...\n55\n[1039, 34603, 285, 13842, 197, 44758, 47952, 4...\n[1039, 34603, 285, 13842, 197, 44758, 47952, 4...\n[1039, 34603, 1640, 10162, 5457, 44840, 26170,...\n[1039, 34603, 285, 13842, 197, 44758, 49636, 1...\n[1039, 34603, 285, 13842, 197, 44758, 47952, 4...\n[1039, 34603, 285, 13842, 197, 44758, 49636, 1...\n...\n[1039, 34603, 1640, 10162, 5457, 44840, 26170,...\n[1039, 34603, 1640, 10162, 5457, 44840, 26170,...\n[1039, 34603, 1640, 10162, 5457, 44840, 26170,...\n[1039, 34603, 1640, 10162, 5457, 44840, 26170,...\n[1039, 34603, 1640, 10162, 5457, 44840, 26170,...\n[1039, 34603, 1640, 10162, 5457, 44840, 26170,...\n[1039, 34603, 1640, 10162, 5457, 44840, 26170,...\n[1039, 34603, 1640, 10162, 5457, 44840, 26170,...\n[1039, 34603, 1640, 10162, 5457, 44840, 26170,...\n[49636, 104, 44458, 47952, 47249, 25522, 285, ...\n\n\n4\n29527\nStore { Observable&lt;T&gt; get() { if(obj == null) ...\n[Store, Ġ{, ĠObserv, able, &lt;, T, &gt;, Ġget, (), ...\n88\n[1039, 34603, 285, 13842, 1296, 14181, 43048, ...\n[1039, 34603, 285, 13842, 1296, 14181, 43048, ...\n[1039, 34603, 285, 13842, 1296, 14181, 43048, ...\n[1039, 34603, 285, 13842, 1296, 14181, 43048, ...\n[1039, 34603, 285, 13842, 1296, 14181, 43048, ...\n[1039, 34603, 285, 13842, 1296, 14181, 43048, ...\n...\n[1039, 34603, 285, 13842, 1296, 14181, 43048, ...\n[1039, 34603, 285, 13842, 1296, 14181, 43048, ...\n[1039, 34603, 285, 13842, 1296, 14181, 43048, ...\n[1039, 34603, 285, 13842, 1296, 14181, 43048, ...\n[1039, 34603, 285, 13842, 1296, 14181, 43048, ...\n[1039, 34603, 285, 13842, 1296, 14181, 43048, ...\n[1039, 34603, 285, 13842, 1296, 14181, 43048, ...\n[1039, 34603, 285, 13842, 1296, 14181, 43048, ...\n[1039, 34603, 285, 13842, 1296, 14181, 43048, ...\n[40266, 25522, 42231, 868, 41552, 565, 15698, ...\n\n\n\n\n5 rows × 35 columns\n\n\n\n\ndf_generated_input.shape\n\n(1000, 35)\n\n\n\n#tst decoding\ndecoded = model.decode(df_generated_input['0'][0])\ndecoded\n\n'@ Test Ġpublic Ġvoid Ġtest Val idate () Ġ{ Ġassert True ( c itizen Id . valid ate ()); Ġassert False ( c itizen Id . valid ate ()); Ġ}'\n\n\n\nprettify_java( lazy_decode( decoded ) )\n\n'@Test public void testValidate() {\\n     assertTrue(citizenId.validate());\\n     assertFalse(citizenId.validate());\\n }\\n'"
  },
  {
    "objectID": "bart_global_rationalization.html#running-rationales",
    "href": "bart_global_rationalization.html#running-rationales",
    "title": "Rationalization @ Global Granularity",
    "section": "Running Rationales",
    "text": "Running Rationales\n\n#Statistics\nnp.mean( [len(i) for i in df_generated_input['0'].values] )\n\n83.684\n\n\n\n#TODO Run the distribution of each experiment. The mean value of tokens or size for each experiment. \nnp.mean( [len(i) for i in df_generated_input['source_sampling'].values] )\n\n74.758\n\n\n\nlen(df_generated_input['0'].values[2])\n\n40\n\n\n\nMAX_TOKEN_SIZE = 150 #Hardocoded!!\n\n\n#If the model is not fine-tuned or compatible, it will rise an error\n#Bear in mind that Athena is a Translation model (not a language one)\n#This function works for one tensor of source token and one tensor of target tokens\ndef rationalize_model(t_source_tokens, t_target_tokens, model, verbose=True):\n    all_source_rationales, all_target_rationales, log = rationalize_conditional_model(\n        model = model, \n        source_tokens = t_source_tokens, #[:MAX_TOKEN_SIZE],\n        target_tokens = t_target_tokens[:MAX_TOKEN_SIZE], \n        verbose=verbose,\n        max_steps=1024 #Max number of steps for greedy rationalization\n    )\n    return all_source_rationales, all_target_rationales, log\n\n\n#tst &lt;--- TestCase1\ndef tst_rationalize_model():\n    gc.collect()\n    torch.cuda.empty_cache() #Cleaning Cache\n    model_activate(model = model)\n\n    t_dict_generated_input = { exp : [ torch.tensor(s).to(model.device) for \n                s in df_generated_input[exp].values ] for exp in df_generated_input.columns[4:]  } #Tensor Columns only\n\n    rationalize_model(  \n        t_source_tokens =  t_dict_generated_input['source_sampling'][0],\n        t_target_tokens =  t_dict_generated_input['0'][0],\n        model = model \n    )\n    pass\n\n#tst_rationalize_model()\n\n\ndef run_multiple_rational(\n    arr_source_tokens, \n    arr_target_tokens, \n    model, \n    seq_id, #mapping sequence id\n    verbose=True\n):\n    arr_log = []\n    for index,val in enumerate( arr_source_tokens ):\n        _, _, log = rationalize_model(\n            t_source_tokens = val, \n            t_target_tokens = arr_target_tokens[index], \n            model = model,\n            verbose = verbose )\n        arr_log.append(log)\n    arr_code_rationales = [ log['rationalizations'] for log in arr_log ] #extracting just rationalizations\n    arr_from_sentence = [ list(np.full( len(val), seq_id[arr_i] )) #arr_i maps to the real sequence id\n                            for arr_i, val in enumerate(arr_code_rationales)]\n    \n    arr_code_rationales = sum( arr_code_rationales, [] ) #flatting\n    arr_from_sentence = sum( arr_from_sentence, [] ) #flatting\n    \n    return arr_code_rationales, arr_from_sentence\n    #return arr_code_rationales\n\n\nimport gc\n\n\n#tst &lt;------- Test Case 2\ndef tst_run_multiple_rationa():\n    \n    gc.collect()\n    torch.cuda.empty_cache() #Cleaning Cache\n    model_activate(model = model)\n\n    t_dict_generated_input = { exp : [ torch.tensor(s).to(model.device) for \n                s in df_generated_input[exp].values ] for exp in df_generated_input.columns[4:]  }\n    \n    arr_rations, seq_id = run_multiple_rational(\n        arr_source_tokens =  t_dict_generated_input['source_sampling'][:2], #With 2 Sequences  \n        arr_target_tokens =  t_dict_generated_input['0'][:2], \n        model = model,\n        seq_id = list( range(2,4) ),\n        verbose = False\n        )\n    return arr_rations, seq_id\n#tst_arr_rations, seq_id = tst_run_multiple_rationa()\n\nModel Activated\n\n\n\ndef pandas_rationales( arr_code_rationales, arr_from_sentence ):\n    #Creating pandas_1 {p_rationale}\n    rational = lambda list_log,typeset: [ (dict_tok['added_token_text'],round(dict_tok['true_token_prob'],6)) for dict_tok in list_log if dict_tok['from']==typeset]\n    log_from = lambda log_row,typeset: [(log_dict['added_token_text'],log_dict['true_token_prob']) for log_dict in log_row if log_dict['from']==typeset] #Typeset\n\n    log_position = lambda log_row,typeset: [log_dict['added_token_position'] for log_dict in log_row if log_dict['from']==typeset] #Position of the Rationale\n    log_prediction = lambda log_row,typeset: [log_dict['true_token_prob'] for log_dict in log_row if log_dict['from']==typeset] #Rationale Prob\n\n    p_rationale = pd.DataFrame()\n\n    p_rationale['goal_token'] = [dict_token['goal_word'] for dict_token in arr_code_rationales]\n    p_rationale['from_seq_id'] = arr_from_sentence\n\n    p_rationale['typesets_tgt'] = [ log_from(log_row,'target') for log_row in [dict_token['log'] for dict_token in arr_code_rationales]]\n    p_rationale['typesets_src'] = [ log_from(log_row,'source') for log_row in [dict_token['log'] for dict_token in arr_code_rationales]]\n\n\n    p_rationale['rationale_pos_tgt'] = [ log_position(log_row,'target') for log_row in [dict_token['log'] for dict_token in arr_code_rationales]]\n    p_rationale['rationale_pos_src'] = [ log_position(log_row,'source') for log_row in [dict_token['log'] for dict_token in arr_code_rationales]]\n\n    p_rationale['rationale_prob_tgt'] = [ log_prediction(log_row,'target') for log_row in [dict_token['log'] for dict_token in arr_code_rationales]]\n    p_rationale['rationale_prob_src'] = [ log_prediction(log_row,'source') for log_row in [dict_token['log'] for dict_token in arr_code_rationales]]\n\n    return p_rationale\n\n\n#Running Rationalization\ndef run_code_rational( \n        df_generated_input,\n        tensor_size, #Control the size of the experiment\n        experiment = '5',\n        batch_size = 100, \n        model = model, \n        verbose = True \n    ):\n\n    arr_rationals = []\n    arr_from_seq = []\n\n    for i in range( 0 , tensor_size , batch_size ):\n        model_activate(model = model)\n        print('************************' + str(i) + '************************')\n        t_generated_input = df_generated_input[ experiment ].values[i:i+batch_size]\n        t_source_sampling = df_generated_input['source_sampling'].values[i:i+batch_size]\n\n        t_generated_input = [ torch.tensor(s).to(model.device) for s in t_generated_input]\n        t_source_sampling = [ torch.tensor(s).to(model.device) for s in t_source_sampling]\n\n        \n        t_arr_rationals,t_arr_from_seq = run_multiple_rational(\n            arr_source_tokens =  t_source_sampling, \n            arr_target_tokens =  t_generated_input, \n            model = model,\n            seq_id = list( range(i,i+batch_size) ),\n            verbose = verbose\n        )\n\n        arr_rationals = arr_rationals + t_arr_rationals\n        arr_from_seq = arr_from_seq + t_arr_from_seq\n\n        gc.collect()\n        torch.cuda.empty_cache() #Cleaning Cache\n\n    #keys_tensor = list( dict_generated_input.keys() )\n    #keys_tensor = keys_tensor[:1] #HardCoded Ratios\n    #dict_arr_rations = { key : for key in keys_tensor}\n    #torch.cuda.empty_cache() #Cleaning Cache\n    print(\"Experiment Finished: \" + experiment)\n    return pandas_rationales( arr_rationals, arr_from_seq )\n\n\ngc.collect()\ntorch.cuda.empty_cache()\n\n\ntorch.cuda.is_available()\n\nTrue\n\n\n\n#tst\ndef tst_run_code_rational_sampling_set(exp='0'):\n    gc.collect()\n    torch.cuda.empty_cache()\n    tensor_n = 3 #df_generated_input.shape[0]\n    EXP = exp\n    BATCH = 1\n    test_arr_rationals = run_code_rational( \n            df_generated_input = df_generated_input.sample( n = tensor_n, replace = False, random_state=2),\n            tensor_size = tensor_n,\n            experiment = EXP,\n            batch_size = BATCH, \n            model = model, \n            verbose = False \n        )\n    #Saving process\n    #print('Saving process')\n    #test_arr_rationals.to_json( params['output_results'] + 'rationales_[t_1000]_[max_100]_02_' + EXP )\n    return test_arr_rationals\n#df_test_run = tst_run_code_rational_sampling_set()\n\nModel Activated\n************************0************************\nModel Activated\n************************1************************\nModel Activated\n************************2************************\nExperiment Finished: 0\n\n\n\n#tst\n#df_test_run[ df_test_run['from_seq_id'] == 1]\n\n\n\n\n\n\n\n\ngoal_token\nfrom_seq_id\ntypesets_tgt\ntypesets_src\nrationale_pos_tgt\nrationale_pos_src\nrationale_prob_tgt\nrationale_prob_src\n\n\n\n\n125\nTest\n1\n[(@, 0.9980469942092896)]\n[]\n[0]\n[]\n[0.9980469942092896]\n[]\n\n\n126\nĠpublic\n1\n[(Test, 0.9563336968421936)]\n[]\n[1]\n[]\n[0.9563336968421936]\n[]\n\n\n127\nĠvoid\n1\n[(Ġpublic, 0.9999794960021973)]\n[]\n[2]\n[]\n[0.9999794960021973]\n[]\n\n\n128\nĠtest\n1\n[(Ġvoid, 0.914583683013916)]\n[]\n[3]\n[]\n[0.914583683013916]\n[]\n\n\n129\nPar\n1\n[(Ġtest, 0.0033622581977397203)]\n[(Ġparse, 0.18080583214759827), (Ġtext, 0.9371...\n[4]\n[37, 41]\n[0.0033622581977397203]\n[0.18080583214759827, 0.937179446220398]\n\n\n130\nse\n1\n[(Par, 0.9929555654525757)]\n[]\n[5]\n[]\n[0.9929555654525757]\n[]\n\n\n131\nEmpty\n1\n[(se, 0.015842923894524574), (Ġpublic, 0.01932...\n[(Ġ}, 0.025482192635536194), (Ġstatic, 0.06598...\n[6, 2]\n[29, 5, 23]\n[0.015842923894524574, 0.01932516135275364]\n[0.025482192635536194, 0.06598009169101715, 0....\n\n\n132\n()\n1\n[(Empty, 0.16571001708507538)]\n[((, 0.7976974844932556)]\n[7]\n[39]\n[0.16571001708507538]\n[0.7976974844932556]\n\n\n133\nĠ{\n1\n[((), 0.6141546368598938)]\n[]\n[8]\n[]\n[0.6141546368598938]\n[]\n\n\n134\nĠStack\n1\n[(Ġ{, 0.000276499631581828)]\n[(ĠStack, 0.9989708662033081)]\n[9]\n[31]\n[0.000276499631581828]\n[0.9989708662033081]\n\n\n135\n&lt;\n1\n[(ĠStack, 0.1649002879858017)]\n[(&lt;, 0.9991776347160339)]\n[10]\n[7]\n[0.1649002879858017]\n[0.9991776347160339]\n\n\n136\nText\n1\n[(&lt;, 0.000326614041114226)]\n[(Text, 0.9993822574615479)]\n[11]\n[8]\n[0.000326614041114226]\n[0.9993822574615479]\n\n\n137\nLe\n1\n[(Text, 8.198193972930312e-05)]\n[(Le, 0.999997615814209)]\n[12]\n[9]\n[8.198193972930312e-05]\n[0.999997615814209]\n\n\n138\naf\n1\n[(Le, 0.17625275254249573)]\n[(af, 0.999993085861206)]\n[13]\n[10]\n[0.17625275254249573]\n[0.999993085861206]\n\n\n139\n&gt;\n1\n[(af, 4.453602377907373e-05)]\n[(&gt;, 0.4462197422981262)]\n[14]\n[11]\n[4.453602377907373e-05]\n[0.4462197422981262]\n\n\n140\nĠparser\n1\n[(&gt;, 3.4169450373155996e-05)]\n[(Parser, 0.0355205237865448), (si, 0.22008731...\n[15]\n[2, 1, 24]\n[3.4169450373155996e-05]\n[0.0355205237865448, 0.22008731961250305, 0.53...\n\n\n141\nĠ=\n1\n[(Ġparser, 0.8924516439437866)]\n[]\n[16]\n[]\n[0.8924516439437866]\n[]\n\n\n142\nĠAn\n1\n[(Ġ=, 2.582160959718749e-06)]\n[(ĠAn, 0.9964381456375122)]\n[17]\n[21]\n[2.582160959718749e-06]\n[0.9964381456375122]\n\n\n143\nsi\n1\n[(ĠAn, 0.0015574246644973755)]\n[(si, 0.9998873472213745)]\n[18]\n[1]\n[0.0015574246644973755]\n[0.9998873472213745]\n\n\n144\nParser\n1\n[(si, 0.0003541821497492492)]\n[(Parser, 0.9984194040298462)]\n[19]\n[2]\n[0.0003541821497492492]\n[0.9984194040298462]\n\n\n145\n.\n1\n[(Parser, 0.08811366558074951), (ĠAn, 0.724938...\n[]\n[20, 18]\n[]\n[0.08811366558074951, 0.7249387502670288]\n[]\n\n\n146\nparse\n1\n[(., 0.014552467502653599)]\n[(parse, 0.9985792636871338)]\n[21]\n[25]\n[0.014552467502653599]\n[0.9985792636871338]\n\n\n147\nText\n1\n[(parse, 0.0021428707987070084)]\n[(Text, 0.9984368681907654)]\n[22]\n[33]\n[0.0021428707987070084]\n[0.9984368681907654]\n\n\n148\n(\"\n1\n[(Text, 0.5837852954864502)]\n[]\n[23]\n[]\n[0.5837852954864502]\n[]\n\n\n149\n\");\n1\n[((\", 0.006528949830681086), (Empty, 0.5122650...\n[]\n[24, 7]\n[]\n[0.006528949830681086, 0.5122650265693665]\n[]\n\n\n150\nĠassert\n1\n[(\");, 0.008671398274600506)]\n[(), 0.5105903148651123)]\n[25]\n[17]\n[0.008671398274600506]\n[0.5105903148651123]\n\n\n151\nThat\n1\n[(Ġassert, 0.09876050055027008)]\n[(&lt;/s&gt;, 0.8925181031227112)]\n[26]\n[44]\n[0.09876050055027008]\n[0.8925181031227112]\n\n\n152\n(\n1\n[(That, 0.9607610106468201)]\n[]\n[27]\n[]\n[0.9607610106468201]\n[]\n\n\n153\nparser\n1\n[((, 1.2671659533225466e-05)]\n[(Parser, 0.10263539850711823), (Ġparse, 0.359...\n[28]\n[23, 37, 42]\n[1.2671659533225466e-05]\n[0.10263539850711823, 0.3599758744239807, 0.74...\n\n\n154\n.\n1\n[(parser, 0.8031877875328064)]\n[]\n[29]\n[]\n[0.8031877875328064]\n[]\n\n\n155\nget\n1\n[(., 0.05058085173368454), (That, 0.6389041543...\n[]\n[30, 27]\n[]\n[0.05058085173368454, 0.6389041543006897]\n[]\n\n\n156\nText\n1\n[(get, 0.0001957392814802006)]\n[(Text, 0.9992320537567139)]\n[31]\n[33]\n[0.0001957392814802006]\n[0.9992320537567139]\n\n\n157\n()\n1\n[(Text, 0.004332142882049084)]\n[(Ġ{, 0.3945482075214386)]\n[32]\n[3]\n[0.004332142882049084]\n[0.3945482075214386]\n\n\n158\n).\n1\n[((), 0.2028176635503769), (That, 0.7752642631...\n[]\n[33, 27]\n[]\n[0.2028176635503769, 0.7752642631530762]\n[]\n\n\n159\nis\n1\n[()., 0.005320017691701651), (That, 0.94591450...\n[]\n[34, 27]\n[]\n[0.005320017691701651, 0.9459145069122314]\n[]\n\n\n160\nE\n1\n[(is, 0.09580342471599579), ()., 0.37910592555...\n[]\n[35, 34]\n[]\n[0.09580342471599579, 0.37910592555999756]\n[]\n\n\n161\nqual\n1\n[(E, 0.34926146268844604)]\n[]\n[36]\n[]\n[0.34926146268844604]\n[]\n\n\n162\nTo\n1\n[(qual, 0.9684934616088867)]\n[]\n[37]\n[]\n[0.9684934616088867]\n[]\n\n\n163\n(\"\n1\n[(To, 0.008859631605446339)]\n[(&gt;, 0.25527673959732056), (si, 0.507836997509...\n[38]\n[11, 22]\n[0.008859631605446339]\n[0.25527673959732056, 0.5078369975090027]\n\n\n164\n\");\n1\n[((\", 0.005594070069491863), (Empty, 0.3316252...\n[]\n[39, 7]\n[]\n[0.005594070069491863, 0.3316252827644348]\n[]\n\n\n165\nĠ}\n1\n[(\");, 0.0009953182889148593), (qual, 0.974664...\n[]\n[40, 37]\n[]\n[0.0009953182889148593, 0.9746649861335754]\n[]\n\n\n166\n&lt;/s&gt;\n1\n[(Ġ}, 0.49038276076316833)]\n[]\n[41]\n[]\n[0.49038276076316833]\n[]\n\n\n\n\n\n\n\n\ndef run_code_rational_all_set(exp, tensor_n = 1000, BATCH = 100): #When Tensor_n and batch differs then 'from_seq_id' is lost\n    gc.collect()\n    torch.cuda.empty_cache()\n    EXP = exp\n    test_arr_rationals = run_code_rational( \n            df_generated_input = df_generated_input,\n            tensor_size = tensor_n,\n            experiment = EXP,\n            batch_size = BATCH, \n            model = model, \n            verbose = False \n        )\n    #Saving process\n    print('Saving process')\n    test_arr_rationals.to_json( params['output_results'] + 'rationales_1_gen_02/' + 'rationales_[t_1000]_[max_src_100]_[max_tgt_150]_02_[exp:' + EXP +']_.json' )\n    return test_arr_rationals\n\n\n#tst\n#df_test_run = run_code_rational_all_set(exp='0')\n\n\nfor i in df_generated_input.columns[4:-1]: #Only Generated Sequences \n    df_test_run = run_code_rational_all_set(i)\n\n\ndf_test_run.head(1)\n\n\n\n\n\n\n\n\ngoal_token\nfrom_seq_id\ntypesets_tgt\ntypesets_src\nrationale_pos_tgt\nrationale_pos_src\nrationale_prob_tgt\nrationale_prob_src\n\n\n\n\n0\nTest\n0\n[(@, 0.9980469942092896)]\n[]\n[0]\n[]\n[0.9980469942092896]\n[]\n\n\n\n\n\n\n\n\n#Running all Experiments\ndef exp_run_all_rationales():\n    dict_arr_rations = { key : run_code_rational(\n        df_generated_input = df_generated_input,\n        experiment = key,\n        batch_size = 10, \n        model = model, \n        verbose = False \n    ) for key in df_generated_input.columns[4:-1] }\n    return dict_arr_rations\n\n\n#arr_df_rationale = [pandas_rationales(dict_arr_rations[key]) for key in dict_arr_rations.keys()]"
  },
  {
    "objectID": "bart_sampling_.html",
    "href": "bart_sampling_.html",
    "title": "Large Scale Encoder-Decoder (BART) Sampling",
    "section": "",
    "text": "from pathlib import Path\nimport csv\nimport seaborn as sns; sns.set_theme()\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nimport functools\n\npd.options.display.float_format = '{:.2f}'.format\nfrom tokenizers import ByteLevelBPETokenizer\nimport torch\nimport importlib\nfrom fairseq.models.transformer import TransformerModel\nrationalization = importlib.import_module(\"sequential-rationales.fairseq.rationalization\")\nrationalize = rationalization.rationalize_lm\nimport warnings\nfrom matplotlib import colors\nimport os\nfrom rationalization import rationalize_lm, rationalize_conditional_model\n\nModuleNotFoundError: No module named 'rationalization'\ndef param_default():\n    corpus = 'fm_fc_ms_ff' #&lt;-- Scope\n    data_path = Path('../semeru-datasets/semeru/methods2test-v2.0/corpus/preprocessed' + corpus + '/')\n    data_path_raw = Path('../athena-datasets/' + corpus + '/raw/')\n    tokenizer_path = Path('../tokenizer/')\n    return {\n        'bpe_path' : tokenizer_path / 'universal_tokenizer/universal_tokenizer/roberta_aug_spaces',\n        'eval_raw': [data_path_raw / 'eval/input.methods.txt',\n                        data_path_raw / 'eval/output.tests.txt'],\n        'test_raw': [data_path_raw / 'test/input.methods.txt', \n                        data_path_raw / 'test/output.tests.txt'],\n        'train_raw': [data_path_raw / 'train/input.methods.txt', \n                        data_path_raw / 'train/output.tests.txt'],\n        'data_labels' : ['test_raw'],#['eval_raw','test_raw','train_raw'], &lt;----- Just Test\n        'output_pandas' : data_path / 'pandas/',\n        'out_processed' : '/datasets/out_processed/',\n        'model_name_or_path' : 'models/checkpoint_dir_01/models/', #Model Path\n        'checkpoint_file': 'checkpoint_best.pt', #Model\n        'data_preprocessed':'/home/davidna/data/dummy/sequential-rationales/fairseq/fairseq/data-bin/bins/',\n        'output_results' : 'results/' \n    }\nparams = param_default()\nparams['checkpoint_file']\n\n'checkpoint_best.pt'\nparams['eval_raw']\n\n[PosixPath('../athena-datasets/fm_fc_ms_ff/raw/eval/input.methods.txt'),\n PosixPath('../athena-datasets/fm_fc_ms_ff/raw/eval/output.tests.txt')]\n#Setting experiments \n#! export CUDA_VISIBLE_DEVICES=\"1\""
  },
  {
    "objectID": "bart_sampling_.html#universal-tokenizer",
    "href": "bart_sampling_.html#universal-tokenizer",
    "title": "Large Scale Encoder-Decoder (BART) Sampling",
    "section": "Universal Tokenizer",
    "text": "Universal Tokenizer\n\ndef load_tokenizer(bpe_path):\n    return ByteLevelBPETokenizer(str(bpe_path)+'-vocab.json',str(bpe_path)+'-merges.txt')\n\n\ndef lazy_decode(bpe_java):\n    return bpe_java.replace(' ','').replace('Ġ',' ').replace('Ċ','\\n')\n\n\ndef prettify_java(minified_java):\n    \"tries to undo Michele's minification. Works decently, although for loops and sets get newlines inserted, and there are no empty lines or comments\"\n    minified_java = minified_java.replace('{','{\\n').replace('}','}\\n').replace(';',';\\n')\n    num_indents = 0\n    pretty_java = ''\n    for line in minified_java.splitlines():\n        if line.lstrip().startswith('}'):\n            num_indents -= 1\n        pretty_java += num_indents*'    '+line+'\\n'\n        if line.endswith('{'):\n            num_indents += 1\n        if line.endswith('}') and not line.lstrip().startswith('}'):\n            num_indents -= 1\n    return pretty_java\n\n\ntokenizer = load_tokenizer(params['bpe_path'])"
  },
  {
    "objectID": "bart_sampling_.html#data-loading-and-testing",
    "href": "bart_sampling_.html#data-loading-and-testing",
    "title": "Large Scale Encoder-Decoder (BART) Sampling",
    "section": "Data Loading and Testing",
    "text": "Data Loading and Testing\n\n#export\ndef method_size_vector( method_vector ):\n    '''Return the size of the tokens for a give method based on id\n        Assuming that method_vector is an array of tokens\n    '''\n    input_ids = [ len(mtd) for mtd in method_vector ]\n    return input_ids\n\n\ndef super_set_code():\n    data = {}\n    for label in params['data_labels']:\n        for val,path_data in enumerate(params[ label ]):\n            df = pd.read_csv( path_data, sep=\"\\n\", header=None, names=[label+str(val)]) #reading file\n            df[label+'_bpe'+str(val)] = [ enc.tokens for enc in tokenizer.encode_batch( df[label+str(val)].values ) ] #bpe\n            df['method_size'+str(val)] = method_size_vector( df[label+'_bpe'+str(val)].values ) #counting tokens\n            data[label+str(val)] =  df  \n        #data[-1].columns = [ label ]\n    return data\n\n\n# Loading Json Sets\ndef load_checkpoint_1():\n    super_df = {}\n    for label in params['data_labels']:\n        for val, _ in enumerate(params[ label ]):\n            super_df[ label+str(val) ] = pd.read_json( params['output_pandas'] / (label+str(val) +'.json')  )\n            print(\"read:\",label+str(val))\n    return super_df\n\n\nsuper_data = load_checkpoint_1()\n\nread: test_raw0\nread: test_raw1\n\n\n\nsuper_data['test_raw0'].head(1) #Source\n\n\n\n\n\n\n\n\ntest_raw0\ntest_raw_bpe0\nmethod_size0\n\n\n\n\n0\nDateUtils { public static Date yearStart() { f...\n[Date, Ut, ils, Ġ{, Ġpublic, Ġstatic, ĠDate, Ġ...\n227\n\n\n\n\n\n\n\n\n#Size Statistics of Source Set\nsuper_data['test_raw0'].method_size0.describe()\n\ncount   78388.00\nmean      423.42\nstd       653.26\nmin         8.00\n25%       143.00\n50%       248.00\n75%       446.00\nmax     31638.00\nName: method_size0, dtype: float64\n\n\n\nSET_METHOD_SIZE = 100 #&lt;---- HARDCODED\nsuper_data['test_raw0'][super_data['test_raw0'].method_size0 &lt;= SET_METHOD_SIZE ].method_size0.describe()\n\ncount   9445.00\nmean      73.55\nstd       18.59\nmin        8.00\n25%       60.00\n50%       76.00\n75%       89.00\nmax      100.00\nName: method_size0, dtype: float64\n\n\n\n#Target Set\nsuper_data['test_raw1'].head(1) #Target\n\n\n\n\n\n\n\n\ntest_raw1\ntest_raw_bpe1\nmethod_size1\n\n\n\n\n0\n@Test public void yearStart() { Date date = Da...\n[@, Test, Ġpublic, Ġvoid, Ġyear, Start, (), Ġ{...\n61"
  },
  {
    "objectID": "bart_sampling_.html#model-loading-and-testing",
    "href": "bart_sampling_.html#model-loading-and-testing",
    "title": "Large Scale Encoder-Decoder (BART) Sampling",
    "section": "Model Loading and Testing",
    "text": "Model Loading and Testing\n\n#Loading a pretrain model\nmodel = TransformerModel.from_pretrained(\n  model_name_or_path = params['model_name_or_path'],\n  checkpoint_file = params['checkpoint_file'],\n  #data_name_or_path = params['data_preprocessed']\n)\n\n\n## Move model to GPU if available and trigger evaluation mode\nif torch.cuda.is_available():\n  model.cuda()\nmodel.eval()\n\nGeneratorHubInterface(\n  (models): ModuleList(\n    (0): BARTModel(\n      (encoder): TransformerEncoderBase(\n        (dropout_module): FairseqDropout()\n        (embed_tokens): Embedding(50348, 512, padding_idx=1)\n        (embed_positions): SinusoidalPositionalEmbedding()\n        (layers): ModuleList(\n          (0): TransformerEncoderLayerBase(\n            (self_attn): MultiheadAttention(\n              (dropout_module): FairseqDropout()\n              (k_proj): Linear(in_features=512, out_features=512, bias=True)\n              (v_proj): Linear(in_features=512, out_features=512, bias=True)\n              (q_proj): Linear(in_features=512, out_features=512, bias=True)\n              (out_proj): Linear(in_features=512, out_features=512, bias=True)\n            )\n            (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n            (dropout_module): FairseqDropout()\n            (activation_dropout_module): FairseqDropout()\n            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n            (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          )\n          (1): TransformerEncoderLayerBase(\n            (self_attn): MultiheadAttention(\n              (dropout_module): FairseqDropout()\n              (k_proj): Linear(in_features=512, out_features=512, bias=True)\n              (v_proj): Linear(in_features=512, out_features=512, bias=True)\n              (q_proj): Linear(in_features=512, out_features=512, bias=True)\n              (out_proj): Linear(in_features=512, out_features=512, bias=True)\n            )\n            (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n            (dropout_module): FairseqDropout()\n            (activation_dropout_module): FairseqDropout()\n            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n            (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          )\n          (2): TransformerEncoderLayerBase(\n            (self_attn): MultiheadAttention(\n              (dropout_module): FairseqDropout()\n              (k_proj): Linear(in_features=512, out_features=512, bias=True)\n              (v_proj): Linear(in_features=512, out_features=512, bias=True)\n              (q_proj): Linear(in_features=512, out_features=512, bias=True)\n              (out_proj): Linear(in_features=512, out_features=512, bias=True)\n            )\n            (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n            (dropout_module): FairseqDropout()\n            (activation_dropout_module): FairseqDropout()\n            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n            (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          )\n          (3): TransformerEncoderLayerBase(\n            (self_attn): MultiheadAttention(\n              (dropout_module): FairseqDropout()\n              (k_proj): Linear(in_features=512, out_features=512, bias=True)\n              (v_proj): Linear(in_features=512, out_features=512, bias=True)\n              (q_proj): Linear(in_features=512, out_features=512, bias=True)\n              (out_proj): Linear(in_features=512, out_features=512, bias=True)\n            )\n            (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n            (dropout_module): FairseqDropout()\n            (activation_dropout_module): FairseqDropout()\n            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n            (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          )\n          (4): TransformerEncoderLayerBase(\n            (self_attn): MultiheadAttention(\n              (dropout_module): FairseqDropout()\n              (k_proj): Linear(in_features=512, out_features=512, bias=True)\n              (v_proj): Linear(in_features=512, out_features=512, bias=True)\n              (q_proj): Linear(in_features=512, out_features=512, bias=True)\n              (out_proj): Linear(in_features=512, out_features=512, bias=True)\n            )\n            (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n            (dropout_module): FairseqDropout()\n            (activation_dropout_module): FairseqDropout()\n            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n            (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          )\n          (5): TransformerEncoderLayerBase(\n            (self_attn): MultiheadAttention(\n              (dropout_module): FairseqDropout()\n              (k_proj): Linear(in_features=512, out_features=512, bias=True)\n              (v_proj): Linear(in_features=512, out_features=512, bias=True)\n              (q_proj): Linear(in_features=512, out_features=512, bias=True)\n              (out_proj): Linear(in_features=512, out_features=512, bias=True)\n            )\n            (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n            (dropout_module): FairseqDropout()\n            (activation_dropout_module): FairseqDropout()\n            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n            (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          )\n        )\n      )\n      (decoder): TransformerDecoderBase(\n        (dropout_module): FairseqDropout()\n        (embed_tokens): Embedding(50348, 512, padding_idx=1)\n        (embed_positions): SinusoidalPositionalEmbedding()\n        (layers): ModuleList(\n          (0): TransformerDecoderLayerBase(\n            (dropout_module): FairseqDropout()\n            (self_attn): MultiheadAttention(\n              (dropout_module): FairseqDropout()\n              (k_proj): Linear(in_features=512, out_features=512, bias=True)\n              (v_proj): Linear(in_features=512, out_features=512, bias=True)\n              (q_proj): Linear(in_features=512, out_features=512, bias=True)\n              (out_proj): Linear(in_features=512, out_features=512, bias=True)\n            )\n            (activation_dropout_module): FairseqDropout()\n            (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n            (encoder_attn): MultiheadAttention(\n              (dropout_module): FairseqDropout()\n              (k_proj): Linear(in_features=512, out_features=512, bias=True)\n              (v_proj): Linear(in_features=512, out_features=512, bias=True)\n              (q_proj): Linear(in_features=512, out_features=512, bias=True)\n              (out_proj): Linear(in_features=512, out_features=512, bias=True)\n            )\n            (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n            (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          )\n          (1): TransformerDecoderLayerBase(\n            (dropout_module): FairseqDropout()\n            (self_attn): MultiheadAttention(\n              (dropout_module): FairseqDropout()\n              (k_proj): Linear(in_features=512, out_features=512, bias=True)\n              (v_proj): Linear(in_features=512, out_features=512, bias=True)\n              (q_proj): Linear(in_features=512, out_features=512, bias=True)\n              (out_proj): Linear(in_features=512, out_features=512, bias=True)\n            )\n            (activation_dropout_module): FairseqDropout()\n            (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n            (encoder_attn): MultiheadAttention(\n              (dropout_module): FairseqDropout()\n              (k_proj): Linear(in_features=512, out_features=512, bias=True)\n              (v_proj): Linear(in_features=512, out_features=512, bias=True)\n              (q_proj): Linear(in_features=512, out_features=512, bias=True)\n              (out_proj): Linear(in_features=512, out_features=512, bias=True)\n            )\n            (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n            (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          )\n          (2): TransformerDecoderLayerBase(\n            (dropout_module): FairseqDropout()\n            (self_attn): MultiheadAttention(\n              (dropout_module): FairseqDropout()\n              (k_proj): Linear(in_features=512, out_features=512, bias=True)\n              (v_proj): Linear(in_features=512, out_features=512, bias=True)\n              (q_proj): Linear(in_features=512, out_features=512, bias=True)\n              (out_proj): Linear(in_features=512, out_features=512, bias=True)\n            )\n            (activation_dropout_module): FairseqDropout()\n            (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n            (encoder_attn): MultiheadAttention(\n              (dropout_module): FairseqDropout()\n              (k_proj): Linear(in_features=512, out_features=512, bias=True)\n              (v_proj): Linear(in_features=512, out_features=512, bias=True)\n              (q_proj): Linear(in_features=512, out_features=512, bias=True)\n              (out_proj): Linear(in_features=512, out_features=512, bias=True)\n            )\n            (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n            (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          )\n          (3): TransformerDecoderLayerBase(\n            (dropout_module): FairseqDropout()\n            (self_attn): MultiheadAttention(\n              (dropout_module): FairseqDropout()\n              (k_proj): Linear(in_features=512, out_features=512, bias=True)\n              (v_proj): Linear(in_features=512, out_features=512, bias=True)\n              (q_proj): Linear(in_features=512, out_features=512, bias=True)\n              (out_proj): Linear(in_features=512, out_features=512, bias=True)\n            )\n            (activation_dropout_module): FairseqDropout()\n            (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n            (encoder_attn): MultiheadAttention(\n              (dropout_module): FairseqDropout()\n              (k_proj): Linear(in_features=512, out_features=512, bias=True)\n              (v_proj): Linear(in_features=512, out_features=512, bias=True)\n              (q_proj): Linear(in_features=512, out_features=512, bias=True)\n              (out_proj): Linear(in_features=512, out_features=512, bias=True)\n            )\n            (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n            (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          )\n          (4): TransformerDecoderLayerBase(\n            (dropout_module): FairseqDropout()\n            (self_attn): MultiheadAttention(\n              (dropout_module): FairseqDropout()\n              (k_proj): Linear(in_features=512, out_features=512, bias=True)\n              (v_proj): Linear(in_features=512, out_features=512, bias=True)\n              (q_proj): Linear(in_features=512, out_features=512, bias=True)\n              (out_proj): Linear(in_features=512, out_features=512, bias=True)\n            )\n            (activation_dropout_module): FairseqDropout()\n            (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n            (encoder_attn): MultiheadAttention(\n              (dropout_module): FairseqDropout()\n              (k_proj): Linear(in_features=512, out_features=512, bias=True)\n              (v_proj): Linear(in_features=512, out_features=512, bias=True)\n              (q_proj): Linear(in_features=512, out_features=512, bias=True)\n              (out_proj): Linear(in_features=512, out_features=512, bias=True)\n            )\n            (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n            (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          )\n          (5): TransformerDecoderLayerBase(\n            (dropout_module): FairseqDropout()\n            (self_attn): MultiheadAttention(\n              (dropout_module): FairseqDropout()\n              (k_proj): Linear(in_features=512, out_features=512, bias=True)\n              (v_proj): Linear(in_features=512, out_features=512, bias=True)\n              (q_proj): Linear(in_features=512, out_features=512, bias=True)\n              (out_proj): Linear(in_features=512, out_features=512, bias=True)\n            )\n            (activation_dropout_module): FairseqDropout()\n            (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n            (encoder_attn): MultiheadAttention(\n              (dropout_module): FairseqDropout()\n              (k_proj): Linear(in_features=512, out_features=512, bias=True)\n              (v_proj): Linear(in_features=512, out_features=512, bias=True)\n              (q_proj): Linear(in_features=512, out_features=512, bias=True)\n              (out_proj): Linear(in_features=512, out_features=512, bias=True)\n            )\n            (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n            (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          )\n        )\n        (output_projection): Linear(in_features=512, out_features=50348, bias=False)\n      )\n      (classification_heads): ModuleDict()\n    )\n  )\n)\n\n\n\nmodel.model = model.models[0]\n\n\nmodel.device\n\ndevice(type='cuda', index=0)\n\n\n\ndef joining_encode_tokens( arr_tokens, model ):\n    if len(arr_tokens) &gt; SET_METHOD_SIZE:\n        arr_tokens = arr_tokens[0:SET_METHOD_SIZE]\n    focal_code = \" \".join(arr_tokens)\n    return model.encode( focal_code )\n\n\n#Sampling without replacement\n#Testing size: 78388\n#Sampling size with 95% of confidence and 3% Error = 1053 ~ 1000\ndef code_sampling(df_super_data ,  FLAG_SAMPLING = True, SIZE_SAMPLING = 1000, random_state = 3): #&lt;---- HARDCODED\n    \n    df_sampled_code = df_super_data['test_raw0'][df_super_data['test_raw0'].method_size0 &lt;= SET_METHOD_SIZE ].sample(\n            n = SIZE_SAMPLING,\n            replace = False,\n            random_state = random_state # For reproducibility\n    )\n\n    if FLAG_SAMPLING:\n        df_sampled_code['input_tokens'] = [ joining_encode_tokens(arr_sample, model=model) for arr_sample in df_sampled_code.test_raw_bpe0.values ]\n        #df_sampled_code['origin_pos'] = df_sampled_code.index\n    else:\n        df_sampled_code['input_tokens_pos'] = [ joining_encode_tokens(arr_sample, model=model) for arr_sample in df_super_data['test_raw0'].test_raw_bpe0.values]\n        #df_sampled_code['origin'] = df_sampled_code.index\n    return df_sampled_code\n\n\ndf_sampled_code = code_sampling(\n    df_super_data = super_data,\n    SIZE_SAMPLING = 1000\n)\n\n\ndf_sampled_code.head()\n\n\n\n\n\n\n\n\ntest_raw0\ntest_raw_bpe0\nmethod_size0\ninput_tokens\n\n\n\n\n5394\nDropImpl extends BaseSqlPart implements Drop {...\n[Drop, Impl, Ġextends, ĠBase, S, ql, Part, Ġim...\n162\n[tensor(43542), tensor(48455), tensor(14269), ...\n\n\n71045\nRuleDatabaseItemUpdateRunnable implements Runn...\n[Rule, Database, Item, Update, Run, n, able, Ġ...\n183\n[tensor(47181), tensor(49187), tensor(47599), ...\n\n\n30570\nInChIToStructure { public IAtomContainer getAt...\n[In, Ch, IT, o, St, ructure, Ġ{, Ġpublic, ĠI, ...\n123\n[tensor(1121), tensor(4771), tensor(2068), ten...\n\n\n28852\nSgfParser { public List&lt;Short&gt; parseGameFromFi...\n[S, g, f, Parser, Ġ{, Ġpublic, ĠList, &lt;, Short...\n150\n[tensor(104), tensor(571), tensor(506), tensor...\n\n\n54142\nPredictionContainerGenerator extends AbstractA...\n[Pred, iction, Container, Gener, ator, Ġextend...\n162\n[tensor(45408), tensor(26579), tensor(48557), ...\n\n\n\n\n\n\n\n\n#df_sampled_code.reset_index()\n\n\n#super_data['test_raw0'].filter( items = df_sampled_code.origin_pos.values, axis=0 ) #&lt;-------- Retrieving original Data\n\n\nlen(df_sampled_code.input_tokens.values)\n\n5\n\n\n\ndf_sampled_code.input_tokens.values[0]\n\ntensor([43542, 48455, 14269, 11056,   104, 44306,  4741, 36987, 21603, 25522,\n          787,  7199, 49302,   787, 49116,   285, 21603, 41836,  2103,  1640,\n         1039,  7199, 49302,   507, 26602,  2103, 31723,    43, 25522,   671,\n         2103,  1640, 15755,     6,  2103, 31723,  4397, 35524,   787,  7199,\n        49302,   787, 49116, 21603, 41836,  2103,  1640,  1039,  7199, 49302,\n          507, 26602,  2103, 31723,  4397,   787,  7199, 49302,   787, 49116,\n        21603, 41836,  2103,  1640,  1039, 49302,   868,   507, 26602,  8503,\n        31723,     6,   787,  7199, 49302,   507, 26602,  2103, 31723,  4397,\n          787,  7199, 49302,   787, 49116, 21603, 41836,  1106,  9089,  1952,\n         2103,  1106,  9089,  1952,  1640,  1039,  7199, 49302,   507, 26602,\n         2103, 31723,  4397,   787,  7199, 49302,   787, 49116, 21603, 41836,\n         1106,  9089,  1952,  2103,  1106,  9089,  1952,  1640,  1039, 49302,\n          868,   507, 26602,  8503, 31723,     6,   787,  7199, 49302,   507,\n        26602,  2103, 31723,  4397,   787, 49116, 13842, 12760,  1397,  3972,\n         1640,  1039,  7199, 49302,   507,  8214,   104, 44306, 45200, 20528,\n         4397,   787, 49302,   868,   787, 49116,   208, 44306,  4741,   986,\n        47006, 35524,     2])\n\n\n\nSAMPLES = 30 #&lt;---- Hardocoded\nMAX_GEN_TOK = 200\n\n\ndef df_sample_generation(\n    df_sampled_code, \n    model, \n    n=1, \n    max_gen_tok = 100\n    ):\n    generated_input = lambda input,model,n,max_gen_tok: model.generate( \n        input,\n        beam = n, \n        maxlen = max_gen_tok, ##WARNING, This parameter is not working\n        #max_length = n, \n        do_sample = False, \n        pad_token_id = 50256 ) ## HARDCODED\n    arr_generated_code = np.array([ generated_input(input, model=model, n=n, \n                                max_gen_tok=max_gen_tok ) for input in df_sampled_code.input_tokens.values ]).T\n    \n    #dict_generated_code = { i: [j['tokens'].cpu().data.numpy()[:max_gen_tok] for j in samples] for i,samples in enumerate(arr_generated_code) } #Max Token Generation\n    dict_generated_code = { i: [j['tokens'].cpu().data.numpy() for j in samples] for i,samples in enumerate(arr_generated_code) }\n    dict_generated_code['source_sampling'] = [ i.cpu().data.numpy() for i in df_sampled_code.input_tokens.values] \n    #return arr_generated_code\n    df_temp = pd.DataFrame().from_dict( data=dict_generated_code ) # DataFrame from Generation\n    df_temp = pd.concat([df_sampled_code.reset_index(), df_temp ], axis=1) #Index before concating\n    #return pd.DataFrame().from_dict( data=dict_generated_code )\n    return df_temp\n\n\n#TODO limit the number of tokens generated\n#WARNING TIME CONSUMING\ndf_generated_input = df_sample_generation( \n    df_sampled_code = df_sampled_code, \n    model = model, \n    n = SAMPLES, \n    max_gen_tok = MAX_GEN_TOK \n)\n# [ sample_generation(input, model=model) for input in input_tokens[:2] ]\n\n\ndf_generated_input\n\n\n\n\n\n\n\n\nindex\ntest_raw0\ntest_raw_bpe0\nmethod_size0\ninput_tokens\n0\n1\n2\n3\n4\n...\n21\n22\n23\n24\n25\n26\n27\n28\n29\nsource_sampling\n\n\n\n\n0\n5394\nDropImpl extends BaseSqlPart implements Drop {...\n[Drop, Impl, Ġextends, ĠBase, S, ql, Part, Ġim...\n162\n[tensor(43542), tensor(48455), tensor(14269), ...\n[1039, 34603, 1640, 10162, 5457, 36993, 45621,...\n[1039, 34603, 1640, 10162, 5457, 36993, 45621,...\n[1039, 34603, 1640, 10162, 5457, 36993, 45621,...\n[1039, 34603, 1640, 10162, 5457, 44840, 26170,...\n[1039, 34603, 1640, 10162, 5457, 36993, 45621,...\n...\n[1039, 34603, 1640, 10162, 5457, 36993, 45621,...\n[1039, 34603, 1640, 10162, 5457, 36993, 45621,...\n[1039, 34603, 1640, 10162, 5457, 36993, 45621,...\n[1039, 34603, 1640, 10162, 5457, 36993, 45621,...\n[1039, 34603, 1640, 10162, 5457, 36993, 45621,...\n[1039, 34603, 1640, 10162, 5457, 36993, 45621,...\n[1039, 34603, 1640, 10162, 5457, 36993, 45621,...\n[1039, 34603, 1640, 10162, 5457, 36993, 45621,...\n[1039, 34603, 1640, 10162, 5457, 36993, 45621,...\n[43542, 48455, 14269, 11056, 104, 44306, 4741,...\n\n\n1\n71045\nRuleDatabaseItemUpdateRunnable implements Runn...\n[Rule, Database, Item, Update, Run, n, able, Ġ...\n183\n[tensor(47181), tensor(49187), tensor(47599), ...\n[1039, 34603, 285, 13842, 197, 44514, 43048, 6...\n[1039, 34603, 285, 13842, 197, 44514, 43048, 6...\n[1039, 34603, 285, 13842, 197, 44514, 43048, 6...\n[1039, 34603, 285, 13842, 197, 44514, 43048, 6...\n[1039, 34603, 285, 13842, 197, 44514, 43048, 6...\n...\n[1039, 34603, 285, 13842, 197, 44514, 43048, 6...\n[1039, 34603, 285, 13842, 197, 44514, 43048, 6...\n[1039, 34603, 285, 13842, 197, 44514, 43048, 6...\n[1039, 34603, 285, 13842, 197, 44514, 43048, 6...\n[1039, 34603, 285, 13842, 197, 44514, 43048, 6...\n[1039, 34603, 285, 13842, 197, 44514, 43048, 6...\n[1039, 34603, 285, 13842, 197, 44514, 43048, 6...\n[1039, 34603, 285, 13842, 197, 44514, 43048, 6...\n[1039, 34603, 285, 13842, 197, 44514, 43048, 6...\n[47181, 49187, 47599, 39962, 33177, 282, 868, ...\n\n\n2\n30570\nInChIToStructure { public IAtomContainer getAt...\n[In, Ch, IT, o, St, ructure, Ġ{, Ġpublic, ĠI, ...\n123\n[tensor(1121), tensor(4771), tensor(2068), ten...\n[1039, 34603, 285, 13842, 1296, 14181, 3750, 1...\n[1039, 34603, 285, 13842, 1296, 14181, 3750, 1...\n[1039, 34603, 285, 13842, 1296, 14181, 3750, 1...\n[1039, 34603, 285, 13842, 1296, 14181, 3750, 1...\n[1039, 34603, 285, 13842, 1296, 14181, 3750, 1...\n...\n[1039, 34603, 285, 13842, 1296, 14181, 3750, 1...\n[1039, 34603, 285, 13842, 1296, 14181, 3750, 1...\n[1039, 34603, 285, 13842, 1296, 14181, 3750, 1...\n[1039, 34603, 285, 13842, 1296, 14181, 3750, 1...\n[1039, 34603, 285, 13842, 1296, 14181, 3750, 1...\n[1039, 34603, 285, 13842, 1296, 14181, 3750, 1...\n[1039, 34603, 285, 13842, 1296, 14181, 3750, 1...\n[1039, 34603, 285, 13842, 1296, 14181, 3750, 1...\n[1039, 34603, 285, 13842, 1296, 14181, 3750, 1...\n[1121, 4771, 2068, 139, 5320, 20636, 25522, 28...\n\n\n3\n28852\nSgfParser { public List&lt;Short&gt; parseGameFromFi...\n[S, g, f, Parser, Ġ{, Ġpublic, ĠList, &lt;, Short...\n150\n[tensor(104), tensor(571), tensor(506), tensor...\n[1039, 34603, 285, 13842, 1296, 22011, 1090, 2...\n[1039, 34603, 285, 13842, 197, 22011, 1090, 20...\n[1039, 34603, 285, 13842, 1296, 22011, 1090, 2...\n[1039, 34603, 285, 13842, 1296, 22011, 1090, 2...\n[1039, 34603, 285, 13842, 1296, 22011, 1090, 2...\n...\n[1039, 34603, 285, 13842, 1296, 22011, 1090, 2...\n[1039, 34603, 285, 13842, 1296, 22011, 1090, 2...\n[1039, 34603, 285, 13842, 1296, 22011, 1090, 2...\n[1039, 34603, 285, 13842, 1296, 22011, 1090, 2...\n[1039, 34603, 285, 13842, 1296, 22011, 1090, 2...\n[1039, 34603, 285, 13842, 1296, 22011, 1090, 2...\n[1039, 34603, 285, 13842, 1296, 22011, 1090, 2...\n[1039, 34603, 285, 13842, 197, 22011, 1090, 20...\n[1039, 34603, 285, 13842, 1296, 22011, 1090, 2...\n[104, 571, 506, 49707, 25522, 285, 9527, 41552...\n\n\n4\n54142\nPredictionContainerGenerator extends AbstractA...\n[Pred, iction, Container, Gener, ator, Ġextend...\n162\n[tensor(45408), tensor(26579), tensor(48557), ...\n[1039, 34603, 285, 13842, 1296, 21527, 45788, ...\n[1039, 34603, 285, 13842, 1296, 21527, 45788, ...\n[1039, 34603, 285, 13842, 1296, 21527, 45788, ...\n[1039, 34603, 285, 13842, 1296, 21527, 45788, ...\n[1039, 34603, 285, 13842, 1296, 21527, 45788, ...\n...\n[1039, 34603, 285, 13842, 1296, 21527, 45788, ...\n[1039, 34603, 285, 13842, 1296, 21527, 45788, ...\n[1039, 34603, 285, 13842, 1296, 21527, 45788, ...\n[1039, 34603, 285, 13842, 1296, 21527, 45788, ...\n[1039, 34603, 285, 13842, 1296, 21527, 45788, ...\n[1039, 34603, 285, 13842, 1296, 21527, 45788, ...\n[1039, 34603, 285, 13842, 1296, 21527, 45788, ...\n[1039, 34603, 285, 13842, 1296, 21527, 45788, ...\n[1039, 34603, 285, 13842, 1296, 21527, 45788, ...\n[45408, 26579, 48557, 40025, 2630, 14269, 4364...\n\n\n\n\n5 rows × 36 columns\n\n\n\n\nStatistics and Checkpoint\n\nnp_len_method = [ (np.array([ len(gen_method) for gen_method in df_generated_input[j] ]).mean(),\n                   np.array([ len(gen_method) for gen_method in df_generated_input[j] ]).std()  )\n                    for j in range(30) ]\n\n\nnp_len_method\n\n[(112.2, 62.92980216082043),\n (112.0, 62.504399845130905),\n (108.0, 61.13264267148934),\n (107.4, 59.87520354871455),\n (107.6, 60.45361858482915),\n (104.8, 59.96465625683182),\n (102.8, 58.714223149080325),\n (102.2, 58.87410296556543),\n (102.4, 58.72852799108794),\n (100.0, 58.32323722153975),\n (103.0, 60.02666074337302),\n (100.0, 58.22370651203855),\n (101.0, 59.32284551502903),\n (99.4, 58.67742325630872),\n (97.2, 58.34175177349408),\n (95.2, 57.241243871879654),\n (94.4, 58.3869848510779),\n (92.6, 58.711498022108074),\n (90.8, 58.89448191469214),\n (92.8, 57.80449809487147),\n (92.8, 57.880566686928695),\n (90.0, 59.03558249056242),\n (91.8, 58.47871407614912),\n (85.0, 61.59545437773797),\n (81.4, 60.88875101363141),\n (79.6, 62.31725282776833),\n (78.8, 62.268451080784075),\n (82.4, 60.65509047062744),\n (82.0, 61.01803012225157),\n (74.0, 65.78145635359557)]\n\n\n\n#Checkpoint of Generation\ndef checkpoint_generation( df , name = '1_generation_[max:100]_02.json' ):\n    df.drop('input_tokens', axis=1).to_json( params['output_results'] + name )\n    pass\n\n\ncheckpoint_generation( df = df_generated_input )\n\n\ndf_generated_input = pd.read_json( params['output_results'] + '1_generation_[max:100]_02.json' )\n\n\ndf_generated_input.head()\n\n\n\n\n\n\n\n\nindex\ntest_raw0\ntest_raw_bpe0\nmethod_size0\n0\n1\n2\n3\n4\n5\n...\n21\n22\n23\n24\n25\n26\n27\n28\n29\nsource_sampling\n\n\n\n\n0\n5394\nDropImpl extends BaseSqlPart implements Drop {...\n[Drop, Impl, Ġextends, ĠBase, S, ql, Part, Ġim...\n162\n[1039, 34603, 1640, 10162, 5457, 36993, 45621,...\n[1039, 34603, 1640, 10162, 5457, 36993, 45621,...\n[1039, 34603, 1640, 10162, 5457, 36993, 45621,...\n[1039, 34603, 1640, 10162, 5457, 44840, 26170,...\n[1039, 34603, 1640, 10162, 5457, 36993, 45621,...\n[1039, 34603, 1640, 10162, 5457, 44840, 26170,...\n...\n[1039, 34603, 1640, 10162, 5457, 36993, 45621,...\n[1039, 34603, 1640, 10162, 5457, 36993, 45621,...\n[1039, 34603, 1640, 10162, 5457, 36993, 45621,...\n[1039, 34603, 1640, 10162, 5457, 36993, 45621,...\n[1039, 34603, 1640, 10162, 5457, 36993, 45621,...\n[1039, 34603, 1640, 10162, 5457, 36993, 45621,...\n[1039, 34603, 1640, 10162, 5457, 36993, 45621,...\n[1039, 34603, 1640, 10162, 5457, 36993, 45621,...\n[1039, 34603, 1640, 10162, 5457, 36993, 45621,...\n[43542, 48455, 14269, 11056, 104, 44306, 4741,...\n\n\n1\n71045\nRuleDatabaseItemUpdateRunnable implements Runn...\n[Rule, Database, Item, Update, Run, n, able, Ġ...\n183\n[1039, 34603, 285, 13842, 197, 44514, 43048, 6...\n[1039, 34603, 285, 13842, 197, 44514, 43048, 6...\n[1039, 34603, 285, 13842, 197, 44514, 43048, 6...\n[1039, 34603, 285, 13842, 197, 44514, 43048, 6...\n[1039, 34603, 285, 13842, 197, 44514, 43048, 6...\n[1039, 34603, 285, 13842, 197, 44514, 43048, 6...\n...\n[1039, 34603, 285, 13842, 197, 44514, 43048, 6...\n[1039, 34603, 285, 13842, 197, 44514, 43048, 6...\n[1039, 34603, 285, 13842, 197, 44514, 43048, 6...\n[1039, 34603, 285, 13842, 197, 44514, 43048, 6...\n[1039, 34603, 285, 13842, 197, 44514, 43048, 6...\n[1039, 34603, 285, 13842, 197, 44514, 43048, 6...\n[1039, 34603, 285, 13842, 197, 44514, 43048, 6...\n[1039, 34603, 285, 13842, 197, 44514, 43048, 6...\n[1039, 34603, 285, 13842, 197, 44514, 43048, 6...\n[47181, 49187, 47599, 39962, 33177, 282, 868, ...\n\n\n2\n30570\nInChIToStructure { public IAtomContainer getAt...\n[In, Ch, IT, o, St, ructure, Ġ{, Ġpublic, ĠI, ...\n123\n[1039, 34603, 285, 13842, 1296, 14181, 3750, 1...\n[1039, 34603, 285, 13842, 1296, 14181, 3750, 1...\n[1039, 34603, 285, 13842, 1296, 14181, 3750, 1...\n[1039, 34603, 285, 13842, 1296, 14181, 3750, 1...\n[1039, 34603, 285, 13842, 1296, 14181, 3750, 1...\n[1039, 34603, 285, 13842, 1296, 14181, 3750, 1...\n...\n[1039, 34603, 285, 13842, 1296, 14181, 3750, 1...\n[1039, 34603, 285, 13842, 1296, 14181, 3750, 1...\n[1039, 34603, 285, 13842, 1296, 14181, 3750, 1...\n[1039, 34603, 285, 13842, 1296, 14181, 3750, 1...\n[1039, 34603, 285, 13842, 1296, 14181, 3750, 1...\n[1039, 34603, 285, 13842, 1296, 14181, 3750, 1...\n[1039, 34603, 285, 13842, 1296, 14181, 3750, 1...\n[1039, 34603, 285, 13842, 1296, 14181, 3750, 1...\n[1039, 34603, 285, 13842, 1296, 14181, 3750, 1...\n[1121, 4771, 2068, 139, 5320, 20636, 25522, 28...\n\n\n3\n28852\nSgfParser { public List&lt;Short&gt; parseGameFromFi...\n[S, g, f, Parser, Ġ{, Ġpublic, ĠList, &lt;, Short...\n150\n[1039, 34603, 285, 13842, 1296, 22011, 1090, 2...\n[1039, 34603, 285, 13842, 197, 22011, 1090, 20...\n[1039, 34603, 285, 13842, 1296, 22011, 1090, 2...\n[1039, 34603, 285, 13842, 1296, 22011, 1090, 2...\n[1039, 34603, 285, 13842, 1296, 22011, 1090, 2...\n[1039, 34603, 285, 13842, 1296, 22011, 1090, 2...\n...\n[1039, 34603, 285, 13842, 1296, 22011, 1090, 2...\n[1039, 34603, 285, 13842, 1296, 22011, 1090, 2...\n[1039, 34603, 285, 13842, 1296, 22011, 1090, 2...\n[1039, 34603, 285, 13842, 1296, 22011, 1090, 2...\n[1039, 34603, 285, 13842, 1296, 22011, 1090, 2...\n[1039, 34603, 285, 13842, 1296, 22011, 1090, 2...\n[1039, 34603, 285, 13842, 1296, 22011, 1090, 2...\n[1039, 34603, 285, 13842, 197, 22011, 1090, 20...\n[1039, 34603, 285, 13842, 1296, 22011, 1090, 2...\n[104, 571, 506, 49707, 25522, 285, 9527, 41552...\n\n\n4\n54142\nPredictionContainerGenerator extends AbstractA...\n[Pred, iction, Container, Gener, ator, Ġextend...\n162\n[1039, 34603, 285, 13842, 1296, 21527, 45788, ...\n[1039, 34603, 285, 13842, 1296, 21527, 45788, ...\n[1039, 34603, 285, 13842, 1296, 21527, 45788, ...\n[1039, 34603, 285, 13842, 1296, 21527, 45788, ...\n[1039, 34603, 285, 13842, 1296, 21527, 45788, ...\n[1039, 34603, 285, 13842, 1296, 21527, 45788, ...\n...\n[1039, 34603, 285, 13842, 1296, 21527, 45788, ...\n[1039, 34603, 285, 13842, 1296, 21527, 45788, ...\n[1039, 34603, 285, 13842, 1296, 21527, 45788, ...\n[1039, 34603, 285, 13842, 1296, 21527, 45788, ...\n[1039, 34603, 285, 13842, 1296, 21527, 45788, ...\n[1039, 34603, 285, 13842, 1296, 21527, 45788, ...\n[1039, 34603, 285, 13842, 1296, 21527, 45788, ...\n[1039, 34603, 285, 13842, 1296, 21527, 45788, ...\n[1039, 34603, 285, 13842, 1296, 21527, 45788, ...\n[45408, 26579, 48557, 40025, 2630, 14269, 4364...\n\n\n\n\n5 rows × 35 columns\n\n\n\n\n#tst decoding\ndecoded = model.decode(df_generated_input['1'][0])\ndecoded\n\n'@ Test ( expected Ġ= ĠIllegal Arg ument Exception . class ) Ġpublic Ġvoid Ġtest Drop Table Null Table () Ġ{ Ġdrop . table ( null ); Ġ}'\n\n\n\nprettify_java( lazy_decode( decoded ) )\n\n'@Test(expected = IllegalArgumentException.class) public void testDropTableNullTable() {\\n     drop.table(null);\\n }\\n'\n\n\n\n## MEMORy DEALLOCATION\ntorch.cuda.empty_cache()\n\nNameError: name 'torch' is not defined"
  },
  {
    "objectID": "mapping_taxonomy.html",
    "href": "mapping_taxonomy.html",
    "title": "Mapping Function",
    "section": "",
    "text": "import pandas as pd\nfrom transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer\nimport json\nimport random\nimport numpy as np\n\n/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (2.0.2) or chardet (3.0.4) doesn't match a supported version!\n  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n2023-07-17 13:37:14.601094: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2023-07-17 13:37:14.754445: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nfrom code_rationales.loader import download_grammars\nfrom tree_sitter import Language, Parser\nimport code_rationales"
  },
  {
    "objectID": "mapping_taxonomy.html#java-taxonomy",
    "href": "mapping_taxonomy.html#java-taxonomy",
    "title": "Mapping Function",
    "section": "Java Taxonomy",
    "text": "Java Taxonomy\n\n#Programming Language Taxonomy\ndef pl_taxonomy_java() -&gt; dict:\n    return {\n  \"parenthesis\": { #Category-Level Label\n    \"&lt;{&gt;\": \"{\", #Token-Level Label\n    \"&lt;}&gt;\": \"}\",\n    \"&lt;[&gt;\": \"[\",\n    \"&lt;]&gt;\": \"]\",\n    \"&lt;(&gt;\": \"(\",\n    \"&lt;)&gt;\": \")\"    \n  },\n  \"semi_colon\":{\n    \"&lt;;&gt;\": \";\",\n    \"&lt;:&gt;\": \":\"\n  },\n  \"comma_dot\":{\n    \"&lt;,&gt;\": \",\",\n    \"&lt;.&gt;\": \".\",\n    \"&lt;...&gt;\": \"...\"\n  },\n  \"exceptions\": {\n    \"&lt;catch&gt;\": \"catch\",\n    \"&lt;try&gt;\": \"try\",\n    \"&lt;finally&gt;\": \"finally\",\n    \"&lt;throw&gt;\": \"throw\",\n    \"&lt;throws&gt;\": \"throws\"\n  },\n  \"oop\": {\n    \"&lt;class&gt;\": \"class\",\n    \"&lt;instanceof&gt;\": \"instanceof\",\n    \"&lt;interface&gt;\": \"interface\",\n    \"&lt;private&gt;\": \"private\",\n    \"&lt;protected&gt;\": \"protected\",\n    \"&lt;public&gt;\": \"public\",\n    \"&lt;abstract&gt;\": \"abstract\",\n    \"&lt;extends&gt;\": \"extends\",\n    \"&lt;package&gt;\": \"package\",\n    \"&lt;this&gt;\": \"this\",\n    \"&lt;implements&gt;\": \"implements\",\n    \"&lt;import&gt;\": \"import\",\n    \"&lt;new&gt;\": \"new\",\n    \"&lt;super&gt;\": \"super\"\n  },\n  \"asserts\": {\n    \"&lt;assert&gt;\": \"assert\"\n  },\n  \"types\": {\n    \"&lt;native&gt;\": \"native\",\n    \"&lt;static&gt;\": \"static\",\n    \"&lt;synchronized&gt;\": \"synchronized\",\n    \"&lt;transient&gt;\": \"transient\",\n    \"&lt;volatile&gt;\": \"volatile\",\n    \"&lt;void&gt;\": \"void\",\n    \"&lt;final&gt;\": \"final\",\n    \"&lt;enum&gt;\": \"enum\",\n    \"&lt;byte&gt;\": \"byte\",\n    \"&lt;char&gt;\": \"char\",\n    \"&lt;float&gt;\": \"float\",\n    \"&lt;boolean&gt;\": \"boolean\",\n    \"&lt;double&gt;\": \"double\",\n    \"&lt;int&gt;\": \"int\",\n    \"&lt;long&gt;\": \"long\",\n    \"&lt;short&gt;\": \"short\",\n    \"&lt;strictfp&gt;\": \"strictfp\"\n  },\n  \"conditionals\": {\n    \"&lt;else&gt;\": \"else\",\n    \"&lt;if&gt;\": \"if\",\n    \"&lt;switch&gt;\": \"switch\",\n    \"&lt;case&gt;\": \"case\",\n    \"&lt;default&gt;\": \"default\"\n  },\n  \"loops\": {\n    \"&lt;break&gt;\": \"break\",\n    \"&lt;do&gt;\": \"do\",\n    \"&lt;for&gt;\": \"for\",\n    \"&lt;while&gt;\": \"while\",\n    \"&lt;continue&gt;\": \"continue\"\n  },\n  \"operators\": {\n    \"&lt;=&gt;\": \"=\",\n    \"&lt;+&gt;\": \"+\",\n    \"&lt;-&gt;\": \"-\",\n    \"&lt;*&gt;\": \"*\",\n    \"&lt;/&gt;\": \"/\",\n    \"&lt;%&gt;\": \"%\",\n    \"&lt;++&gt;\": \"++\",\n    \"&lt;--&gt;\": \"--\",\n    \"&lt;!&gt;\": \"!\",\n    \"&lt;==&gt;\": \"==\",\n    \"&lt;!=&gt;\": \"!=\",\n    \"&lt;greater_equal&gt;\": \"&gt;=\",\n    \"&lt;lesser_equal&gt;\": \"&lt;=\",\n    \"&lt;&&&gt;\": \"&&\",\n    \"&lt;||&gt;\": \"||\",\n    \"&lt;?&gt;\": \"?\",\n    \"&lt;:&gt;\": \":\",\n    \"&lt;~&gt;\": \"~\",\n    \"&lt;double_lesser&gt;\": \"&lt;&lt;\",\n    \"&lt;double_greater&gt;\": \"&gt;&gt;\",\n    \"&lt;triple_greater&gt;\": \"&gt;&gt;&gt;\",\n    \"&lt;&&gt;\": \"&\",\n    \"&lt;^&gt;\": \"^\",\n    \"&lt;|&gt;\": \"|\"\n  },\n  \"newline\": {\n    \"&lt;n&gt;\": \"\\n\"\n  },\n  \"tab\": {\n    \"&lt;t&gt;\": \"\\t\"\n  },\n  \"ampersand\": {\n    \"&lt;@&gt;\": \"@\"\n  },\n  \"bool\": {\n    \"&lt;true&gt;\": \"true\",\n    \"&lt;false&gt;\": \"false\",\n  }\n}\n\n\ndef map_from_preprocessed_java( preprocessed_sequence ) -&gt; list:\n    a = preprocessed_sequence\n    return list(a)"
  },
  {
    "objectID": "mapping_taxonomy.html#python-taxonomy",
    "href": "mapping_taxonomy.html#python-taxonomy",
    "title": "Mapping Function",
    "section": "Python Taxonomy",
    "text": "Python Taxonomy\n\n#Programming Language Taxonomy\ndef pl_taxonomy_python() -&gt; dict:\n    return {\n  \"parenthesis\": ['{', '}', '[', ']', '(', ')'], \n  \"exceptions\": ['raise_statement','catch', 'try', 'finally', 'throw', 'throws', 'except'],\n  \"oop\": ['def','class','instanceof','interface','private','protected','public','abstract','extends','package','this','implements','import','new','super'],\n  \"asserts\": ['assert'],\n  \"types\": ['pair','subscript','type','none','dictionary','integer','native','static','synchronized','transient','volatile','void','final','enum','byte','char','float','boolean','double','int','long','short','strictfp'],\n  \"conditionals\": ['else', 'if', 'switch', 'case', 'default'],\n  \"loops\": ['break', 'do', 'for', 'while', 'continue'],\n  \"operators\": ['or','not','**','slice','%','+','&lt;','&gt;','=','+','-','*','/','%','++','--','!','==','!=','&gt;=','&lt;=','&&','||','?',':','~','&lt;&lt;','&gt;&gt;','&gt;&gt;&gt;','&','^','|'],\n  \"newline\": ['\\n'],\n  \"tab\": ['\\t'],\n  \"ampersand\": ['@'],\n  \"bool\": ['true', 'false'], \n  \"string\": ['string'], \n  \"punctuation\" : ['\\\"', ',', '.', '...',']', ';', ':'], \n  \"with\" : ['with'], \n  \"structural\" : ['module', 'argument_list', 'lambda_parameters'],\n  \"statements\" : ['return']\n\n}\n\n\n'pair' in pl_taxonomy_python()['types']\n\nFalse"
  },
  {
    "objectID": "mapping_taxonomy.html#ast-mapping",
    "href": "mapping_taxonomy.html#ast-mapping",
    "title": "Mapping Function",
    "section": "AST Mapping",
    "text": "AST Mapping\n\nCalculate Spans\n\n#df_rationals = pd.read_csv('/workspaces/code-rationales/data/rationales/gpt/testing/[t_100]_[max_tgt_44]_[exp:0]_.csv',index_col=0)\n\n\n#df_rationals = df_rationals[df_rationals['from_seq_id'] == 0]\n\n\n### Retrieve the generated output\n#initial_token = eval(df_rationals['typesets_tgt'][0])[0][0]\n#code = initial_token + ''.join(df_rationals['goal_token'])\n#code\n\n\n#### Add Span column\n#calculate_left_span = lambda index : len(initial_token + ''.join(df_rationals['goal_token'][:index]))\n#calculate_right_span = lambda left_span, token : len(left_span) + len(token)\n#span_col = list(map(lambda tuple: (tuple[0],tuple[0]+len(tuple[1])),[(calculate_left_span(index),token) for index, token in df_rationals['goal_token'].items()]))\n#df_rationals.insert(loc=df_rationals.columns.get_loc('goal_token')+1, column='span', value=span_col)\n\n\n#df_rationals\n\n\n\nMap Tokens with Nodes\n\nlanguages=['python', 'java']\ndownload_grammars(languages)\n\n/usr/local/lib/python3.8/dist-packages/code_rationales/grammars\n\n\n\ndef unroll_node_types(\n    nested_node_types: dict  # node_types from tree-sitter\n) -&gt; list: # list of node types\n    def iterate_and_unroll_dict(nested_node_types: dict, all_node_types: set):\n        for key, value in nested_node_types.items():\n            if key == 'type' and type(value) == str:\n                all_node_types.add(value)\n            if type(value) == dict:\n                iterate_and_unroll_dict(value, all_node_types)\n            if type(value) == list:\n                for element in value:\n                    iterate_and_unroll_dict(element, all_node_types) \n    all_node_types = set()\n    for dictionary in nested_node_types:\n        iterate_and_unroll_dict(dictionary, all_node_types)\n    all_node_types.add('ERROR')\n    return list(all_node_types)\n\n\ndef create_parser(lang: str):\n    # Grab the node types from the tree-sitter language\n    language = Language(f\"{code_rationales.__path__[0]}/grammars/tree-sitter-languages.so\", lang)\n    node_path = f\"{code_rationales.__path__[0]}/grammars/tree-sitter-{lang}/src/node-types.json\"\n    with open(node_path) as f:\n            node_types = json.load(f)\n    node_types = unroll_node_types(node_types)\n    # Create a parser for the language\n    parser = Parser()\n    parser.set_language(language)\n    return parser, node_types\n\n\ndef traverse(\n    node,       # tree-sitter node\n) -&gt; None:\n    \"\"\"Traverse in a recursive way, a tree-sitter node and append results to a list.\"\"\"\n    results = []\n    def traverse_tree(node, results):\n        if node.type == 'string':\n            results.append(node)\n            return\n        for n in node.children:\n            traverse_tree(n, results)\n        if not node.children:\n            results.append(node)\n    traverse_tree(node, results)\n    return results\n\n\ndef convert_to_offset(\n    point,              #point to convert\n    lines: list         #list of lines in the source code\n    ):\n        \"\"\"Convert the point to an offset\"\"\"\n        row, column = point\n        chars_in_rows = sum(map(len, lines[:row])) + row\n        chars_in_columns = len(lines[row][:column])\n        offset = chars_in_rows + chars_in_columns\n        return offset\n\n\ndef get_node_span(node, lines):\n    \"\"\"Get the span position of the node in the code string\"\"\"\n    start_span = convert_to_offset(node.start_point, lines)\n    end_span = convert_to_offset(node.end_point, lines)\n    return start_span, end_span\n\n\ndef get_token_type(\n    tok_span: tuple, # (start, end) position of a token in tokenizer\n    nodes: list,     # list of tree-sitter nodes\n    lines: list,     # list of lines in the code\n) -&gt; tuple: # (parent_type, token_type) of the token\n    \"\"\"Get the parent AST type and token AST type of a token.\"\"\"\n    node_spans = [get_node_span(node, lines) for node in nodes]\n    for i, span in enumerate(node_spans):\n        if (span[0] &lt;= tok_span[0] and tok_span[0] &lt; span[1]) or (span[0] &lt; tok_span[1] and tok_span[1] &lt;= span[1]):\n            return nodes[i].parent.type, nodes[i].type\n\n\ndef get_token_nodes(\n    tok_span: tuple, # (start, end) position of a token in tokenizer\n    node,            # tree-sitter node\n    lines: list,     # list of lines in the code\n) -&gt; list: \n    \"\"\"Get all AST types for the given token span\"\"\"\n    results = []\n    def traverse_and_get_types(tok_span, node, lines, results) -&gt; None:\n        node_span = get_node_span(node, lines)\n        if (node_span[0] &lt;= tok_span[0] and tok_span[0] &lt; node_span[1]) or (node_span[0] &lt; tok_span[1] and tok_span[1] &lt;= node_span[1]):\n            results.append(node.type)\n        for n in node.children:\n            traverse_and_get_types(tok_span, n, lines, results)\n    traverse_and_get_types(tok_span, node, lines, results)\n    return results\n\n\n#parser, node_types = create_parser('python')\n\n\n#nodes = traverse(parser.parse(bytes(code, 'utf8')).root_node)\n\n\n#print(get_token_type(df_rationals['span'][40], nodes, code.split(\"\\n\")))\n\n\n#print(get_token_nodes(df_rationals['span'][42], parser.parse(bytes(code, 'utf8')).root_node, code.split(\"\\n\")))\n\n\n#print(eval(df_rationals['rationale_pos_tgt'][2]))\n#print(eval(df_rationals['rationale_prob_tgt'][2]))\n\n\n#print(df_rationals['goal_token'][eval(df_rationals['rationale_pos_tgt'][2])[0]-1])\n#print(df_rationals['span'][eval(df_rationals['rationale_pos_tgt'][2])[0]-1])"
  },
  {
    "objectID": "mapping_taxonomy.html#rational-global-aggregates",
    "href": "mapping_taxonomy.html#rational-global-aggregates",
    "title": "Mapping Function",
    "section": "Rational Global Aggregates",
    "text": "Rational Global Aggregates\n\ndef param_default():\n    return {\n        #'dataset' : 'code_completion_random_cut_5k_30_512_tokens',\n        'dataset' : 'code_completion_docstring_random_cut_3.8k_30_150_tokens',\n        #'dataset' : 'code_completion_docstring_signature_3.8k_30_150_tokens',\n        #'dataset' : 'code_completion_docstring_5k_30_150_tokens',\n        'rational_results': '/workspaces/code-rationales/data/rationales/gpt',\n        'global_results': '/workspaces/code-rationales/data/global_results/gpt',\n        'num_samples' : 100, \n        'size_samples' : 146,\n        'num_experiments': 30, \n        'bootstrapping' : 500\n    }\nparams = param_default()\n\n\nget_experiment_path =  lambda samples, size, exp: params['rational_results'] + '/' + params['dataset'] + '/' + '[t_'+str(samples)+']_[max_tgt_'+str(size)+']_[exp:'+str(exp)+']_.csv'\ncalculate_left_span = lambda index, initial_token, df_rationals : len(str(initial_token) + ''.join(str(df_rationals['goal_token'][:index])))\ncalculate_right_span = lambda left_span, token : len(left_span) + len(token)\n\n\n### Retrieve experiments\nexperiment_paths = [get_experiment_path(params['num_samples'], params['size_samples'], exp) for exp in range(params['num_experiments'])]\n### Define parser\nparser, node_types = create_parser('python')\n\n\ndef aggregate_rationals(experiment_paths: list, parser, node_types: list):\n    global_results = {node_type : {node_type : [] for node_type in node_types} for node_type in node_types}\n    for exp_idx, experiment_path in enumerate(experiment_paths):\n        df_experiment = pd.read_csv(experiment_path, index_col=0)\n        experiment_rational_results = [df_experiment[(df_experiment['from_seq_id'] == sample_idx) | (df_experiment['from_seq_id'] == str(sample_idx))].reset_index() for sample_idx in range(params['num_samples'])]\n        print('*'*10 +'Aggregating rationales for exp: ' +str(exp_idx) + '*'*10)\n        for experiment_rational_result in experiment_rational_results:\n            initial_token = eval(experiment_rational_result['typesets_tgt'][0])[0][0]\n            experiment_rational_result.insert(loc=experiment_rational_result.columns.get_loc('goal_token')+1, column='span', value=list(map(lambda tuple: (tuple[0],tuple[0]+len(tuple[1])),[(calculate_left_span(index, initial_token, experiment_rational_result), str(token)) for index, token in experiment_rational_result['goal_token'].items()])))\n            target_code = eval(experiment_rational_result['typesets_tgt'][0])[0][0] + ''.join(str(experiment_rational_result['goal_token']))\n            target_ast = parser.parse(bytes(target_code, 'utf8')).root_node\n            for target_token_idx in range(len(experiment_rational_result['span'])):\n                target_node_types = get_token_nodes(experiment_rational_result['span'][target_token_idx], target_ast, target_code.split(\"\\n\"))\n                for rational_idx, rational_pos in enumerate(eval(experiment_rational_result['rationale_pos_tgt'][target_token_idx])):\n                    if eval(experiment_rational_result['rationale_pos_tgt'][target_token_idx])[rational_idx] &gt; 0: #rational 1 position.\n                        try:\n                            rational_node_types = get_token_nodes(experiment_rational_result['span'][eval(experiment_rational_result['rationale_pos_tgt'][target_token_idx])[rational_idx]-1], target_ast, target_code.split(\"\\n\"))\n                            [global_results[target_node_type][rational_node_type].append(eval(experiment_rational_result['rationale_prob_tgt'][target_token_idx])[rational_idx]) for target_node_type in target_node_types for rational_node_type in rational_node_types]\n                        except Exception as e:\n                            print('rational pos out of range')\n    return global_results\n\n\ndef clean_global_results(global_results):\n    def clean_dictonary(result_dict):\n        clean_dict = result_dict.copy()\n        for key, value in result_dict.items():\n            if not value:\n                clean_dict.pop(key)\n        return clean_dict\n    for key, value in global_results.items():\n        global_results[key] = clean_dictonary(value)\n    return clean_dictonary(global_results)\n\n\ndef bootstrapping( np_data, np_func, size ):\n    \"\"\"Create a bootstrap sample given data and a function\n    For instance, a bootstrap sample of means, or mediands. \n    The bootstrap replicates are a long as the original size\n    we can choose any observation more than once (resampling with replacement:np.random.choice)\n    \"\"\"\n    \n    #Cleaning NaNs\n    #np_data_clean = np_data[ np.logical_not( np.isnan(np_data) ) ] \n    \n    #The size of the bootstrap replicate is as big as size\n    #Creating the boostrap replicates as long as the orignal data size\n    #This strategy might work as imputation \n    bootstrap_repl = [ np_func( np.random.choice( np_data, size=len(np_data) ) ) for i in range( size ) ]\n    \n    #logging.info(\"Covariate: \" + cov) #Empirical Mean\n    #logging.info(\"Empirical Mean: \" + str(np.mean(np_data_clean))) #Empirical Mean\n    #logging.info(\"Bootstrapped Mean: \" + str( np.mean(bootstrap_repl) ) ) #Bootstrapped Mean\n    \n    return np.array( bootstrap_repl )\n\n\ndef bootstrap_samples_global_results(global_results: dict, size: int):\n    for target_type, target_value in global_results.items():\n        for source_type, source_value in target_value.items():\n            global_results[target_type][source_type] = bootstrapping(source_value, np.mean, size).tolist()\n\n\n### WARNING TAKES TIME\nglobal_results = clean_global_results(aggregate_rationals(experiment_paths, parser, node_types))\n\n**********Aggregating rationales for exp: 0**********\n\n\n\n### WARNING TAKES TIME\nbootstrap_samples_global_results(global_results, params['bootstrapping'])\n\n\n#with open(params['global_results'] + '/' + params['dataset'] + '.txt', 'w') as file:\n#    file.write(json.dumps(global_results))\n\n\nwith open(params['global_results'] + '/' + params['dataset'] + '.txt', 'r') as file:\n    global_results = json.load(file)\n\n\n#print(len(global_results.keys()))\nfor key in global_results.keys():\n    print(key)\n\n&gt;\nfor\nexcept\ninteger\nsubscript\n*\nfrom\ndictionary\n&lt;\nstring\n\"\n:\nnone\n+\n%\nif\ntype\nslice\nraise_statement\n;\nwith\ndictionary_splat\ndef\n**\nassert\n|\n=\n&lt;=\nnot\n)\nmodule\nlist_splat\npair\nargument_list\nlambda_parameters\n{\nreturn\nor\nwith_item\nwith_statement\nblock\nboolean_operator\nreturn_statement\n]\ncomparison_operator\nescape_sequence\nfalse\n==\nlist\nas_pattern\nset\nas_pattern_target\n}\nand\nERROR\nbinary_operator\nwith_clause\nassert_statement\nin\n@\n[\ntrue\nunary_operator\nis\nexec\n,\nasync\nyield\nclass\n/\ntuple\nexpression_statement\nwhile_statement\n.\ncall\nellipsis\n(\nas\nbreak_statement\nassignment\n//\n-\nparenthesized_expression\npattern_list\nbreak\nidentifier\ntry\ncomment\nwhile\nclass_definition\nraise\nlambda\nattribute"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "code-rationales",
    "section": "",
    "text": "This file will become your README and also the index of your documentation."
  },
  {
    "objectID": "index.html#install",
    "href": "index.html#install",
    "title": "code-rationales",
    "section": "Install",
    "text": "Install\npip install code_rationales"
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "code-rationales",
    "section": "How to use",
    "text": "How to use\nFill me in please! Don’t forget code examples:\n\n1+1\n\n2"
  },
  {
    "objectID": "gpt_global_rationalization.html",
    "href": "gpt_global_rationalization.html",
    "title": "Rationalization @ Global Granularity",
    "section": "",
    "text": "from pathlib import Path\nimport csv\nimport seaborn as sns; sns.set_theme()\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nimport functools\n\npd.options.display.float_format = '{:.2f}'.format\nfrom sacrebleu.metrics import BLEU\nfrom transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer\nfrom datasets import load_dataset, load_from_disk\nimport torch\n\n/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (2.0.2) or chardet (3.0.4) doesn't match a supported version!\n  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n2023-07-13 20:49:06.515351: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2023-07-13 20:49:06.720195: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nimport warnings\nimport importlib\nfrom matplotlib import colors\nimport os\nimport sys\nsys.path.insert(1, '/workspaces/code-rationales/sequential-rationales/huggingface')\nfrom rationalization import rationalize_lm\ndef param_default():\n    return {\n        'model_name' : '/workspaces/code-rationales/data/codeparrot-small/checkpoints/checkpoint-29000', \n        'cache_dir': '/workspaces/code-rationales/datax/df_cache_dir',\n        #'dataset' : 'code_completion_random_cut_5k_30_512_tokens',\n        #'dataset' : 'code_completion_docstring_random_cut_3.8k_30_150_tokens',\n        #'dataset' : 'code_completion_docstring_signature_3.8k_30_150_tokens',\n        #'dataset' : 'code_completion_docstring_5k_30_150_tokens',\n        'dataset' : 'code_completion_docstring_signature_5k_30_512_tokens',\n        'sampling_results': '/workspaces/code-rationales/data/sampling/gpt',\n        'rational_results': '/workspaces/code-rationales/data/rationales/gpt',\n    }\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
  },
  {
    "objectID": "gpt_global_rationalization.html#model-loading-and-testing",
    "href": "gpt_global_rationalization.html#model-loading-and-testing",
    "title": "Rationalization @ Global Granularity",
    "section": "Model Loading and Testing",
    "text": "Model Loading and Testing\n\nmodel = AutoModelForCausalLM.from_pretrained(\n            param_default()['model_name'],\n            cache_dir=param_default()['cache_dir'])\n\n\nmodel.to(device)\nmodel.eval()\n\nGPT2LMHeadModel(\n  (transformer): GPT2Model(\n    (wte): Embedding(32768, 768)\n    (wpe): Embedding(1024, 768)\n    (drop): Dropout(p=0.1, inplace=False)\n    (h): ModuleList(\n      (0): GPT2Block(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2Attention(\n          (c_attn): Conv1D()\n          (c_proj): Conv1D()\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D()\n          (c_proj): Conv1D()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (1): GPT2Block(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2Attention(\n          (c_attn): Conv1D()\n          (c_proj): Conv1D()\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D()\n          (c_proj): Conv1D()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (2): GPT2Block(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2Attention(\n          (c_attn): Conv1D()\n          (c_proj): Conv1D()\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D()\n          (c_proj): Conv1D()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (3): GPT2Block(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2Attention(\n          (c_attn): Conv1D()\n          (c_proj): Conv1D()\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D()\n          (c_proj): Conv1D()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (4): GPT2Block(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2Attention(\n          (c_attn): Conv1D()\n          (c_proj): Conv1D()\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D()\n          (c_proj): Conv1D()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (5): GPT2Block(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2Attention(\n          (c_attn): Conv1D()\n          (c_proj): Conv1D()\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D()\n          (c_proj): Conv1D()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (6): GPT2Block(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2Attention(\n          (c_attn): Conv1D()\n          (c_proj): Conv1D()\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D()\n          (c_proj): Conv1D()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (7): GPT2Block(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2Attention(\n          (c_attn): Conv1D()\n          (c_proj): Conv1D()\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D()\n          (c_proj): Conv1D()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (8): GPT2Block(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2Attention(\n          (c_attn): Conv1D()\n          (c_proj): Conv1D()\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D()\n          (c_proj): Conv1D()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (9): GPT2Block(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2Attention(\n          (c_attn): Conv1D()\n          (c_proj): Conv1D()\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D()\n          (c_proj): Conv1D()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (10): GPT2Block(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2Attention(\n          (c_attn): Conv1D()\n          (c_proj): Conv1D()\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D()\n          (c_proj): Conv1D()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (11): GPT2Block(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2Attention(\n          (c_attn): Conv1D()\n          (c_proj): Conv1D()\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D()\n          (c_proj): Conv1D()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  )\n  (lm_head): Linear(in_features=768, out_features=32768, bias=False)\n)"
  },
  {
    "objectID": "gpt_global_rationalization.html#tokenizer-loading-and-testing",
    "href": "gpt_global_rationalization.html#tokenizer-loading-and-testing",
    "title": "Rationalization @ Global Granularity",
    "section": "Tokenizer Loading and Testing",
    "text": "Tokenizer Loading and Testing\n\ntokenizer = AutoTokenizer.from_pretrained(param_default()['model_name'])"
  },
  {
    "objectID": "gpt_global_rationalization.html#data-loading-and-testing",
    "href": "gpt_global_rationalization.html#data-loading-and-testing",
    "title": "Rationalization @ Global Granularity",
    "section": "Data Loading and Testing",
    "text": "Data Loading and Testing\n\n#Loading Code Generation\ndf_generated_input = pd.read_csv( param_default()['sampling_results'] + '/' + param_default()['dataset'] +'.csv', index_col=0)\n\n\ndf_generated_input.columns[5:] #Tensor Columns\n\nIndex(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12',\n       '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24',\n       '25', '26', '27', '28', '29'],\n      dtype='object')\n\n\n\ndf_generated_input.head()\n\n\n\n\n\n\n\n\nindex\nprompt\nground_truth\nsize\ninput_ids\n0\n1\n2\n3\n4\n...\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n\n\n\n\n0\n0\nGenerate Pyhton code that Tests checkboxes but...\nTests checkboxes but also acts a regression te...\n50\n[6864, 1611, 517, 265, 1233, 626, 6496, 1104, ...\n[6864, 1611, 517, 265, 1233, 626, 6496, 1104, ...\n[6864, 1611, 517, 265, 1233, 626, 6496, 1104, ...\n[6864, 1611, 517, 265, 1233, 626, 6496, 1104, ...\n[6864, 1611, 517, 265, 1233, 626, 6496, 1104, ...\n[6864, 1611, 517, 265, 1233, 626, 6496, 1104, ...\n...\n[6864, 1611, 517, 265, 1233, 626, 6496, 1104, ...\n[6864, 1611, 517, 265, 1233, 626, 6496, 1104, ...\n[6864, 1611, 517, 265, 1233, 626, 6496, 1104, ...\n[6864, 1611, 517, 265, 1233, 626, 6496, 1104, ...\n[6864, 1611, 517, 265, 1233, 626, 6496, 1104, ...\n[6864, 1611, 517, 265, 1233, 626, 6496, 1104, ...\n[6864, 1611, 517, 265, 1233, 626, 6496, 1104, ...\n[6864, 1611, 517, 265, 1233, 626, 6496, 1104, ...\n[6864, 1611, 517, 265, 1233, 626, 6496, 1104, ...\n[6864, 1611, 517, 265, 1233, 626, 6496, 1104, ...\n\n\n1\n1\nGenerate Pyhton code that \\n Factory me...\n\\n Factory method to produce an instanc...\n45\n[6864, 1611, 517, 265, 1233, 626, 4960, 23616,...\n[6864, 1611, 517, 265, 1233, 626, 4960, 23616,...\n[6864, 1611, 517, 265, 1233, 626, 4960, 23616,...\n[6864, 1611, 517, 265, 1233, 626, 4960, 23616,...\n[6864, 1611, 517, 265, 1233, 626, 4960, 23616,...\n[6864, 1611, 517, 265, 1233, 626, 4960, 23616,...\n...\n[6864, 1611, 517, 265, 1233, 626, 4960, 23616,...\n[6864, 1611, 517, 265, 1233, 626, 4960, 23616,...\n[6864, 1611, 517, 265, 1233, 626, 4960, 23616,...\n[6864, 1611, 517, 265, 1233, 626, 4960, 23616,...\n[6864, 1611, 517, 265, 1233, 626, 4960, 23616,...\n[6864, 1611, 517, 265, 1233, 626, 4960, 23616,...\n[6864, 1611, 517, 265, 1233, 626, 4960, 23616,...\n[6864, 1611, 517, 265, 1233, 626, 4960, 23616,...\n[6864, 1611, 517, 265, 1233, 626, 4960, 23616,...\n[6864, 1611, 517, 265, 1233, 626, 4960, 23616,...\n\n\n2\n2\nGenerate Pyhton code that True if this Entry h...\nTrue if this Entry has references from any App...\n52\n[6864, 1611, 517, 265, 1233, 626, 715, 340, 64...\n[6864, 1611, 517, 265, 1233, 626, 715, 340, 64...\n[6864, 1611, 517, 265, 1233, 626, 715, 340, 64...\n[6864, 1611, 517, 265, 1233, 626, 715, 340, 64...\n[6864, 1611, 517, 265, 1233, 626, 715, 340, 64...\n[6864, 1611, 517, 265, 1233, 626, 715, 340, 64...\n...\n[6864, 1611, 517, 265, 1233, 626, 715, 340, 64...\n[6864, 1611, 517, 265, 1233, 626, 715, 340, 64...\n[6864, 1611, 517, 265, 1233, 626, 715, 340, 64...\n[6864, 1611, 517, 265, 1233, 626, 715, 340, 64...\n[6864, 1611, 517, 265, 1233, 626, 715, 340, 64...\n[6864, 1611, 517, 265, 1233, 626, 715, 340, 64...\n[6864, 1611, 517, 265, 1233, 626, 715, 340, 64...\n[6864, 1611, 517, 265, 1233, 626, 715, 340, 64...\n[6864, 1611, 517, 265, 1233, 626, 715, 340, 64...\n[6864, 1611, 517, 265, 1233, 626, 715, 340, 64...\n\n\n3\n3\nGenerate Pyhton code that Set packet parent.\\n...\nSet packet parent.\\n When packet is an ...\n52\n[6864, 1611, 517, 265, 1233, 626, 2494, 3644, ...\n[6864, 1611, 517, 265, 1233, 626, 2494, 3644, ...\n[6864, 1611, 517, 265, 1233, 626, 2494, 3644, ...\n[6864, 1611, 517, 265, 1233, 626, 2494, 3644, ...\n[6864, 1611, 517, 265, 1233, 626, 2494, 3644, ...\n[6864, 1611, 517, 265, 1233, 626, 2494, 3644, ...\n...\n[6864, 1611, 517, 265, 1233, 626, 2494, 3644, ...\n[6864, 1611, 517, 265, 1233, 626, 2494, 3644, ...\n[6864, 1611, 517, 265, 1233, 626, 2494, 3644, ...\n[6864, 1611, 517, 265, 1233, 626, 2494, 3644, ...\n[6864, 1611, 517, 265, 1233, 626, 2494, 3644, ...\n[6864, 1611, 517, 265, 1233, 626, 2494, 3644, ...\n[6864, 1611, 517, 265, 1233, 626, 2494, 3644, ...\n[6864, 1611, 517, 265, 1233, 626, 2494, 3644, ...\n[6864, 1611, 517, 265, 1233, 626, 2494, 3644, ...\n[6864, 1611, 517, 265, 1233, 626, 2494, 3644, ...\n\n\n4\n4\nGenerate Pyhton code that Remove packet parent...\nRemove packet parent.\\n When packet is ...\n52\n[6864, 1611, 517, 265, 1233, 626, 5852, 3644, ...\n[6864, 1611, 517, 265, 1233, 626, 5852, 3644, ...\n[6864, 1611, 517, 265, 1233, 626, 5852, 3644, ...\n[6864, 1611, 517, 265, 1233, 626, 5852, 3644, ...\n[6864, 1611, 517, 265, 1233, 626, 5852, 3644, ...\n[6864, 1611, 517, 265, 1233, 626, 5852, 3644, ...\n...\n[6864, 1611, 517, 265, 1233, 626, 5852, 3644, ...\n[6864, 1611, 517, 265, 1233, 626, 5852, 3644, ...\n[6864, 1611, 517, 265, 1233, 626, 5852, 3644, ...\n[6864, 1611, 517, 265, 1233, 626, 5852, 3644, ...\n[6864, 1611, 517, 265, 1233, 626, 5852, 3644, ...\n[6864, 1611, 517, 265, 1233, 626, 5852, 3644, ...\n[6864, 1611, 517, 265, 1233, 626, 5852, 3644, ...\n[6864, 1611, 517, 265, 1233, 626, 5852, 3644, ...\n[6864, 1611, 517, 265, 1233, 626, 5852, 3644, ...\n[6864, 1611, 517, 265, 1233, 626, 5852, 3644, ...\n\n\n\n\n5 rows × 35 columns\n\n\n\n\ndf_generated_input.shape\n\n(100, 35)\n\n\n\n#tst decoding\ndecoded = tokenizer.decode(eval(df_generated_input['0'][1]))\ndecoded\n\n'Generate Pyhton code that \\n        Factory method to produce an instance of this class using the default kube config location\\n         and signature is def from_environment(cls):\\n            with open(os.environ[\\'WORKER_CRAMDOOT\\'], \\'r\\') as f:\\n                f.read() \\n    from_configuration = Factory({\\n        \\n       \\'version\\': u\\'\\'\\n        })\\n    from_config.from_config_path()\\n\\n    from_context_manager = Factory(dict(\\n        uuid\\n        u\\'username\\': u\\'berry\\',\\n        u\\'password\\': u\\'password\\'\\n    ),\\n        dict(username = u\\'berry\\', password = u\\'pass\\'\\n    ),\\n        u\\'BOOKE_API_KEY\\': oxo_site_config[\"BOOKE_API_KEY\"]\\n    )\\n\\n    return HttpContext(options_dict(\\n        filtered_services=[\"requests\", \\n                             config_manager, \\n                             config_loader,\\n                             config_finders,\\n                             config_finds_dir,\\n                            config_manager_is_imported,\\n                            config_manager_args,\\n                            config_loader_log\"]),\\n        run_args={\"requests\": advanced_group, \"search\": advanced_search, \"search_pages\": advanced_search_pages, \"streaming\": advanced_streaming, \"client\": custom_client}\\n    ))\\n                      \\n           \\n        # kwarg to the class\\n        setup=from_configuration[\"setup\"],\\n        tool=blueprint_tools.toolhOLDER_COMMAND_SERVER_ARG,\\n        config_manager=from_configuration[\"config_manager\"],\\n        server=config_manager.get(\\'server\\', \\'127.0.0.1\\'),\\n        config_loader=from_configuration[\"client\"],\\n        context=from_context_manager,\\n        config_finders=config_finders,\\n        env=vmdk_session.get_nt_context()\\n    ))\\nclass Artifact(with_cached_http=True,\\n           controller=artifact_manager.ArtifactManagerController):\\n    \"\"\"\\n    A generator around a python script inside the Bokeh server.\\n    \\n    Use this class to make an zeros spark web client mounted in the session and attach a python script\\n     to it as a decorator. This decorator will attach an object to a'"
  },
  {
    "objectID": "gpt_global_rationalization.html#running-rationales",
    "href": "gpt_global_rationalization.html#running-rationales",
    "title": "Rationalization @ Global Granularity",
    "section": "Running Rationales",
    "text": "Running Rationales\n\n#Statistics\nnp.mean( [len(eval(i)) for i in df_generated_input['0'].values] )\n\n464.0\n\n\n\n#TODO Run the distribution of each experiment. The mean value of tokens or size for each experiment. \nnp.mean( [len(eval(i)) for i in df_generated_input['input_ids'].values] )\n\n88.12\n\n\n\nlen(df_generated_input['0'].values[1])\n\n2392\n\n\n\nMAX_TOKEN_SIZE = df_generated_input['size'].max() #Hardocoded!!\n\n\n#If the model is not fine-tuned or compatible, it will rise an error\n#This function works for one tensor of source token and one tensor of target tokens\ndef rationalize_model(model, tokenizer, input_ids, verbose=True):\n    all_rationales, log = rationalize_lm(\n        model = model,\n        input_ids = input_ids[:MAX_TOKEN_SIZE],\n        tokenizer = tokenizer,\n        verbose = verbose,\n        max_steps=1024 #Max number of steps for greedy rationalization\n    )\n    return all_rationales, log\n\n\n#tst &lt;------- Test Case 2\ndef tst_rationalize_model():\n    torch.cuda.empty_cache() #Cleaning Cache\n    #WARNING TIME CONSUMING\n    all_rationales, log = rationalize_model(\n        model=model, \n        tokenizer=tokenizer, \n        input_ids=torch.tensor(eval(df_generated_input['0'][0])).to(model.device),\n        verbose=False\n    )\n    pass\ntst_rationalize_model()\n\n\ndef run_multiple_rational(\n    model,\n    tokenizer, \n    arr_target_tokens, \n    seq_id, #mapping sequence id\n    verbose=True\n):\n    arr_log = []\n    for index, val in enumerate(arr_target_tokens):\n        all_rationales, log = rationalize_model(\n            model=model, \n            tokenizer=tokenizer, \n            input_ids=val,\n            verbose=False\n        )\n        arr_log.append(log)\n    arr_code_rationales = [ log['rationalization'] for log in arr_log ] #extracting just rationalizations\n    arr_from_sentence = [ list(np.full( len(val), seq_id[arr_i] )) #arr_i maps to the real sequence id\n                            for arr_i, val in enumerate(arr_code_rationales)]\n    arr_code_rationales = sum( arr_code_rationales, [] ) #flatting\n    arr_from_sentence = sum( arr_from_sentence, [] ) #flatting\n    return arr_code_rationales, arr_from_sentence\n\n\nimport gc\n\n\n#tst &lt;------- Test Case 2\ndef tst_run_multiple_rationa():\n    gc.collect()\n    torch.cuda.empty_cache() #Cleaning Cache\n    t_dict_generated_input = { exp : [ torch.tensor(eval(s)).to(model.device) for \n                s in df_generated_input[exp].values ] for exp in df_generated_input.columns[5:]  }\n    \n    arr_rations, seq_id = run_multiple_rational(\n        model = model,\n        tokenizer = tokenizer,\n        arr_target_tokens =  t_dict_generated_input['0'][:2], \n        seq_id = list( range(2,4) ),\n        verbose = False\n        )\n    return arr_rations, seq_id\ntst_arr_rations, seq_id = tst_run_multiple_rationa()\n\n\ndef pandas_rationales( arr_code_rationales, arr_from_sentence ):\n    #Creating pandas_1 {p_rationale}\n    rational = lambda list_log,typeset: [ (dict_tok['added_token_text'],round(dict_tok['true_token_prob'],6)) for dict_tok in list_log if dict_tok['from']==typeset]\n    log = lambda log_row: [(log_dict['added_token_text'],log_dict['true_token_prob']) for log_dict in log_row] #Typeset\n\n    log_position = lambda log_row: [log_dict['added_token_position'] for log_dict in log_row] #Position of the Rationale\n    log_prediction = lambda log_row: [log_dict['true_token_prob'] for log_dict in log_row] #Rationale Prob\n\n    p_rationale = pd.DataFrame()\n\n    p_rationale['goal_token'] = [dict_token['goal_word'] for dict_token in arr_code_rationales]\n    p_rationale['from_seq_id'] = arr_from_sentence\n\n    p_rationale['typesets_tgt'] = [ log(log_row) for log_row in [dict_token['log'] for dict_token in arr_code_rationales]]\n    \n    p_rationale['rationale_pos_tgt'] = [ log_position(log_row) for log_row in [dict_token['log'] for dict_token in arr_code_rationales]]\n    p_rationale['rationale_prob_tgt'] = [ log_prediction(log_row) for log_row in [dict_token['log'] for dict_token in arr_code_rationales]]\n\n\n    return p_rationale\n\n\n#Running Rationalization\ndef run_code_rational( \n        df_generated_input,\n        tensor_size, #Control the size of the experiment\n        experiment = '5',\n        batch_size = 100, \n        model = model, \n        verbose = True \n    ):\n\n    arr_rationals = []\n    arr_from_seq = []\n\n    for i in range( 0 , tensor_size , batch_size ):\n        print('************************' + str(i) + '************************')\n        t_generated_input = df_generated_input[experiment].values[i:i+batch_size]\n        t_generated_input = [ torch.tensor(eval(s)).to(model.device) for s in t_generated_input]\n\n        t_arr_rationals,t_arr_from_seq = run_multiple_rational(\n            model = model,\n            tokenizer = tokenizer,\n            arr_target_tokens =  t_generated_input, \n            seq_id = list(range(i,i+batch_size)),\n            verbose = verbose\n        )\n\n        arr_rationals = arr_rationals + t_arr_rationals\n        arr_from_seq = arr_from_seq + t_arr_from_seq\n\n        gc.collect()\n        torch.cuda.empty_cache() #Cleaning Cache\n\n    #keys_tensor = list( dict_generated_input.keys() )\n    #keys_tensor = keys_tensor[:1] #HardCoded Ratios\n    #dict_arr_rations = { key : for key in keys_tensor}\n    #torch.cuda.empty_cache() #Cleaning Cache\n    print(\"Experiment Finished: \" + experiment)\n    return pandas_rationales( arr_rationals, arr_from_seq )\n\n\n#tst\ndef tst_run_code_rational_sampling_set(exp='0'):\n    gc.collect()\n    torch.cuda.empty_cache()\n    tensor_n = 3 #df_generated_input.shape[0]\n    EXP = exp\n    BATCH = 1\n    test_arr_rationals = run_code_rational( \n            df_generated_input = df_generated_input.sample( n = tensor_n, replace = False, random_state=2),\n            tensor_size = tensor_n,\n            experiment = EXP,\n            batch_size = BATCH, \n            model = model, \n            verbose = False \n        )\n    return test_arr_rationals\ndf_test_run = tst_run_code_rational_sampling_set()\n\n************************0************************\n************************1************************\n************************2************************\nExperiment Finished: 0\n\n\n\n#tst\ndf_test_run[ df_test_run['from_seq_id'] == 1]\n\n\n\n\n\n\n\n\ngoal_token\nfrom_seq_id\ntypesets_tgt\nrationale_pos_tgt\nrationale_prob_tgt\n\n\n\n\n463\nPy\n1\n[(Generate, 7.423743954859674e-05)]\n[0]\n[7.423743954859674e-05]\n\n\n464\nht\n1\n[( Py, 0.00010367632057750598), (Generate, 2.5...\n[1, 0]\n[0.00010367632057750598, 2.5881863621179946e-05]\n\n\n465\non\n1\n[(ht, 0.003939895424991846), (Generate, 0.0129...\n[2, 0, 1]\n[0.003939895424991846, 0.012913737446069717, 0...\n\n\n466\ncode\n1\n[(on, 8.253633131971583e-05), ( Py, 0.00018459...\n[3, 1, 0, 2]\n[8.253633131971583e-05, 0.00018459450802765787...\n\n\n467\nthat\n1\n[( code, 0.0016732582589611411), (Generate, 0....\n[4, 0, 1, 3, 2]\n[0.0016732582589611411, 0.018313312903046608, ...\n\n\n...\n...\n...\n...\n...\n...\n\n\n921\n_\n1\n[(lower, 0.015630951151251793), (METRICS, 0.31...\n[458, 443]\n[0.015630951151251793, 0.31624388694763184]\n\n\n922\npromote\n1\n[(_, 1.509848516434431e-06), (promote, 0.00830...\n[459, 122, 451]\n[1.509848516434431e-06, 0.008305856958031654, ...\n\n\n923\n()\n1\n[(promote, 0.04420287907123566), ((), 0.108644...\n[460, 346, 272, 74]\n[0.04420287907123566, 0.10864423960447311, 0.1...\n\n\n924\n\\n\n1\n[((), 0.3412705957889557)]\n[461]\n[0.3412705957889557]\n\n\n925\nproperties\n1\n[(\\n , 0.00015490688383579254), ( proper...\n[462, 446, 445]\n[0.00015490688383579254, 0.016215959563851357,...\n\n\n\n\n463 rows × 5 columns\n\n\n\n\ndef run_code_rational_all_set(exp, tensor_n = 100, BATCH = 10): #When Tensor_n and batch differs then 'from_seq_id' is lost\n    gc.collect()\n    torch.cuda.empty_cache()\n    EXP = exp\n    test_arr_rationals = run_code_rational( \n            df_generated_input = df_generated_input,\n            tensor_size = tensor_n,\n            experiment = EXP,\n            batch_size = BATCH, \n            model = model, \n            verbose = False \n        )\n    #Saving process\n    print('Saving process')\n    test_arr_rationals.to_csv(param_default()['rational_results'] + '/' + param_default()['dataset'] + '/' + '[t_'+str(tensor_n)+']_[max_tgt_'+str(MAX_TOKEN_SIZE)+']_[exp:' + str(EXP) +']_.csv')\n    return test_arr_rationals\n\n\n#tst\n#df_test_run = run_code_rational_all_set(exp='0')\n\n\nfor i in df_generated_input.columns[5:]: #Only Generated Sequences \n    df_test_run = run_code_rational_all_set(exp=i, tensor_n=df_generated_input.shape[0])\n\n************************0************************\n************************10************************\n\n\n\ndf_test_run.head(1)\n\n\n\n\n\n\n\n\ngoal_token\nfrom_seq_id\ntypesets_tgt\nrationale_pos_tgt\nrationale_prob_tgt\n\n\n\n\n0\nskip\n0\n[(def, 0.00029721998726017773)]\n[0]\n[0.00029721998726017773]\n\n\n\n\n\n\n\n\n#Running all Experiments\ndef exp_run_all_rationales():\n    dict_arr_rations = { key : run_code_rational(\n        df_generated_input = df_generated_input,\n        experiment = key,\n        batch_size = 10, \n        model = model, \n        verbose = False \n    ) for key in df_generated_input.columns[5:] }\n    return dict_arr_rations\n\n\n#arr_df_rationale = [pandas_rationales(dict_arr_rations[key]) for key in dict_arr_rations.keys()]"
  },
  {
    "objectID": "gpt_sampling_.html",
    "href": "gpt_sampling_.html",
    "title": "Decoder Based Transformer (GPT-2) Sampling",
    "section": "",
    "text": "from pathlib import Path\nimport csv\nimport seaborn as sns; sns.set_theme()\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nimport functools\n\npd.options.display.float_format = '{:.2f}'.format\nfrom transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer\nfrom datasets import load_dataset, load_from_disk\nimport torch\n\n/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (2.0.2) or chardet (3.0.4) doesn't match a supported version!\n  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n2023-07-14 15:38:41.969726: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2023-07-14 15:38:42.126810: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nimport warnings\nfrom matplotlib import colors\nimport os\ndef param_default():\n    return {\n        #'dataset' : 'codeparrot/codeparrot-clean-valid', #Deprecated\n        'galeras_path' : '/workspaces/code-rationales/semeru-datasets/semeru/galeras/code_rationales',\n        #'dataset' : 'code_completion_random_cut_5k_30_512_tokens',\n        #'dataset' : 'code_completion_docstring_random_cut_3.8k_30_150_tokens',\n        #'dataset' : 'code_completion_docstring_signature_3.8k_30_150_tokens',\n        'dataset' : 'code_completion_docstring_5k_30_150_tokens',\n        'dataset_disk_path': '/workspaces/code-rationales/semeru-datasets/codeparrot-clean-valid',\n        'model_name': '/workspaces/code-rationales/data/codeparrot-small/checkpoints/checkpoint-29000', \n        'cache_dir': '/workspaces/code-rationales/datax/df_cache_dir', \n        'sampling_results': '/workspaces/code-rationales/data/sampling/gpt',\n        'num_samples' : 100\n    }\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
  },
  {
    "objectID": "gpt_sampling_.html#data-loading-and-testing",
    "href": "gpt_sampling_.html#data-loading-and-testing",
    "title": "Decoder Based Transformer (GPT-2) Sampling",
    "section": "Data Loading and Testing",
    "text": "Data Loading and Testing\n\n# Save dataset in nfs\n#raw_datasets = load_dataset(param_default()['dataset'], cache_dir=param_default()['cache_dir'])\n#raw_datasets.save_to_disk(param_default()['dataset_disk_path'])\n\n\n# Reload with the `json` script\n#test_dataset = load_from_disk(param_default()['dataset_disk_path'])\n#test_dataset\n\n\ntest_dataset = load_dataset('json',data_files=param_default()['galeras_path'] + '/' + param_default()['dataset'] + '.json', cache_dir=param_default()['cache_dir'])\n\nDownloading and preparing dataset json/default to /workspaces/code-rationales/datax/df_cache_dir/json/default-3ec3ca87bbb8b171/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4...\nDataset json downloaded and prepared to /workspaces/code-rationales/datax/df_cache_dir/json/default-3ec3ca87bbb8b171/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4. Subsequent calls will reuse this data.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntest_dataset = test_dataset['train']\ntest_dataset[0]\n\n{'nloc': 9,\n 'url': 'https://github.com/OpenMined/PySyft.git',\n 'n_ast_nodes': 20,\n 'repo': 'PySyft',\n 'n_words': 6,\n 'n_whitespaces': 12,\n 'n_ast_errors': 0,\n 'prompt': 'Generate Pyhton code that Check if the grid client is up.\\n\\n    Check for Ping? Get a Pong!\\n\\n    Returns:\\n        str: pong :',\n 'signature': 'def ping() -&gt; str',\n 'commit_id': 'e92a1b85d7e22328cc525c74c710cb20b880fd21',\n 'code': 'def ping() -&gt; str:\\n    \\n\\n    return \"pong\"\\n',\n 'commit_message': 'grid: add docstring and tests for grid status api',\n 'fun_name': 'ping',\n 'ast_levels': 6,\n 'language': 'Python',\n 'token_counts': 9,\n 'path': 'packages/grid/backend/grid/api/meta/status.py',\n 'ast_errors': '',\n 'complexity': 1,\n 'id': 151,\n 'file_name': 'status.py',\n 'vocab_size': 6,\n 'n_identifiers': 2,\n 'docstring': 'Check if the grid client is up.\\n\\n    Check for Ping? Get a Pong!\\n\\n    Returns:\\n        str: pong\\n    '}\n\n\n\ntest_dataset = test_dataset.select(range(param_default()['num_samples']))\n\n\n###### IMPORTANT MODIFIY \n#df_sampled_code = test_dataset.to_pandas()[['code','prompt']] #code_completion_random_cut_5k_30_512_tokens\ndf_sampled_code = test_dataset.to_pandas()[['docstring','code','prompt']] # others\n\n\ndf_sampled_code.describe()\n\n\n\n\n\n\n\n\ndocstring\ncode\nprompt\n\n\n\n\ncount\n100\n100\n100\n\n\nunique\n100\n100\n100\n\n\ntop\nCheck if the grid client is up.\\n\\n Check f...\ndef ping() -&gt; str:\\n \\n\\n return \"pong\"\\n\nGenerate Pyhton code that Check if the grid cl...\n\n\nfreq\n1\n1\n1"
  },
  {
    "objectID": "gpt_sampling_.html#model-loading-and-testing",
    "href": "gpt_sampling_.html#model-loading-and-testing",
    "title": "Decoder Based Transformer (GPT-2) Sampling",
    "section": "Model Loading and Testing",
    "text": "Model Loading and Testing\n\nmodel = AutoModelForCausalLM.from_pretrained(\n            param_default()['model_name'],\n            cache_dir=param_default()['cache_dir'])\n\n\nmodel.to(device)\nmodel.eval()\n\nGPT2LMHeadModel(\n  (transformer): GPT2Model(\n    (wte): Embedding(32768, 768)\n    (wpe): Embedding(1024, 768)\n    (drop): Dropout(p=0.1, inplace=False)\n    (h): ModuleList(\n      (0): GPT2Block(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2Attention(\n          (c_attn): Conv1D()\n          (c_proj): Conv1D()\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D()\n          (c_proj): Conv1D()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (1): GPT2Block(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2Attention(\n          (c_attn): Conv1D()\n          (c_proj): Conv1D()\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D()\n          (c_proj): Conv1D()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (2): GPT2Block(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2Attention(\n          (c_attn): Conv1D()\n          (c_proj): Conv1D()\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D()\n          (c_proj): Conv1D()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (3): GPT2Block(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2Attention(\n          (c_attn): Conv1D()\n          (c_proj): Conv1D()\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D()\n          (c_proj): Conv1D()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (4): GPT2Block(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2Attention(\n          (c_attn): Conv1D()\n          (c_proj): Conv1D()\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D()\n          (c_proj): Conv1D()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (5): GPT2Block(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2Attention(\n          (c_attn): Conv1D()\n          (c_proj): Conv1D()\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D()\n          (c_proj): Conv1D()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (6): GPT2Block(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2Attention(\n          (c_attn): Conv1D()\n          (c_proj): Conv1D()\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D()\n          (c_proj): Conv1D()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (7): GPT2Block(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2Attention(\n          (c_attn): Conv1D()\n          (c_proj): Conv1D()\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D()\n          (c_proj): Conv1D()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (8): GPT2Block(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2Attention(\n          (c_attn): Conv1D()\n          (c_proj): Conv1D()\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D()\n          (c_proj): Conv1D()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (9): GPT2Block(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2Attention(\n          (c_attn): Conv1D()\n          (c_proj): Conv1D()\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D()\n          (c_proj): Conv1D()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (10): GPT2Block(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2Attention(\n          (c_attn): Conv1D()\n          (c_proj): Conv1D()\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D()\n          (c_proj): Conv1D()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (11): GPT2Block(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2Attention(\n          (c_attn): Conv1D()\n          (c_proj): Conv1D()\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D()\n          (c_proj): Conv1D()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  )\n  (lm_head): Linear(in_features=768, out_features=32768, bias=False)\n)\n\n\n\nmodel.device\n\ndevice(type='cuda', index=0)"
  },
  {
    "objectID": "gpt_sampling_.html#tokenizer-loading-and-testing",
    "href": "gpt_sampling_.html#tokenizer-loading-and-testing",
    "title": "Decoder Based Transformer (GPT-2) Sampling",
    "section": "Tokenizer Loading and Testing",
    "text": "Tokenizer Loading and Testing\n\ntokenizer = AutoTokenizer.from_pretrained(param_default()['model_name'])"
  },
  {
    "objectID": "gpt_sampling_.html#samples-encoding-and-filtering",
    "href": "gpt_sampling_.html#samples-encoding-and-filtering",
    "title": "Decoder Based Transformer (GPT-2) Sampling",
    "section": "Samples Encoding and Filtering",
    "text": "Samples Encoding and Filtering\n\n###### IMPORTANT MODIFIY \n#df_sampled_code['ground_truth'] = df_sampled_code['code'] #code_completion_random_cut_5k_30_512_tokens\ndf_sampled_code['ground_truth'] = df_sampled_code['docstring'] + '\\n' + df_sampled_code['code'] #others\n\n\ndf_sampled_code['size'] =  df_sampled_code['ground_truth'].map(lambda code: len(tokenizer(code)['input_ids']))\ndf_sampled_code['input_ids'] = tokenizer(df_sampled_code['prompt'].tolist())['input_ids']\n\n\ndf_sampled_code = df_sampled_code.drop('docstring', axis=1)\ndf_sampled_code = df_sampled_code.drop('code', axis=1)\n\n\ndf_sampled_code.head(5)\n\n\n\n\n\n\n\n\nprompt\nground_truth\nsize\ninput_ids\n\n\n\n\n0\nGenerate Pyhton code that Check if the grid cl...\nCheck if the grid client is up.\\n\\n Check f...\n42\n[6864, 1611, 517, 265, 1233, 626, 2670, 340, 3...\n\n\n1\nGenerate Pyhton code that Ensure that powershe...\n\\n Ensure that powershell processes inl...\n142\n[6864, 1611, 517, 265, 1233, 626, 7523, 626, 7...\n\n\n2\nGenerate Pyhton code that Encode a bytestring ...\n\\n Encode a bytestring to a base64 string f...\n59\n[6864, 1611, 517, 265, 1233, 626, 19244, 282, ...\n\n\n3\nGenerate Pyhton code that Add the arguments fo...\nAdd the arguments for the protocol to the clie...\n97\n[6864, 1611, 517, 265, 1233, 626, 2654, 314, 2...\n\n\n4\nGenerate Pyhton code that Locking should inclu...\nLocking should include hashes for *all* platf...\n60\n[6864, 1611, 517, 265, 1233, 626, 15536, 316, ..."
  },
  {
    "objectID": "gpt_sampling_.html#model-sampling-generation",
    "href": "gpt_sampling_.html#model-sampling-generation",
    "title": "Decoder Based Transformer (GPT-2) Sampling",
    "section": "Model Sampling Generation",
    "text": "Model Sampling Generation\n\nprint(df_sampled_code['size'].max())\nprint(df_sampled_code['size'].mean())\n#take the 75% percentile \ndf_sampled_code['size'].quantile(0.85)\n\n157\n83.97\n\n\n130.0\n\n\n\nSAMPLES = 30 #&lt;---- Hardocoded\nMAX_GEN_TOK = df_sampled_code['size'].max()\n#MAX_GEN_TOK = 112\n\n\nMAX_GEN_TOK\n\n157\n\n\n\ninputs = tokenizer([\"def hello_world():\"], return_tensors=\"pt\")\ninputs.to(device)\noutputs = model.generate(**inputs, do_sample=True, max_length=MAX_GEN_TOK, top_k=0, num_return_sequences=2, pad_token_id=tokenizer.eos_token_id)\n#tokenizer.decode(outputs[0][len(inputs['input_ids'][0]):])\n\n\noutputs\n\ntensor([[  318, 16509,    63,  7617,   837,   272,   372,  3839,   342,   421,\n           199,   318, 17242,    63,  7617,     8,   589,    12,  4353,    63,\n           815,    29,   403,     9,  1035,   488,    26,   272,   372,   413,\n           339,   340, 10061,   436,  2688,     8,  8258,    12,   283,  2325,\n          3097,   735,   267, 10061,    14,  2325,  3097,   342,   272,   372,\n         17242,   342,   421,   199,   318, 13318,  5199,   837,   272,   408,\n           272,   372,   282,  1059,   370,  1413,  4993,   272,   408,   272,\n           862,    26,   267,   372, 17242,    63,  7617,   342,   272,   871,\n           334,  8647,   547,    12, 13483,   304,   267,   327, 13244,  2366,\n          1928,  7252,   267,   746,   421,   199,   318, 16509,    63,  7617,\n             8,   589,    12,  4353,    63,   815,    29,   403,     9,  1035,\n          2155,    26,   272,   340,  4353,    63,   815,   365,   488,    26,\n           267,  2967,    67,  7301,   275,  4884,    14,  1623,   647,  1418,\n            63,  2967,    67,  7301,   342,   267,   340,  2967,    67,  7301,\n            26,   288, 25159,    14, 11697,     8,   277,    12,   298,   583,\n         22703,   401,  7239,    63,  1247,  1524,   363],\n        [  318, 16509,    63,  7617,   837,   288,   372,   283,  1421,  6701,\n             7,   267,   943,    14,   815,   360,  1421,   365,   298, 15056,\n             2,  4681,   358,   339,   347, 10391,    63,   745,     8,   277,\n           304,   267,   372,   283, 16321,    14, 15391,     7,   339,   702,\n           822,     8,   736,    14,  7374,  1547,  1095,    14, 17533,    14,\n           957,    15, 15391,  6331,   430,   508,   413,   272,   943,    14,\n           815,   360,  1421,   365,   298, 15056,     2, 15507,   358,   339,\n           702,   283,   658,     5,    83, 19097,   450,   334,  7019,    63,\n           745,     9,   508,   283, 15815,     7,   199,   624,   774,  3102,\n           641, 10061,  1041,   199,   646,   747,   199,   504, 14090,   492,\n          6184,    12,  3820,    12,  4879,    12, 11634,   199,   199,   504,\n          1134,  1038,    14,  5707,  3199,    14,  3924,   492,  3492,  6082,\n          1513,  1514,  4477,  1726,   199,   504,  1134,  1038,    14,  7546,\n           423,   581,   645,   471,    63,   493,    63,   646,   492,   492,\n            63,  3098,    63,  3749,    63,  3806,   421,   199,   318,   511,\n            63,  1180,    63,  1730,    63,  2609,    63]], device='cuda:0')\n\n\n\ntorch.tensor(outputs.tolist()).to(model.device)\n\ntensor([[  318, 16509,    63,  7617,   837,   272,   372,  3839,   342,   421,\n           199,   318, 17242,    63,  7617,     8,   589,    12,  4353,    63,\n           815,    29,   403,     9,  1035,   488,    26,   272,   372,   413,\n           339,   340, 10061,   436,  2688,     8,  8258,    12,   283,  2325,\n          3097,   735,   267, 10061,    14,  2325,  3097,   342,   272,   372,\n         17242,   342,   421,   199,   318, 13318,  5199,   837,   272,   408,\n           272,   372,   282,  1059,   370,  1413,  4993,   272,   408,   272,\n           862,    26,   267,   372, 17242,    63,  7617,   342,   272,   871,\n           334,  8647,   547,    12, 13483,   304,   267,   327, 13244,  2366,\n          1928,  7252,   267,   746,   421,   199,   318, 16509,    63,  7617,\n             8,   589,    12,  4353,    63,   815,    29,   403,     9,  1035,\n          2155,    26,   272,   340,  4353,    63,   815,   365,   488,    26,\n           267,  2967,    67,  7301,   275,  4884,    14,  1623,   647,  1418,\n            63,  2967,    67,  7301,   342,   267,   340,  2967,    67,  7301,\n            26,   288, 25159,    14, 11697,     8,   277,    12,   298,   583,\n         22703,   401,  7239,    63,  1247,  1524,   363],\n        [  318, 16509,    63,  7617,   837,   288,   372,   283,  1421,  6701,\n             7,   267,   943,    14,   815,   360,  1421,   365,   298, 15056,\n             2,  4681,   358,   339,   347, 10391,    63,   745,     8,   277,\n           304,   267,   372,   283, 16321,    14, 15391,     7,   339,   702,\n           822,     8,   736,    14,  7374,  1547,  1095,    14, 17533,    14,\n           957,    15, 15391,  6331,   430,   508,   413,   272,   943,    14,\n           815,   360,  1421,   365,   298, 15056,     2, 15507,   358,   339,\n           702,   283,   658,     5,    83, 19097,   450,   334,  7019,    63,\n           745,     9,   508,   283, 15815,     7,   199,   624,   774,  3102,\n           641, 10061,  1041,   199,   646,   747,   199,   504, 14090,   492,\n          6184,    12,  3820,    12,  4879,    12, 11634,   199,   199,   504,\n          1134,  1038,    14,  5707,  3199,    14,  3924,   492,  3492,  6082,\n          1513,  1514,  4477,  1726,   199,   504,  1134,  1038,    14,  7546,\n           423,   581,   645,   471,    63,   493,    63,   646,   492,   492,\n            63,  3098,    63,  3749,    63,  3806,   421,   199,   318,   511,\n            63,  1180,    63,  1730,    63,  2609,    63]], device='cuda:0')\n\n\n\ndef df_sampled_generation(\n        df_sampled_code, \n        model,\n        number_samples = 1,\n        max_gen_tok = 100, \n        top_k = 0\n    ):\n    dict_generated_code = {i: [] for i in range(number_samples)}\n    for idx_prompt, prompt in enumerate(df_sampled_code['prompt']):\n        input = tokenizer([prompt], return_tensors=\"pt\")\n        input.to(model.device)\n        outputs = model.generate(**input, do_sample=True,\n                                 #max_length=max_gen_tok if df_sampled_code['size'][idx_prompt] &lt; max_gen_tok else df_sampled_code['size'][idx_prompt],\n                                 max_length=max_gen_tok,\n                                 top_k=top_k, \n                                 num_return_sequences=number_samples, \n                                 pad_token_id=tokenizer.eos_token_id)\n        for index, output in enumerate(outputs):\n            #dict_generated_code[index].append(output[len(input['input_ids'][0]):].tolist())\n            dict_generated_code[index].append(output.tolist())\n    df_temp = pd.DataFrame().from_dict(data=dict_generated_code) # DataFrame from Generation\n    df_temp = pd.concat([df_sampled_code.reset_index(), df_temp ], axis=1) #Index before concating\n    return df_temp\n\n\n#WARNING TIME CONSUMING\ndf_generated_input = df_sampled_generation(\n    df_sampled_code=df_sampled_code, \n    model=model, \n    number_samples=SAMPLES, \n    max_gen_tok=MAX_GEN_TOK)\n\n\ndf_generated_input.head(5)\n\n\n\n\n\n\n\n\nindex\nprompt\nground_truth\nsize\ninput_ids\n0\n1\n2\n3\n4\n...\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n\n\n\n\n0\n0\nGenerate Pyhton code that Check if the grid cl...\nCheck if the grid client is up.\\n\\n Check f...\n42\n[6864, 1611, 517, 265, 1233, 626, 2670, 340, 3...\n[6864, 1611, 517, 265, 1233, 626, 2670, 340, 3...\n[6864, 1611, 517, 265, 1233, 626, 2670, 340, 3...\n[6864, 1611, 517, 265, 1233, 626, 2670, 340, 3...\n[6864, 1611, 517, 265, 1233, 626, 2670, 340, 3...\n[6864, 1611, 517, 265, 1233, 626, 2670, 340, 3...\n...\n[6864, 1611, 517, 265, 1233, 626, 2670, 340, 3...\n[6864, 1611, 517, 265, 1233, 626, 2670, 340, 3...\n[6864, 1611, 517, 265, 1233, 626, 2670, 340, 3...\n[6864, 1611, 517, 265, 1233, 626, 2670, 340, 3...\n[6864, 1611, 517, 265, 1233, 626, 2670, 340, 3...\n[6864, 1611, 517, 265, 1233, 626, 2670, 340, 3...\n[6864, 1611, 517, 265, 1233, 626, 2670, 340, 3...\n[6864, 1611, 517, 265, 1233, 626, 2670, 340, 3...\n[6864, 1611, 517, 265, 1233, 626, 2670, 340, 3...\n[6864, 1611, 517, 265, 1233, 626, 2670, 340, 3...\n\n\n1\n1\nGenerate Pyhton code that Ensure that powershe...\n\\n Ensure that powershell processes inl...\n142\n[6864, 1611, 517, 265, 1233, 626, 7523, 626, 7...\n[6864, 1611, 517, 265, 1233, 626, 7523, 626, 7...\n[6864, 1611, 517, 265, 1233, 626, 7523, 626, 7...\n[6864, 1611, 517, 265, 1233, 626, 7523, 626, 7...\n[6864, 1611, 517, 265, 1233, 626, 7523, 626, 7...\n[6864, 1611, 517, 265, 1233, 626, 7523, 626, 7...\n...\n[6864, 1611, 517, 265, 1233, 626, 7523, 626, 7...\n[6864, 1611, 517, 265, 1233, 626, 7523, 626, 7...\n[6864, 1611, 517, 265, 1233, 626, 7523, 626, 7...\n[6864, 1611, 517, 265, 1233, 626, 7523, 626, 7...\n[6864, 1611, 517, 265, 1233, 626, 7523, 626, 7...\n[6864, 1611, 517, 265, 1233, 626, 7523, 626, 7...\n[6864, 1611, 517, 265, 1233, 626, 7523, 626, 7...\n[6864, 1611, 517, 265, 1233, 626, 7523, 626, 7...\n[6864, 1611, 517, 265, 1233, 626, 7523, 626, 7...\n[6864, 1611, 517, 265, 1233, 626, 7523, 626, 7...\n\n\n2\n2\nGenerate Pyhton code that Encode a bytestring ...\n\\n Encode a bytestring to a base64 string f...\n59\n[6864, 1611, 517, 265, 1233, 626, 19244, 282, ...\n[6864, 1611, 517, 265, 1233, 626, 19244, 282, ...\n[6864, 1611, 517, 265, 1233, 626, 19244, 282, ...\n[6864, 1611, 517, 265, 1233, 626, 19244, 282, ...\n[6864, 1611, 517, 265, 1233, 626, 19244, 282, ...\n[6864, 1611, 517, 265, 1233, 626, 19244, 282, ...\n...\n[6864, 1611, 517, 265, 1233, 626, 19244, 282, ...\n[6864, 1611, 517, 265, 1233, 626, 19244, 282, ...\n[6864, 1611, 517, 265, 1233, 626, 19244, 282, ...\n[6864, 1611, 517, 265, 1233, 626, 19244, 282, ...\n[6864, 1611, 517, 265, 1233, 626, 19244, 282, ...\n[6864, 1611, 517, 265, 1233, 626, 19244, 282, ...\n[6864, 1611, 517, 265, 1233, 626, 19244, 282, ...\n[6864, 1611, 517, 265, 1233, 626, 19244, 282, ...\n[6864, 1611, 517, 265, 1233, 626, 19244, 282, ...\n[6864, 1611, 517, 265, 1233, 626, 19244, 282, ...\n\n\n3\n3\nGenerate Pyhton code that Add the arguments fo...\nAdd the arguments for the protocol to the clie...\n97\n[6864, 1611, 517, 265, 1233, 626, 2654, 314, 2...\n[6864, 1611, 517, 265, 1233, 626, 2654, 314, 2...\n[6864, 1611, 517, 265, 1233, 626, 2654, 314, 2...\n[6864, 1611, 517, 265, 1233, 626, 2654, 314, 2...\n[6864, 1611, 517, 265, 1233, 626, 2654, 314, 2...\n[6864, 1611, 517, 265, 1233, 626, 2654, 314, 2...\n...\n[6864, 1611, 517, 265, 1233, 626, 2654, 314, 2...\n[6864, 1611, 517, 265, 1233, 626, 2654, 314, 2...\n[6864, 1611, 517, 265, 1233, 626, 2654, 314, 2...\n[6864, 1611, 517, 265, 1233, 626, 2654, 314, 2...\n[6864, 1611, 517, 265, 1233, 626, 2654, 314, 2...\n[6864, 1611, 517, 265, 1233, 626, 2654, 314, 2...\n[6864, 1611, 517, 265, 1233, 626, 2654, 314, 2...\n[6864, 1611, 517, 265, 1233, 626, 2654, 314, 2...\n[6864, 1611, 517, 265, 1233, 626, 2654, 314, 2...\n[6864, 1611, 517, 265, 1233, 626, 2654, 314, 2...\n\n\n4\n4\nGenerate Pyhton code that Locking should inclu...\nLocking should include hashes for *all* platf...\n60\n[6864, 1611, 517, 265, 1233, 626, 15536, 316, ...\n[6864, 1611, 517, 265, 1233, 626, 15536, 316, ...\n[6864, 1611, 517, 265, 1233, 626, 15536, 316, ...\n[6864, 1611, 517, 265, 1233, 626, 15536, 316, ...\n[6864, 1611, 517, 265, 1233, 626, 15536, 316, ...\n[6864, 1611, 517, 265, 1233, 626, 15536, 316, ...\n...\n[6864, 1611, 517, 265, 1233, 626, 15536, 316, ...\n[6864, 1611, 517, 265, 1233, 626, 15536, 316, ...\n[6864, 1611, 517, 265, 1233, 626, 15536, 316, ...\n[6864, 1611, 517, 265, 1233, 626, 15536, 316, ...\n[6864, 1611, 517, 265, 1233, 626, 15536, 316, ...\n[6864, 1611, 517, 265, 1233, 626, 15536, 316, ...\n[6864, 1611, 517, 265, 1233, 626, 15536, 316, ...\n[6864, 1611, 517, 265, 1233, 626, 15536, 316, ...\n[6864, 1611, 517, 265, 1233, 626, 15536, 316, ...\n[6864, 1611, 517, 265, 1233, 626, 15536, 316, ...\n\n\n\n\n5 rows × 35 columns\n\n\n\n\ndf_generated_input[0]\n\n0     [6864, 1611, 517, 265, 1233, 626, 2670, 340, 3...\n1     [6864, 1611, 517, 265, 1233, 626, 7523, 626, 7...\n2     [6864, 1611, 517, 265, 1233, 626, 19244, 282, ...\n3     [6864, 1611, 517, 265, 1233, 626, 2654, 314, 2...\n4     [6864, 1611, 517, 265, 1233, 626, 15536, 316, ...\n                            ...                        \n95    [6864, 1611, 517, 265, 1233, 626, 1432, 314, 1...\n96    [6864, 1611, 517, 265, 1233, 626, 1432, 314, 6...\n97    [6864, 1611, 517, 265, 1233, 626, 2816, 1175, ...\n98    [6864, 1611, 517, 265, 1233, 626, 6516, 316, 2...\n99    [6864, 1611, 517, 265, 1233, 626, 1432, 3775, ...\nName: 0, Length: 100, dtype: object\n\n\n\nStatistics and Checkpoint\n\nnp_len_method = [ (np.array([ len(gen_method) for gen_method in df_generated_input[j] ]).mean(),\n                   np.array([ len(gen_method) for gen_method in df_generated_input[j] ]).std()  )\n                    for j in range(30) ]\n\n\nnp_len_method\n\n[(157.0, 0.0),\n (157.0, 0.0),\n (157.0, 0.0),\n (157.0, 0.0),\n (157.0, 0.0),\n (157.0, 0.0),\n (157.0, 0.0),\n (157.0, 0.0),\n (157.0, 0.0),\n (157.0, 0.0),\n (157.0, 0.0),\n (157.0, 0.0),\n (157.0, 0.0),\n (157.0, 0.0),\n (157.0, 0.0),\n (157.0, 0.0),\n (157.0, 0.0),\n (157.0, 0.0),\n (157.0, 0.0),\n (157.0, 0.0),\n (157.0, 0.0),\n (157.0, 0.0),\n (157.0, 0.0),\n (157.0, 0.0),\n (157.0, 0.0),\n (157.0, 0.0),\n (157.0, 0.0),\n (157.0, 0.0),\n (157.0, 0.0),\n (157.0, 0.0)]\n\n\n\n#Checkpoint of Generation\ndef checkpoint_generation( df , name = 'output' ):\n    df.to_csv(param_default()['sampling_results'] +  '/' + name + '.csv')\n    pass\n\n\ncheckpoint_generation( df = df_generated_input, name=param_default()['dataset'])\n\n\ndf_generated_input = pd.read_csv( param_default()['sampling_results'] + '/' + param_default()['dataset'] +'.csv' , index_col=0)\n\n\ndf_generated_input.head()\n\n\n\n\n\n\n\n\nindex\nprompt\nground_truth\nsize\ninput_ids\n0\n1\n2\n3\n4\n...\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n\n\n\n\n0\n0\nGenerate Pyhton code that Check if the grid cl...\nCheck if the grid client is up.\\n\\n Check f...\n42\n[6864, 1611, 517, 265, 1233, 626, 2670, 340, 3...\n[6864, 1611, 517, 265, 1233, 626, 2670, 340, 3...\n[6864, 1611, 517, 265, 1233, 626, 2670, 340, 3...\n[6864, 1611, 517, 265, 1233, 626, 2670, 340, 3...\n[6864, 1611, 517, 265, 1233, 626, 2670, 340, 3...\n[6864, 1611, 517, 265, 1233, 626, 2670, 340, 3...\n...\n[6864, 1611, 517, 265, 1233, 626, 2670, 340, 3...\n[6864, 1611, 517, 265, 1233, 626, 2670, 340, 3...\n[6864, 1611, 517, 265, 1233, 626, 2670, 340, 3...\n[6864, 1611, 517, 265, 1233, 626, 2670, 340, 3...\n[6864, 1611, 517, 265, 1233, 626, 2670, 340, 3...\n[6864, 1611, 517, 265, 1233, 626, 2670, 340, 3...\n[6864, 1611, 517, 265, 1233, 626, 2670, 340, 3...\n[6864, 1611, 517, 265, 1233, 626, 2670, 340, 3...\n[6864, 1611, 517, 265, 1233, 626, 2670, 340, 3...\n[6864, 1611, 517, 265, 1233, 626, 2670, 340, 3...\n\n\n1\n1\nGenerate Pyhton code that Ensure that powershe...\n\\n Ensure that powershell processes inl...\n142\n[6864, 1611, 517, 265, 1233, 626, 7523, 626, 7...\n[6864, 1611, 517, 265, 1233, 626, 7523, 626, 7...\n[6864, 1611, 517, 265, 1233, 626, 7523, 626, 7...\n[6864, 1611, 517, 265, 1233, 626, 7523, 626, 7...\n[6864, 1611, 517, 265, 1233, 626, 7523, 626, 7...\n[6864, 1611, 517, 265, 1233, 626, 7523, 626, 7...\n...\n[6864, 1611, 517, 265, 1233, 626, 7523, 626, 7...\n[6864, 1611, 517, 265, 1233, 626, 7523, 626, 7...\n[6864, 1611, 517, 265, 1233, 626, 7523, 626, 7...\n[6864, 1611, 517, 265, 1233, 626, 7523, 626, 7...\n[6864, 1611, 517, 265, 1233, 626, 7523, 626, 7...\n[6864, 1611, 517, 265, 1233, 626, 7523, 626, 7...\n[6864, 1611, 517, 265, 1233, 626, 7523, 626, 7...\n[6864, 1611, 517, 265, 1233, 626, 7523, 626, 7...\n[6864, 1611, 517, 265, 1233, 626, 7523, 626, 7...\n[6864, 1611, 517, 265, 1233, 626, 7523, 626, 7...\n\n\n2\n2\nGenerate Pyhton code that Encode a bytestring ...\n\\n Encode a bytestring to a base64 string f...\n59\n[6864, 1611, 517, 265, 1233, 626, 19244, 282, ...\n[6864, 1611, 517, 265, 1233, 626, 19244, 282, ...\n[6864, 1611, 517, 265, 1233, 626, 19244, 282, ...\n[6864, 1611, 517, 265, 1233, 626, 19244, 282, ...\n[6864, 1611, 517, 265, 1233, 626, 19244, 282, ...\n[6864, 1611, 517, 265, 1233, 626, 19244, 282, ...\n...\n[6864, 1611, 517, 265, 1233, 626, 19244, 282, ...\n[6864, 1611, 517, 265, 1233, 626, 19244, 282, ...\n[6864, 1611, 517, 265, 1233, 626, 19244, 282, ...\n[6864, 1611, 517, 265, 1233, 626, 19244, 282, ...\n[6864, 1611, 517, 265, 1233, 626, 19244, 282, ...\n[6864, 1611, 517, 265, 1233, 626, 19244, 282, ...\n[6864, 1611, 517, 265, 1233, 626, 19244, 282, ...\n[6864, 1611, 517, 265, 1233, 626, 19244, 282, ...\n[6864, 1611, 517, 265, 1233, 626, 19244, 282, ...\n[6864, 1611, 517, 265, 1233, 626, 19244, 282, ...\n\n\n3\n3\nGenerate Pyhton code that Add the arguments fo...\nAdd the arguments for the protocol to the clie...\n97\n[6864, 1611, 517, 265, 1233, 626, 2654, 314, 2...\n[6864, 1611, 517, 265, 1233, 626, 2654, 314, 2...\n[6864, 1611, 517, 265, 1233, 626, 2654, 314, 2...\n[6864, 1611, 517, 265, 1233, 626, 2654, 314, 2...\n[6864, 1611, 517, 265, 1233, 626, 2654, 314, 2...\n[6864, 1611, 517, 265, 1233, 626, 2654, 314, 2...\n...\n[6864, 1611, 517, 265, 1233, 626, 2654, 314, 2...\n[6864, 1611, 517, 265, 1233, 626, 2654, 314, 2...\n[6864, 1611, 517, 265, 1233, 626, 2654, 314, 2...\n[6864, 1611, 517, 265, 1233, 626, 2654, 314, 2...\n[6864, 1611, 517, 265, 1233, 626, 2654, 314, 2...\n[6864, 1611, 517, 265, 1233, 626, 2654, 314, 2...\n[6864, 1611, 517, 265, 1233, 626, 2654, 314, 2...\n[6864, 1611, 517, 265, 1233, 626, 2654, 314, 2...\n[6864, 1611, 517, 265, 1233, 626, 2654, 314, 2...\n[6864, 1611, 517, 265, 1233, 626, 2654, 314, 2...\n\n\n4\n4\nGenerate Pyhton code that Locking should inclu...\nLocking should include hashes for *all* platf...\n60\n[6864, 1611, 517, 265, 1233, 626, 15536, 316, ...\n[6864, 1611, 517, 265, 1233, 626, 15536, 316, ...\n[6864, 1611, 517, 265, 1233, 626, 15536, 316, ...\n[6864, 1611, 517, 265, 1233, 626, 15536, 316, ...\n[6864, 1611, 517, 265, 1233, 626, 15536, 316, ...\n[6864, 1611, 517, 265, 1233, 626, 15536, 316, ...\n...\n[6864, 1611, 517, 265, 1233, 626, 15536, 316, ...\n[6864, 1611, 517, 265, 1233, 626, 15536, 316, ...\n[6864, 1611, 517, 265, 1233, 626, 15536, 316, ...\n[6864, 1611, 517, 265, 1233, 626, 15536, 316, ...\n[6864, 1611, 517, 265, 1233, 626, 15536, 316, ...\n[6864, 1611, 517, 265, 1233, 626, 15536, 316, ...\n[6864, 1611, 517, 265, 1233, 626, 15536, 316, ...\n[6864, 1611, 517, 265, 1233, 626, 15536, 316, ...\n[6864, 1611, 517, 265, 1233, 626, 15536, 316, ...\n[6864, 1611, 517, 265, 1233, 626, 15536, 316, ...\n\n\n\n\n5 rows × 35 columns\n\n\n\n\ndf_generated_input.describe()\n\n\n\n\n\n\n\n\nindex\nsize\n\n\n\n\ncount\n100.00\n100.00\n\n\nmean\n49.50\n83.97\n\n\nstd\n29.01\n36.43\n\n\nmin\n0.00\n29.00\n\n\n25%\n24.75\n50.00\n\n\n50%\n49.50\n82.50\n\n\n75%\n74.25\n115.00\n\n\nmax\n99.00\n157.00\n\n\n\n\n\n\n\n\n#tst decoding\ndecoded_input = tokenizer.decode(eval(df_generated_input['input_ids'][1]))\ndecoded_output = tokenizer.decode(eval(df_generated_input['1'][1]))\nprint(decoded_input)\nprint('-'*100)\nprint(decoded_output)\n\nGenerate Pyhton code that Ensure that powershell processes inline script in args with powershell\n        core :\n----------------------------------------------------------------------------------------------------\nGenerate Pyhton code that Ensure that powershell processes inline script in args with powershell\n        core : Python object you need to start the Python window\n        args : PyHTonCmd arguments to be run here, e.g., powershell.py\n        \"\"\"  \n        core.powershell()   \n        return 0    \n    # implement pyhton\n    # add error hook.\n    # add trace hook.  This is useful if we need from urglue tracebacks.\n    # no sanity checking this could go back to pyhon.processStream() whenever it works.\n    def main(self, argv, debug):\n       \n        Parse arguments and ensure it feasible.\n        \"\"\"  \n        if argv[1:]:\n            argv = argv[1:]\n        gpu =\n\n\n\n## MEMORy DEALLOCATION\ntorch.cuda.empty_cache()"
  }
]